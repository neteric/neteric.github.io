<?xml version="1.0" encoding="utf-8"?>






<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>neteric</title>
        <link>https://neteric.top/</link>
        <description>neteric&#39;s blog, record work and life</description>
        <generator>Hugo 0.101.0 https://gohugo.io/</generator>
        
            <language>en</language>
        
        
            <managingEditor>neteric@126.com (neteric)</managingEditor>
        
        
            <webMaster>neteric@126.com (neteric)</webMaster>
        
        
            <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
        
        <lastBuildDate>Wed, 23 Nov 2022 16:00:22 &#43;0800</lastBuildDate>
        
            <atom:link rel="self" type="application/rss&#43;xml" href="https://neteric.top/rss.xml" />
        
        
            <item>
                <title>离在线混部调研</title>
                <link>https://neteric.top/posts/colocation_investigate/</link>
                <guid isPermaLink="true">https://neteric.top/posts/colocation_investigate/</guid>
                <pubDate>Tue, 15 Nov 2022 16:10:03 &#43;0800</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;h2 id=&#34;1-背景&#34;&gt;1. 背景&lt;/h2&gt;
&lt;p&gt;公司战略目标：降本增效&lt;/p&gt;
&lt;p&gt;HPA上线使得在线业务的波谷期空闲出大量的计算资源&lt;/p&gt;
&lt;p&gt;大数据业务云原生化是必然的趋势，公司大数据团队正在着手落地MR/Spark类的离线业务云原生化&lt;/p&gt;
&lt;h2 id=&#34;2-离在线混部的目标&#34;&gt;2. 离在线混部的目标&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;尽可以能的提高资源利用率&lt;/li&gt;
&lt;li&gt;保证在线业务的SLA&lt;/li&gt;
&lt;li&gt;保证离线业务运行良好&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-业界离在线混部方案分类&#34;&gt;3. 业界离在线混部方案分类&lt;/h2&gt;
&lt;p&gt;按照混合类型分： 分时复用和部分混合和全部混合&lt;/p&gt;
&lt;p&gt;按照隔离类型分： 资源独享和资源共享&lt;/p&gt;
&lt;p&gt;按照部署方式分： 虚拟机部署和容器部署&lt;/p&gt;
&lt;p&gt;这三个维度的组合，目前实际应用中主要是&lt;code&gt;分时复用 + 独占资源 + 物理机部署&lt;/code&gt;、&lt;code&gt;部分混合 + 独占资源 + 容器部署&lt;/code&gt;、&lt;code&gt;全部混合 + 共享资源 + 容器部署&lt;/code&gt; 这三种模式，这三种模式其实也是离在线混部的三个阶段。&lt;/p&gt;
&lt;h3 id=&#34;31-分时复用--独占资源--物理机部署&#34;&gt;3.1 分时复用 + 独占资源 + 物理机部署&lt;/h3&gt;
&lt;p&gt;这种组合属于入门级的在离线混部选择，比如物理机运行服务且分时整机腾挪&lt;/p&gt;
&lt;p&gt;好处是能够快速实现在离线混部，收获成本降低的红利。技术门槛上，这种方式规避了前面说的复杂的资源隔离问题，并且调度决策足够清晰，服务部署节奏有明确的预期，整个系统可以设计得比较简单。缺点是没有充分发挥出在离线混部的资源利用率潜力，目前主要是一些初创企业在应用。阿里早期在大促期间，将所有离线作业节点下线换上在线服务，也可以看做这种形态的近似版本&lt;/p&gt;
&lt;h3 id=&#34;32-部分混合--独占资源--容器部署&#34;&gt;3.2 部分混合 + 独占资源 + 容器部署&lt;/h3&gt;
&lt;p&gt;在这种模型下，业务开发人员将服务部署在云原生部署平台，选择某些指标（大部分伴随着流量潮汐特性）来衡量服务负载，平台会按照业务指定规则对服务节点数量扩缩容。当流量低峰期来临时，在线服务会有大量碎片资源可释放，部署平台会整理碎片资源，将碎片资源化零为整后，以整机的方式将资源租借给离线作业使用&lt;/p&gt;
&lt;h3 id=&#34;33-全部混合--共享资源--容器部署&#34;&gt;3.3 全部混合 + 共享资源 + 容器部署&lt;/h3&gt;
&lt;p&gt;与上述几种方案最大的不同在于，转让的资源规则是动态决策的。在一个大企业中，服务数量数以万计，要求所有在线服务制定扩缩容决策是很难做到的。同时，业务在部署服务时更关注服务稳定性，常常按照最大流量评估资源，这样就导致流量低峰期有大量的资源浪费。
比较典型的是阿里，百度、腾讯的方案&lt;/p&gt;
&lt;h2 id=&#34;4-离在线混部的难点和常见方案&#34;&gt;4. 离在线混部的难点和常见方案&lt;/h2&gt;
&lt;h3 id=&#34;41-资源隔离&#34;&gt;4.1 资源隔离&lt;/h3&gt;
&lt;p&gt;为了保证离线业务的运行不能影响在线业务的SLA，那必须要做好离在线资源的隔离,其实K8S本身的QoS策略就是为了按照优先级保障业务的稳定性，当Node节点的资源到达安全阈值的时候会发生&lt;a href=&#34;https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/&#34;&gt;节点压力驱逐&lt;/a&gt;优先驱逐掉BestEffort类型的离线业务，其次是Burstable。但是当资源发生争抢，系统负载高的时候，仅仅通过K8S自身的手段，保证不了在线业务质量，在线业务会有明显的长尾延迟。所以一般会通过一些隔离手段来保证在线业务的稳定性，业界主要有如下一些方式&lt;/p&gt;
&lt;h4 id=&#34;cpu-隔离&#34;&gt;CPU 隔离&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;CPU Burst&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CPU Burst 技术最初由阿里云操作系统团队提出并贡献到Linux社区和龙蜥社区，分别在 Linux 5.14 和龙蜥ANCK 4.19 版本可用，在传统的 CPU Bandwidth Controller quota 和 period 基础上引入 burst 的概念。当容器的 CPU 使用低于 quota 时，可用于突发的 burst 资源累积下来；当容器的 CPU 使用超过 quota，允许使用累积的 burst 资源。最终达到的效果是将容器更长时间的平均 CPU 消耗限制在 quota 范围内，允许短时间内的 CPU 使用超过其 quota&lt;/p&gt;
&lt;p&gt;在容器场景中使用 CPU Burst 之后，测试容器的服务质量显著提升，在实测中可以发现使用该特性技术以后，RT长尾问题几乎消失&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developer.aliyun.com/article/786430?spm=a2c4g.11186623.0.0.48787123wBxE3H&#34;&gt;干掉讨厌的 CPU 限流，让容器跑得更快&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cfs 抢占调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CFS让高优任务占尽优势的同时，也保留了低优任务使用CPU的权利，作为一个通用的调度算法，CFS是公平也是合理的。但当我们但在混部的场景下这会导致一个监控上的毛刺，或引发线上服务的抖动,CFS从设计理念上就不支持绝对抢占，因此，腾讯云原生内核自行研发了支持绝对抢占的离线调度器——BT调度&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/1876817&#34;&gt;云原生下，TencentOS “如意” CPU QoS之绝对抢占&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L3 Cache隔离技术&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;英特尔RDT提供了一个由多个组件功能（包括 CMT、CAT、CDP、MBM 和 MBA）组成的框架，用于缓存和内存监控及分配功能。这些技术可以跟踪和控制平台上同时运行的多个应用程序、容器或 VM 使用的共享资源，例如最后一级缓存 (LLC) 和主内存 (DRAM) 带宽。RDT 可以帮助检测“吵闹的邻居”并减少性能干扰，从而确保复杂环境中关键工作负载的性能&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.cn/content/www/cn/zh/architecture-and-technology/resource-director-technology.html&#34;&gt;英特尔® 资源调配技术 (英特尔® RDT) 框架&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;超线程隔离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在某些线上业务场景中，使用超线程情况下的 QPS 比未使用超线程时下降明显，并且相应 RT 也增加了不少。根本原因跟超线程的物理性质有关，超线程技术在一个物理核上模拟两个逻辑核，两个逻辑核具有各自独立的寄存器（eax、ebx、ecx、msr 等等）和 APIC，但会共享使用物理核的执行资源，包括执行引擎、L1/L2 缓存、TLB 和系统总线等等。这就意味着，如果一对 HT 的一个核上跑了在线任务，与此同时它对应的 HT 核上跑了一个离线任务，那么它们之间是会发生竞争的，这就是我们需要解决的问题。&lt;/p&gt;
&lt;p&gt;为了尽可能减轻这种竞争的影响，我们想要让一个核上的在线任务执行的时候，它对应的 HT 上不再运行离线任务；或者当一个核上有离线任务运行的时候，在线任务调度到了其对应的 HT 上时，离线任务会被驱赶走。听起来离线混得很惨对不对？但是这就是我们保证 HT 资源不被争抢的机制&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://help.aliyun.com/document_detail/338407.html&#34;&gt;龙蜥Group Identity 技术&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/_DTQ4Q2dC-kN3zyozGf9QA&#34;&gt;龙蜥超线程隔离 SMT expeller 技术&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;离线大框，动态绑核&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过cpuset cgroup对整体混部大框做了绑核处理，避免混部任务进程频繁切换干扰在线业务进程。当混部算力改变时，再给离线大框动态选取相应的cpu核心进行绑定。同时在选取cpu核心的时候还需要考虑了cpu HT，即尽量将同一个物理核上的逻辑核同时绑定给混部任务使用。否则，如果在线任务和混部任务分别跑在一个物理核的两个逻辑核上，在线任务还是有可能受到“noisy neighbor”干扰&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/pPEkfrLm0XEpgMU1KjiD4A&#34;&gt;B站云原生混部技术实践&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;内存隔离&#34;&gt;内存隔离&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;存带宽分配(MBA)隔离&lt;/li&gt;
&lt;li&gt;内存提前异步回收&lt;/li&gt;
&lt;li&gt;oom score&lt;/li&gt;
&lt;li&gt;基于内存安全阈值的主动驱逐机制&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;磁盘隔离&#34;&gt;磁盘隔离&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;使用分布式存储实现存算分离&lt;/li&gt;
&lt;li&gt;对块设备执行iops限速&lt;/li&gt;
&lt;li&gt;bufferio 使用cgroupv2限制cache使用量&lt;/li&gt;
&lt;li&gt;离线业务remote shuffle&lt;/li&gt;
&lt;li&gt;使用diskquota对文件系统空间和inode数量限制&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;网络隔离&#34;&gt;网络隔离&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;tc&lt;/li&gt;
&lt;li&gt;网络QoS标签&lt;/li&gt;
&lt;li&gt;ipset/iptables&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;干扰检测&#34;&gt;干扰检测&lt;/h4&gt;
&lt;p&gt;虽然上述讨论了几种主要资源的管理，但由于底层技术限制，部分资源的管理还不完善，并且竞争资源不仅仅是这些，所以，为了保证安全混部，还要增加干扰检测和冲突处理。换句话说，资源隔离是为了避免干扰，干扰检测是根据应用的实际运行情况在冲突发生前或发生后采取措施。在线作业可以提供获取时延数据的方式，或者暴露相关接口，另外还可以收集一些硬件指标，如通过perf收集CPI数据，RDT收集L3 Cache，或epbf收集内核关键路径数据。通过算法分析来判断在线作业是否受影响，若发现异常，便开始降低离线资源，驱逐离线任务&lt;/p&gt;
&lt;h3 id=&#34;42-离线业务的资源保障&#34;&gt;4.2 离线业务的资源保障&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;可压缩先压缩&lt;/li&gt;
&lt;li&gt;不可压缩资源避让足量&lt;/li&gt;
&lt;li&gt;基于资源满足的驱逐机制&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;43-离在线调度框架的整合&#34;&gt;4.3 离在线调度框架的整合&lt;/h3&gt;
&lt;p&gt;k8s的每个节点都会上报节点资源总量（例如allocatable cpu）。对于一个待调度的pod，k8s调度器会查看pod的资源请求配额（例如cpu reqeust）以及相关调度约束，然后经过预选和优选阶段，最终挑选出一个最佳的node用于部署该pod。如果混部任务直接使用这套原生的调度机制会存在几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;混部pod会占用原生的资源配额（例如cpu request），这会导致在线任务发布的时候没有可用资源；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;原生调度本质是静态调度，没有考虑机器实际负载，因此没法有效地将混部任务调度到实际负载有空闲的机器上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不支持Gang调度&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;44-可观测性体系的建立&#34;&gt;4.4 可观测性体系的建立&lt;/h3&gt;
&lt;p&gt;可观测性对于排查单机的混部问题非常有用，我们需要监控曲线来查看某个时刻的混部资源使用情况。同时，我们也可以随时查看整体集群的混部算力使用趋势&lt;/p&gt;
&lt;h3 id=&#34;45-部门墙&#34;&gt;4.5 部门墙&lt;/h3&gt;
&lt;p&gt;公司内部，机器的产品线一般是固定的，成本和利用率也是按照产品线计算，所以通常情况下机器是不会在不同部门之间自由流转的。引入在离线混部之后，势必需要打破部门墙，对成本和资源利用率计算有一个能融合能分解的调整，才能准确反映出混部的巨大成本价值并持续精细化运营&lt;/p&gt;
&lt;h2 id=&#34;5-koordinator-介绍&#34;&gt;5. Koordinator 介绍&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;在版本迭代过程中，Koordinator社区始终围绕三大能力而构建，即任务调度、差异化 SLO 以及 QoS 感知调度能力。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/%E7%A6%BB%E5%9C%A8%E7%BA%BF%E6%B7%B7%E9%83%A8%E8%B0%83%E7%A0%94_koordinator_arch.png&#34; alt=&#34;picture 1&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;51-任务调度&#34;&gt;5.1 任务调度&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Enhanced Coscheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;机器学习和大数据领域中广泛存在的具备 All-or-Nothing 需求的作业负载。例如当提交一个Job 时会产生多个任务，这些任务期望要么全部调度成功，要么全部失败。这种需求称为 All-or-Nothing。为了解决 All-or-Nothing 调度需求，Koordinator 基于社区已有的 Coscheduling 实现了 Enhanced Coscheduling：
支持 Strict/NonStrict 模式，解决大作业场景长时间得不到资源问题
支持 AI 场景多角色联合的 coscheduling 策略，例如一个 TF  训练 Job 中包含 PS 和 Worker 两种角色，并且两种角色都需要单独定义 MinMember，但又期望两种角色的 All-or-Nothing 都满足后才能继续调度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enhanced ElasticQuota Scheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;企业的 Kubernetes 一般由多个产品和研发团队共用一个规模比较大的 Kubernetes 集群，由资源运维团队统一管理集群内大量 CPU/Memory/Disk 等资源。
Koordinator 为帮助用户管理好资源额度，提升资源额度的使用效率，实现降本增效，Koordinator 基于基于社区 ElasticQuota CRD 实现了 Enhanced  ElasticQuota Scheduling&lt;/p&gt;
&lt;p&gt;Koordinator ElasticQuota Scheduling 通过额度借用机制和公平性保障机制，Koordinator 把空闲的额度复用给更需要的业务使用。当额度的所有者需要额度时，Koordinator 又可以保障有额度可用。通过树形管理机制管理 Quota，可以方便的与大多数公司的组织结构映射，解决同部门内或者不同部门间的额度管理需求。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fine-grained Device Scheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 AI 领域，GPU、RDMA、FPGA 等异构资源被广泛的应用，Koordinator 针对异构资源调度场景，提供了精细化的设备调度管理机制，包括：
支持 GPU 共享，GPU 独占，GPU 超卖
支持百分比的 GPU 调度
支持 GPU 多卡调度
NVLink 拓扑感知调度（doing）&lt;/p&gt;
&lt;h3 id=&#34;52-差异化-slo&#34;&gt;5.2 差异化 SLO&lt;/h3&gt;
&lt;p&gt;差异化 SLO 是 Koordinator 提供的核心混部能力，保障资源超卖之后的 Pod 的运行稳定性。Koordinator 定了一一组 Priority &amp;amp; QoS，用户按照这一最佳实践的方式接入应用，配合完善的资源隔离策略，最终保障不同应用类型的服务质量。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU Supress&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Koordinator 的单机组件 koordlet 会根据节点的负载水位情况，调整 BestEffort 类型 Pod 的 CPU 资源额度。这种机制称为 CPU Suppress。当节点的在线服务类应用的负载较低时，koordlet 会把更多空闲的资源分配给 BestEffort 类型的 Pod 使用；当在线服务类应用的负载上升时，koordlet 又会把分配给 BestEffort 类型的 Pod 使用的 CPU 还给在线服务类应用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU Burst&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CPU Burst 是一种服务级别目标 (SLO) 感知的资源调度功能。用户可以使用 CPU Burst 来提高对延迟敏感的应用程序的性能。内核的调度器会因为容器设置的 CPU Limit 压制容器的 CPU，这个过程称为 CPU Throttle。该过程会降低应用程序的性能。
Koordinator 自动检测 CPU Throttle 事件，并自动将 CPU Limit 调整为适当的值。通过 CPU Burst 机制能够极大地提高延迟敏感的应用程序的性能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于内存安全阈值的主动驱逐机制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当延迟敏感的应用程序对外提供服务时，内存使用量可能会由于突发流量而增加。类似地，BestEffort 类型的工作负载可能存在类似的场景，例如，当前计算负载超过预期的资源请求/限制。这些场景会增加节点整体内存使用量，对节点侧的运行时稳定性产生不可预知的影响。例如，它会降低延迟敏感的应用程序的服务质量，甚至变得不可用。尤其是在混部场景下，这个问题更具挑战性&lt;/p&gt;
&lt;p&gt;Koordinator 中实现了基于内存安全阈值的主动驱逐机制。koordlet 会定期检查 Node 和 Pods 最近的内存使用情况，检查是否超过了安全阈值。如果超过，它将驱逐一些 BestEffort 类型的 Pod 释放内存。在驱逐前根据 Pod 指定的优先级排序，优先级越低，越优先被驱逐。相同的优先级会根据内存使用率（RSS）进行排序，内存使用率越高越优先被驱逐。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于资源满足的驱逐机制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CPU Suppress 在线应用的负载升高时可能会频繁的压制离线任务，这虽然可以很好的保障在线应用的运行时质量，但是对离线任务还是有一些影响的。虽然离线任务是低优先级的，但频繁压制会导致离线任务的性能得不到满足，严重的也会影响到离线的服务质量。而且频繁的压制还存在一些极端的情况，如果离线任务在被压制时持有内核全局锁等特殊资源，那么频繁的压制可能会导致优先级反转之类的问题，反而会影响在线应用。虽然这种情况并不经常发生。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，Koordinator 提出了一种基于资源满足度的驱逐机制。我们把实际分配的 CPU 总量与期望分配的 CPU 总量的比值成为 CPU 满足度。当离线任务组的 CPU 满足度低于阈值，而且离线任务组的 CPU 利用率超过 90% 时，koordlet 会驱逐一些低优先级的离线任务，释放出一些资源给更高优先级的离线任务使用。通过这种机制能够改善离线任务的资源需求。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L3 Cache 和内存带宽分配(MBA) 隔离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;混部场景下，同一台机器上部署不同类型的工作负载，这些工作负载会在硬件更底层的维度发生频繁的资源竞争。因此如果竞争冲突严重时，是无法保障工作负载的服务质量的。
Koordinator 基于 Resource Director Technology (RDT, 资源导向技术) ，控制由不同优先级的工作负载可以使用的末级缓存（服务器上通常为 L3 缓存）。RDT 还使用内存带宽分配 (MBA) 功能来控制工作负载可以使用的内存带宽。这样可以隔离工作负载使用的 L3 缓存和内存带宽，确保高优先级工作负载的服务质量，并提高整体资源利用率。&lt;/p&gt;
&lt;h3 id=&#34;53-qos-感知调度能力&#34;&gt;5.3 QoS 感知调度能力&lt;/h3&gt;
&lt;p&gt;Koordinator 差异化 SLO 能力在节点侧提供了诸多 QoS 保障能力能够很好的解决运行时的质量问题。同时 Koordinator Scheduler 也在集群维度提供了增强的调度能力，保障在调度阶段为不同优先级和类型的 Pod 分配合适的节点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负载感知调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Koordinator 的调度器提供了一个可配置的调度插件控制集群的利用率。该调度能力主要依赖于 koordlet 上报的节点指标数据，在调度时会过滤掉负载高于某个阈值的节点，防止 Pod 在这种负载较高的节点上无法获得很好的资源保障，另一方面是避免负载已经较高的节点继续恶化。在打分阶段选择利用率更低的节点。该插件会基于时间窗口和预估机制规避因瞬间调度太多的 Pod 到冷节点机器出现一段时间后冷节点过热的情况。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;精细化 CPU 调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;随着资源利用率的提升进入到混部的深水区，需要对资源运行时的性能做更深入的调优，更精细的资源编排可以更好的保障运行时质量，从而通过混部将利用率推向更高的水平。
我们把 Koordinator QoS 在线应用 LS 类型做了更细致的划分，分为 LSE、LSR 和 LS 三种类型。拆分后的 QoS 类型具备更高的隔离性和运行时质量。通过这样的拆分，整个 Koordinator QoS 语义更加精确和完整，并且兼容 Kubernetes 已有的 QoS 语义&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://koordinator.sh/docs&#34;&gt;更多参考&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;54-总结&#34;&gt;5.4 总结&lt;/h3&gt;
&lt;p&gt;除了一些隔离特性依赖于内核的支持和缺乏一些运营平台如可观测平台，用户资源画像平台外，Koordinator是一个相对完善的在离线混部的解决方案&lt;/p&gt;
&lt;h2 id=&#34;6-volcano-介绍&#34;&gt;6. Volcano 介绍&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Volcano是CNCF 下首个也是唯一的基于Kubernetes的容器批量计算平台，主要用于高性能计算场景。它提供了Kubernetes目前缺 少的一套机制，这些机制通常是机器学习大数据应用、科学计算、特效渲染等多种高性能工作负载所需的。作为一个通用批处理平台，Volcano与几乎所有的主流计算框 架无缝对接，如Spark 、TensorFlow 、PyTorch 、 Flink 、Argo 、MindSpore 、 PaddlePaddle 等。它还提供了包括基于各种主流架构的CPU、GPU在内的异构设备混合调度能力&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/%E7%A6%BB%E5%9C%A8%E7%BA%BF%E6%B7%B7%E9%83%A8%E8%B0%83%E7%A0%94_volcano_arch.png&#34; alt=&#34;picture 2&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;61-生态支持比较全面&#34;&gt;6.1 生态支持比较全面&lt;/h3&gt;
&lt;p&gt;Volcano已经支持几乎所有的主流计算框架：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;TensorFlow&lt;/li&gt;
&lt;li&gt;PyTorch&lt;/li&gt;
&lt;li&gt;Flink&lt;/li&gt;
&lt;li&gt;Argo&lt;/li&gt;
&lt;li&gt;MindSpore&lt;/li&gt;
&lt;li&gt;PaddlePaddle&lt;/li&gt;
&lt;li&gt;OpenMPI&lt;/li&gt;
&lt;li&gt;Horovod&lt;/li&gt;
&lt;li&gt;mxnet&lt;/li&gt;
&lt;li&gt;Kubeflow&lt;/li&gt;
&lt;li&gt;KubeGene&lt;/li&gt;
&lt;li&gt;Cromwell&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;62-丰富的调度策略&#34;&gt;6.2 丰富的调度策略&lt;/h3&gt;
&lt;p&gt;Volcano支持各种调度策略，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gang-scheduling&lt;/li&gt;
&lt;li&gt;Fair-share scheduling&lt;/li&gt;
&lt;li&gt;Queue scheduling&lt;/li&gt;
&lt;li&gt;Preemption scheduling&lt;/li&gt;
&lt;li&gt;Topology-based scheduling&lt;/li&gt;
&lt;li&gt;Reclaims&lt;/li&gt;
&lt;li&gt;Backfill&lt;/li&gt;
&lt;li&gt;Resource Reservation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;得益于可扩展性的架构设计，Volcano支持用户自定义plugin和action以支持更多调度算法。&lt;/p&gt;
&lt;h3 id=&#34;63--增强型的job管理能力&#34;&gt;6.3  增强型的Job管理能力&lt;/h3&gt;
&lt;p&gt;Volcano提供了增强型的Job管理能力以适配高性能计算场景。这些特性罗列如下：&lt;/p&gt;
&lt;p&gt;多pod类型job
增强型的异常处理
可索引Job&lt;/p&gt;
&lt;h3 id=&#34;64-异构设备的支持&#34;&gt;6.4 异构设备的支持&lt;/h3&gt;
&lt;p&gt;Volcano提供了基于多种架构的计算资源的混合调度能力：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x86&lt;/li&gt;
&lt;li&gt;ARM&lt;/li&gt;
&lt;li&gt;鲲鹏&lt;/li&gt;
&lt;li&gt;昇腾&lt;/li&gt;
&lt;li&gt;GPU&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;65-总结&#34;&gt;6.5 总结&lt;/h3&gt;
&lt;p&gt;Volcano支持丰富的离线计算生态，对现有大数据和AI计算调度场景天生兼容比较好，但其发展重点侧重于解决调度问题，作为离在线整体的解决方案还需要自己补充很多其他特性&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://volcano.sh/zh/docs/&#34;&gt;更多参考&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;7-总结实践思路&#34;&gt;7. 总结：实践思路&lt;/h2&gt;
&lt;p&gt;线上：分时复用 + 独占资源 + 动态调度&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/%E7%A6%BB%E5%9C%A8%E7%BA%BF%E6%B7%B7%E9%83%A8%E8%B0%83%E7%A0%94_colocation_online_arch.png&#34; alt=&#34;picture 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;线下：共享内核 + 容器部署 + 动态策略&lt;/p&gt;
&lt;p&gt;![picture 4](/images/&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/colocation_investigate_colocation_offline_arch_new.png&#34; alt=&#34;picture 5&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://cloud.tencent.com/developer/article/1759977&#34;&gt;腾讯：Caelus—全场景在离线混部解决方案&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&#34;https://cloud.tencent.com/developer/news/653586&#34;&gt;字节跳动：自动化弹性伸缩如何支持百万级核心错峰混部&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&#34;https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/bytedance-performance-evaluation-optimization.html&#34;&gt;字节跳动：混布环境下集群的性能评估与优化&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&#34;https://www.infoq.cn/article/aeut*zaiffp0q4mskdsg&#34;&gt;百度大规模战略性混部系统演进&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] &lt;a href=&#34;https://mp.weixin.qq.com/s/12XFN2lPB3grS5FteaF__A&#34;&gt;百度混部实践：如何提高 Kubernetes 集群资源利用率？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] &lt;a href=&#34;https://developer.aliyun.com/article/651202&#34;&gt;阿里巴巴：数据中心日均 CPU 利用率 45% 的运行之道&amp;ndash;阿里巴巴规模化混部技术演进&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] &lt;a href=&#34;https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf&#34;&gt;阿里巴巴：Co-Location of Workloads with High Resource Efficiency&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] &lt;a href=&#34;https://mp.weixin.qq.com/s/_DTQ4Q2dC-kN3zyozGf9QA&#34;&gt;阿里大规模业务混部下的全链路资源隔离技术演进&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] &lt;a href=&#34;https://www.infoq.cn/article/knqswz6qrggwmv6axwqu&#34;&gt;一文看懂业界在离线混部技术&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10] &lt;a href=&#34;https://mp.weixin.qq.com/s/hQKM9beWcx7CKMvpJxznfQ&#34;&gt;美团：提升资源利用率与保障服务质量，鱼与熊掌不可兼得？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[11] &lt;a href=&#34;https://mp.weixin.qq.com/s/pPEkfrLm0XEpgMU1KjiD4A&#34;&gt;B站云原生混部技术实践&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[12] &lt;a href=&#34;https://www.bilibili.com/video/BV1AZ4y1X7AQ/?vd_source=cad2cc6310088fc3945e9d1cb002adee&#34;&gt;华为：基于Volcano的离在线业务混部技术探索&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[13] &lt;a href=&#34;https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/&#34;&gt;K8S节点压力驱逐&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[14] &lt;a href=&#34;https://www.youtube.com/watch?v=9ao7Ix2ugK4&#34;&gt;摆脱不必要的节流,同时实现高CPU利用率和高应用程序性能&lt;/a&gt;&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/categories/%E6%B7%B7%E9%83%A8/">混部</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/%E6%B7%B7%E9%83%A8/">混部</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/%E8%B0%83%E5%BA%A6/">调度</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB/">资源隔离</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>捋一捋ElasticSearch（四）| 分布式搜索原理总结</title>
                <link>https://neteric.top/posts/elasticsearch_principle_five/</link>
                <guid isPermaLink="true">https://neteric.top/posts/elasticsearch_principle_five/</guid>
                <pubDate>Thu, 06 Oct 2022 10:21:24 &#43;0800</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;blockquote&gt;
&lt;p&gt;本文在前文的基础上，整体分析下elasticsearch的文档索引和文档搜索的过程&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;1elasticsearch-文档索引过程写&#34;&gt;1.ElasticSearch 文档索引过程（写）&lt;/h2&gt;
&lt;h3 id=&#34;11-数据持久化过程&#34;&gt;1.1 数据持久化过程&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_es_data_dur_process.png&#34; alt=&#34;picture 7&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户端发送文档索引请求到任意节点，这时候该节点被称为协调节点，协调节点负责根据文档-&amp;gt;分片的路由规则计算出主分片，然后从集群的Meta中找到该分片的节点信息&lt;/li&gt;
&lt;li&gt;协调节点将请求转发到主分片所在的node节点的Lucene实例上&lt;/li&gt;
&lt;li&gt;当分片所在的节点接收到来自协调节点的请求后，会调用Lucene的&lt;code&gt;addDocument&lt;/code&gt;接口并传入文档数据来创建索引，此时文档数据和对应的索引都在Buffer Pool中&lt;/li&gt;
&lt;li&gt;为了降低对磁盘io的影响，在内存Buffer Pool中建立好了索引过后并不会立马把buffer pool中是索引flush到硬盘，而是会把buffer pool的内容&lt;strong&gt;追加&lt;/strong&gt;到tranlog文件中，然后返回给协调节点写入成功的响应，此时该索引对客户端是unsearchable的&lt;/li&gt;
&lt;li&gt;为了让客户端能尽快的搜索到刚才提交成功的文档，而后默认每一秒中进行一次调用Lucene的&lt;code&gt;commit&lt;/code&gt;接口的操作将Buffer Pool内的文档和索引commit到硬盘形成segment，一旦segment形成那么Lucene的IndexSearch模块就可以拿到文件句柄并据此成功的搜到该文档的内容。这个过程，在ElasticSearch中叫做&lt;strong&gt;refresh&lt;/strong&gt;，一旦segment形成buffer pool的内存空间就会立马释放&lt;/li&gt;
&lt;li&gt;refresh的完成后，虽然segment文件已经形成但此时segment的数据还在filesystem的pagecache中没有真正落盘，默认情况下，操作系统会按照自己的策略来flush pagecache中的数据到硬盘，当然elasticsearch的用户也可以手动调用&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/5.6/indices-flush.html&#34;&gt;flush api&lt;/a&gt;来刷盘，但一般不建议这样做，因为即使系统这时候挂掉因为有了tranlog的保障，数据也可以恢复的&lt;/li&gt;
&lt;li&gt;每隔一段时间或者tranlog大到一定的尺寸过后， Lucene会触发调用&lt;code&gt;flush&lt;/code&gt;接口，将所有Thread的In-memory buffer flush成segment文件，然后老的translog被删除，page cache 被清空&lt;/li&gt;
&lt;li&gt;段合并&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;因为3的原因，ElasticSearch被称为是近实时的搜索引擎。但当Elasticsearch作为NoSQL数据库时，查询方式是GetById，这种查询可以直接从TransLog中查询，这时候就成了RT（Real Time）实时系统。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;12-分步骤看数据持久化过程&#34;&gt;1.2 分步骤看数据持久化过程&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Lucene数据持久化过程：write -&amp;gt; refresh -&amp;gt; flush -&amp;gt; merge&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;write&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个新文档过来，会存储在 in-memory buffer 内存缓存区中，顺便会记录 Translog（Elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录）。 这时候数据还没到 segment ，是搜不到这个新文档的。数据只有被 refresh 后，才可以被搜索到
&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_es_write_process1.png&#34; alt=&#34;picture 1&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;refresh&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;refresh 默认 1 秒钟，执行一次下图流程。ES 是支持修改这个值的，通过 index.refresh_interval 设置 refresh （冲刷）间隔时间。refresh 流程大致如下： in-memory buffer 中的文档写入到新的 segment 中，但 segment 是存储在文件系统的缓存中。此时文档可以被搜索到 最后清空 in-memory buffer。注意: Translog 没有被清空，为了将 segment 数据写到磁盘 文档经过 refresh 后， segment 暂时写到文件系统缓存，这样避免了性能 IO 操作，又可以使文档搜索到。refresh 默认 1 秒执行一次，性能损耗太大。一般建议稍微延长这个 refresh 时间间隔，比如 5 s。因此，ES 其实就是准实时，达不到真正的实时。
&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_es_refresh_process1.png&#34; alt=&#34;picture 2&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;flush&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每隔一段时间或者translog大到一定的阈值（默认512M）索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行,
上个过程中 segment 在文件系统缓存中，会有意外故障文档丢失的风险，为了保证文档不会丢失，需要将文档写入磁盘。那么文档从文件缓存写入磁盘的过程就是 flush。写入磁盘后，清空 translog。具体内容如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有在内存缓冲区的文档都被写入一个新的段，buffer pool缓冲区被清空&lt;/li&gt;
&lt;li&gt;一个Commit Point被写入硬盘&lt;/li&gt;
&lt;li&gt;文件系统缓存通过 fsync 被刷新（flush）&lt;/li&gt;
&lt;li&gt;老的 translog 被删除。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_es_flush_process1.png&#34; alt=&#34;picture 3&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;megre&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。 Elasticsearch通过在后台进行Merge Segment来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段, 这个过程是不是和LSM的SST Merge很相似？ 这可能就是大佬嘴里的那句话，技术都是相通的吧
当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中，这并不会中断索引和搜索&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_es_segment_merge_process.png&#34; alt=&#34;picture 4&#34;&gt;&lt;/p&gt;
&lt;p&gt;一旦合并结束：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;新的段被刷新（flush）到了磁盘&lt;/li&gt;
&lt;li&gt;写入一个包含新段且排除旧的和较小的段的新提交点&lt;/li&gt;
&lt;li&gt;新的段被打开用来搜&lt;/li&gt;
&lt;li&gt;老的段被删除&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_es_index_alltotal_process.png&#34; alt=&#34;picture 5&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;13-分布式写入总结&#34;&gt;1.3 分布式写入总结&lt;/h3&gt;
&lt;h4 id=&#34;coordination-node&#34;&gt;coordination node&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Ingest Pipeline&lt;/p&gt;
&lt;p&gt;在这一步可以对原始文档做一些处理，比如HTML解析，自定义的处理，具体处理逻辑可以通过插件来实现。在Elasticsearch中，由于Ingest Pipeline会比较耗费CPU等资源，可以设置专门的Ingest Node，专门用来处理Ingest Pipeline逻辑。 如果当前Node不能执行Ingest Pipeline，则会将请求发给另一台可以执行Ingest Pipeline的Node&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Auto Create Index&lt;/p&gt;
&lt;p&gt;判断当前Index是否存在，如果不存在，则需要自动创建Index，这里需要和Master交互。也可以通过配置关闭自动创建Index的功能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set Routing&lt;/p&gt;
&lt;p&gt;设置路由条件，如果Request中指定了路由条件，则直接使用Request中的Routing，否则使用Mapping中配置的，如果Mapping中无配置，则使用默认的_id字段值。 在这一步中，如果没有指定id字段，则会自动生成一个唯一的_id字段，目前使用的是UUID&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Construct BulkShardRequest&lt;/p&gt;
&lt;p&gt;由于Bulk Request中会包括多个(Index/Update/Delete)请求，这些请求根据routing可能会落在多个Shard上执行，这一步会按Shard挑拣Single Write Request，同一个Shard中的请求聚集在一起，构建BulkShardRequest，每个BulkShardRequest对应一个Shard&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Send Request To Primary&lt;/p&gt;
&lt;p&gt;这一步会将每一个BulkShardRequest请求发送给相应Shard的Primary Node&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;primary-node&#34;&gt;primary node&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Index or Update or Delete&lt;/p&gt;
&lt;p&gt;循环执行每个Single Write Request，对于每个Request，根据操作类型(CREATE/INDEX/UPDATE/DELETE)选择不同的处理逻辑。 其中，Create/Index是直接新增Doc，Delete是直接根据_id删除Doc，Update会稍微复杂些，我们下面就以Update为例来介绍&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Translate Update To Index or Delete&lt;/p&gt;
&lt;p&gt;这一步是Update操作的特有步骤，在这里，会将Update请求转换为Index或者Delete请求。首先，会通过GetRequest查询到已经存在的同_id Doc（如果有）的完整字段和值（依赖_source字段），然后和请求中的Doc合并。同时，这里会获取到读到的Doc版本号，记做V1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parse Doc&lt;/p&gt;
&lt;p&gt;这里会解析Doc中各个字段。生成ParsedDocument对象，同时会生成uid Term。在Elasticsearch中，_uid = type #_id，对用户，_Id可见，而Elasticsearch中存储的是_uid。这一部分生成的ParsedDocument中也有Elasticsearch的系统字段，大部分会根据当前内容填充，部分未知的会在后面继续填充ParsedDocument&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update Mapping&lt;/p&gt;
&lt;p&gt;Elasticsearch中有个自动更新Mapping的功能，就在这一步生效。会先挑选出Mapping中未包含的新Field，然后判断是否运行自动更新Mapping，如果允许，则更新Mapping&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Get Sequence Id and Version&lt;/p&gt;
&lt;p&gt;由于当前是Primary Shard，则会从SequenceNumber Service获取一个sequenceID和Version。SequenceID在Shard级别每次递增1，SequenceID在写入Doc成功后，会用来初始化LocalCheckpoint。Version则是根据当前Doc的最大Version递增&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add Doc To Lucene&lt;/p&gt;
&lt;p&gt;这一步开始的时候会给特定_uid加锁，然后判断该_uid对应的Version是否等于之前Translate Update To Index步骤里获取到的Version，如果不相等，则说明刚才读取Doc后，该Doc发生了变化，出现了版本冲突，这时候会抛出一个VersionConflict的异常，该异常会在Primary Node最开始处捕获，重新从“Translate Update To Index or Delete”开始执行。 如果Version相等，则继续执行，如果已经存在同id的Doc，则会调用Lucene的UpdateDocument(uid, doc)接口，先根据uid删除Doc，然后再Index新Doc。如果是首次写入，则直接调用Lucene的AddDocument接口完成Doc的Index，AddDocument也是通过UpdateDocument实现。 这一步中有个问题是，如何保证Delete-Then-Add的原子性，怎么避免中间状态时被Refresh？答案是在开始Delete之前，会加一个Refresh Lock，禁止被Refresh，只有等Add完后释放了Refresh Lock后才能被Refresh，这样就保证了Delete-Then-Add的原子性。 Lucene的UpdateDocument接口中就只是处理多个Field，会遍历每个Field逐个处理，处理顺序是invert index，store field，doc values，point dimension，具体参考前文&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write Translog&lt;/p&gt;
&lt;p&gt;写完Lucene的Segment后，会以keyvalue的形式写TransLog，Key是_id，Value是Doc内容。当查询的时候，如果请求是GetDocByID，则可以直接根据_id从TransLog中读取到，满足NoSQL场景下的实时性要去。 需要注意的是，这里只是写入到内存的TransLog，是否Sync到磁盘的逻辑还在后面。 这一步的最后，会标记当前SequenceID已经成功执行，接着会更新当前Shard的LocalCheckPoint&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Renew Bulk Request&lt;/p&gt;
&lt;p&gt;这里会重新构造Bulk Request，原因是前面已经将UpdateRequest翻译成了Index或Delete请求，则后续所有Replica中只需要执行Index或Delete请求就可以了，不需要再执行Update逻辑，一是保证Replica中逻辑更简单，性能更好，二是保证同一个请求在Primary和Replica中的执行结果一样&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flush Translog&lt;/p&gt;
&lt;p&gt;这里会根据TransLog的策略，选择不同的执行方式，要么是立即Flush到磁盘，要么是等到以后再Flush。Flush的频率越高，可靠性越高，对写入性能影响越大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Send Requests To Replicas&lt;/p&gt;
&lt;p&gt;这里会将刚才构造的新的Bulk Request并行发送给多个Replica，然后等待Replica的返回，这里需要等待所有Replica返回后（可能有成功，也有可能失败），Primary Node才会返回用户。如果某个Replica失败了，则Primary会给Master发送一个Remove Shard请求，要求Master将该Replica Shard从可用节点中移除。 这里，同时会将SequenceID，PrimaryTerm，GlobalCheckPoint等传递给Replica。 发送给Replica的请求中，Action Name等于原始ActionName + [R]，这里的R表示Replica。通过这个[R]的不同，可以找到处理Replica请求的Handler&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Receive Response From Replicas&lt;/p&gt;
&lt;p&gt;Replica中请求都处理完后，会更新Primary Node的LocalCheckPoint&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;replica-node&#34;&gt;Replica Node&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Index or Delete 根据请求类型是Index还是Delete，选择不同的执行逻辑。这里没有Update，是因为在Primary Node中已经将Update转换成了Index或Delete请求了&lt;/li&gt;
&lt;li&gt;Parse Doc&lt;/li&gt;
&lt;li&gt;Update Mapping 以上都和Primary Node中逻辑一致&lt;/li&gt;
&lt;li&gt;Get Sequence Id and Version Primary Node中会生成Sequence ID和Version，然后放入ReplicaRequest中，这里只需要从Request中获取到就行&lt;/li&gt;
&lt;li&gt;Add Doc To Lucene 由于已经在Primary Node中将部分Update请求转换成了Index或Delete请求，这里只需要处理Index和Delete两种请求，不再需要处理Update请求了。比Primary Node会更简单一些&lt;/li&gt;
&lt;li&gt;Write Translog&lt;/li&gt;
&lt;li&gt;Flush Translog 以上都和Primary Node中逻辑一致。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-elasticsearch-文档搜索过程读&#34;&gt;2. ElasticSearch 文档搜索过程（读）&lt;/h2&gt;
&lt;h3 id=&#34;21-搜索过程拆解&#34;&gt;2.1 搜索过程拆解&lt;/h3&gt;
&lt;p&gt;几乎所有的搜索系统一般都是两阶段查询，第一阶段通过倒排索引等查询到匹配的DocID，第二阶段再查询DocID对应的完整文档，这种在Elasticsearch中称为query_then_fetch&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_pic_es_read_simple_process.png&#34; alt=&#34;图 1&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在初始&lt;strong&gt;查询阶段&lt;/strong&gt;时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列&lt;/li&gt;
&lt;li&gt;每个分片返回各自优先队列中 所有文档的 ID 和排序值给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。&lt;/li&gt;
&lt;li&gt;接下来就是&lt;strong&gt;取回阶段&lt;/strong&gt;，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并丰富文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;22-分布式搜索总结&#34;&gt;2.2 分布式搜索总结&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_five_pic_es_totalread_process.png&#34; alt=&#34;picture 6&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;search-coordination-node&#34;&gt;search coordination node&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Get Remote Cluster Shard&lt;/p&gt;
&lt;p&gt;判断是否需要跨集群访问，如果需要，则获取到要访问的Shard列表&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Get Search Shard Iterator&lt;/p&gt;
&lt;p&gt;获取当前Cluster中要访问的Shard，和上一步中的Remote Cluster Shard合并，构建出最终要访问的完整Shard列表。 这一步中，会根据Request请求中的参数从Primary Node和多个Replica Node中选择出一个要访问的Shard&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For Every Shard:Perform&lt;/p&gt;
&lt;p&gt;遍历每个Shard，对每个Shard执行后面逻辑&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Send Request To Query Shard&lt;/p&gt;
&lt;p&gt;将查询阶段请求发送给相应的Shard&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Merge Docs&lt;/p&gt;
&lt;p&gt;上一步将请求发送给多个Shard后，这一步就是异步等待返回结果，然后对结果合并。这里的合并策略是维护一个Top N大小的优先级队列，每当收到一个shard的返回，就把结果放入优先级队列做一次排序，直到所有的Shard都返回。 翻页逻辑也是在这里，如果需要取Top 30~ Top 40的结果，这个的意思是所有Shard查询结果中的第30到40的结果，那么在每个Shard中无法确定最终的结果，每个Shard需要返回Top 40的结果给Client Node，然后Client Node中在merge docs的时候，计算出Top 40的结果，最后再去除掉Top 30，剩余的10个结果就是需要的Top 30~ Top 40的结果。 上述翻页逻辑有一个明显的缺点就是每次Shard返回的数据中包括了已经翻过的历史结果，如果翻页很深，则在这里需要排序的Docs会很多，比如Shard有1000，取第9990到10000的结果，那么这次查询，Shard总共需要返回1000 * 10000，也就是一千万Doc，这种情况很容易导致OOM。 另一种翻页方式是使用search_after，这种方式会更轻量级，如果每次只需要返回10条结构，则每个Shard只需要返回search_after之后的10个结果即可，返回的总数据量只是和Shard个数以及本次需要的个数有关，和历史已读取的个数无关。这种方式更安全一些，推荐使用这种。 如果有aggregate，也会在这里做聚合，但是不同的aggregate类型的merge策略不一样&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Send Request To Fetch Shard&lt;/p&gt;
&lt;p&gt;选出Top N个Doc ID后发送给这些Doc ID所在的Shard执行Fetch Phase，最后会返回Top N的Doc的内容。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;query-phase&#34;&gt;Query Phase&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;第一阶段查询的步骤&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create Search Context&lt;/p&gt;
&lt;p&gt;创建Search Context，之后Search过程中的所有中间状态都会存在Context中，这些状态总共有50多个，具体可以查看DefaultSearchContext或者其他SearchContext的子类&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parse Query&lt;/p&gt;
&lt;p&gt;解析Query的Source，将结果存入Search Context。这里会根据请求中Query类型的不同创建不同的Query对象，比如TermQuery、FuzzyQuery等，最终真正执行TermQuery、FuzzyQuery等语义的地方是在Lucene中。 这里包括了dfsPhase、queryPhase和fetchPhase三个阶段的preProcess部分，只有queryPhase的preProcess中有执行逻辑，其他两个都是空逻辑，执行完preProcess后，所有需要的参数都会设置完成。 由于Elasticsearch中有些请求之间是相互关联的，并非独立的，比如scroll请求，所以这里同时会设置Context的生命周期。 同时会设置lowLevelCancellation是否打开，这个参数是集群级别配置，同时也能动态开关，打开后会在后面执行时做更多的检测，检测是否需要停止后续逻辑直接返回&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Get From Cache&lt;/p&gt;
&lt;p&gt;判断请求是否允许被Cache，如果允许，则检查Cache中是否已经有结果，如果有则直接读取Cache，如果没有则继续执行后续步骤，执行完后，再将结果加入Cache&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add Collectors Collector&lt;/p&gt;
&lt;p&gt;主要目标是收集查询结果，实现排序，对自定义结果集过滤和收集等。这一步会增加多个Collectors，多个Collector组成一个List。 FilteredCollector：先判断请求中是否有Post Filter，Post Filter用于Search，Agg等结束后再次对结果做Filter，希望Filter不影响Agg结果。如果有Post Filter则创建一个FilteredCollector，加入Collector List中。 PluginInMultiCollector：判断请求中是否制定了自定义的一些Collector，如果有，则创建后加入Collector List。 MinimumScoreCollector：判断请求中是否制定了最小分数阈值，如果指定了，则创建MinimumScoreCollector加入Collector List中，在后续收集结果时，会过滤掉得分小于最小分数的Doc。 EarlyTerminatingCollector：判断请求中是否提前结束Doc的Seek，如果是则创建EarlyTerminatingCollector，加入Collector List中。在后续Seek和收集Doc的过程中，当Seek的Doc数达到Early Terminating后会停止Seek后续倒排链。 CancellableCollector：判断当前操作是否可以被中断结束，比如是否已经超时等，如果是会抛出一个TaskCancelledException异常。该功能一般用来提前结束较长的查询请求，可以用来保护系统。 EarlyTerminatingSortingCollector：如果Index是排序的，那么可以提前结束对倒排链的Seek，相当于在一个排序递减链表上返回最大的N个值，只需要直接返回前N个值就可以了。这个Collector会加到Collector List的头部。EarlyTerminatingSorting和EarlyTerminating的区别是，EarlyTerminatingSorting是一种对结果无损伤的优化，而EarlyTerminating是有损的，人为掐断执行的优化。 TopDocsCollector：这个是最核心的Top N结果选择器，会加入到Collector List的头部。TopScoreDocCollector和TopFieldCollector都是TopDocsCollector的子类，TopScoreDocCollector会按照固定的方式算分，排序会按照分数+doc id的方式排列，如果多个doc的分数一样，先选择doc id小的文档。而TopFieldCollector则是根据用户指定的Field的值排序&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lucene Search&lt;/p&gt;
&lt;p&gt;这一步会调用Lucene中IndexSearch的search接口，执行真正的搜索逻辑。每个Shard中会有多个Segment，每个Segment对应一个LeafReaderContext，这里会遍历每个Segment，到每个Segment中去Search结果，然后计算分数。 搜索里面一般有两阶段算分，第一阶段是在这里算的，会对每个Seek到的Doc都计算分数，为了减少CPU消耗，一般是算一个基本分数。这一阶段完成后，会有个排序。然后在第二阶段，再对Top 的结果做一次二阶段算分，在二阶段算分的时候会考虑更多的因子。二阶段算分在后续操作中。 具体请求，比如TermQuery、WildcardQuery的查询逻辑都在Lucene中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rescore&lt;/p&gt;
&lt;p&gt;根据Request中是否包含rescore配置决定是否进行二阶段排序，如果有则执行二阶段算分逻辑，会考虑更多的算分因子。二阶段算分也是一种计算机中常见的多层设计，是一种资源消耗和效率的折中。 Elasticsearch中支持配置多个Rescore，这些rescore逻辑会顺序遍历执行。每个rescore内部会先按照请求参数window选择出Top window的doc，然后对这些doc排序，排完后再合并回原有的Top 结果顺序中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suggest Execute&lt;/p&gt;
&lt;p&gt;如果有推荐请求，则在这里执行推荐请求。如果请求中只包含了推荐的部分，则很多地方可以优化。推荐不是今天的重点，这里就不介绍了，后面有机会再介绍&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;aggregation::execute()&lt;/p&gt;
&lt;p&gt;如果含有聚合统计请求，则在这里执行。Elasticsearch中的aggregate的处理逻辑也类似于Search，通过多个Collector来实现。在Client Node中也需要对aggregation做合并。aggregate逻辑更复杂一些，就不在这里赘述了，后面有需要就再单独开文章介绍。 上述逻辑都执行完成后，如果当前查询请求只需要查询一个Shard，那么会直接在当前Node执行Fetch Phase&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;fetch-phase&#34;&gt;Fetch Phase&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;第二阶段Fetch的步骤&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Elasticsearch作为搜索系统时除了Query阶段外，还会有一个Fetch阶段，这个Fetch阶段在数据库类系统中是没有的。搜索系统中额外增加Fetch阶段的原因是搜索系统中数据分布导致的，在搜索中，数据通过routing分Shard的时候，只能根据一个主字段值来决定，但是查询的时候可能会根据其他非主字段查询，那么这个时候所有Shard中都可能会存在相同非主字段值的Doc，所以需要查询所有Shard才能不会出现结果遗漏。同时如果查询主字段，那么这个时候就能直接定位到Shard，就只需要查询特定Shard即可，这个时候就类似于数据库系统了。另外，数据库中的二级索引又是另外一种情况，但类似于查主字段的情况，这里就不多说了&lt;/p&gt;
&lt;p&gt;基于上述原因，第一阶段查询的时候并不知道最终结果会在哪个Shard上，所以每个Shard中都需要查询完整结果，比如需要Top 10，那么每个Shard都需要查询当前Shard的所有数据，找出当前Shard的Top 10，然后返回给Client Node。如果有100个Shard，那么就需要返回100 * 10 = 1000个结果，而Fetch Doc内容的操作比较耗费IO和CPU，如果在第一阶段就Fetch Doc，那么这个资源开销就会非常大。所以，一般是当Client Node选择出最终Top N的结果后，再对最终的Top N读取Doc内容。通过增加一点网络开销而避免大量IO和CPU操作，这个折中是非常划算的&lt;/p&gt;
&lt;p&gt;Fetch阶段的目的是通过DocID获取到用户需要的完整Doc内容。这些内容包括了DocValues，Store，Source，Script和Highlight等&lt;/p&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/34674517&#34;&gt;Elasticsearch内核解析&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/categories/distribute-storge/">distribute storge</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/elasticsearch/">elasticsearch</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/distribte-storage/">distribte storage</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/storage/">storage</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>捋一捋ElasticSearch（三）| Lucene存储引擎</title>
                <link>https://neteric.top/posts/elasticsearch_principle_four/</link>
                <guid isPermaLink="true">https://neteric.top/posts/elasticsearch_principle_four/</guid>
                <pubDate>Wed, 05 Oct 2022 10:21:24 &#43;0800</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;h2 id=&#34;1-背景&#34;&gt;1. 背景&lt;/h2&gt;
&lt;p&gt;Elasticsearch是一个基于Apache Lucene的开源搜索引擎，其依靠Lucene完成索引创建和搜索功能，可以将ElstaicSearch理解为是一个Lucene的分布式封装的搜索引擎。之所以ElasticSearch选用Lucene,是因为无论在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的全文搜索引擎库。Lucene非常复杂，你需要深入了解检索的相关知识才能完全理解它是如何工作的，本文只作提纲挈领的总结一下Lucene的相关知识和流程&lt;/p&gt;
&lt;h2 id=&#34;2-倒排索引&#34;&gt;2. 倒排索引&lt;/h2&gt;
&lt;h3 id=&#34;21-什么是倒排索引&#34;&gt;2.1 什么是倒排索引&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;在全文搜索中我们问的不只是“这个文档匹配查询吗”，而是“该文档匹配查询的程度有多大？”换句话说，该文档与给定查询的相关性如何？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;搜索的核心需求是全文检索，全文检索简单来说就是要在大量文档中找到包含某个单词出现的位置，在传统关系型数据库中，数据检索只能通过 like 来实现，例如需要在手机数据中查询名称包含苹果的手机，需要通过如下 sql 实现：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;select&lt;/span&gt; * from phone_table where phone_brand like &lt;span class=&#34;s1&#34;&gt;&amp;#39;%苹果手机%&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这种实现方式实际会存在很多问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无法使用数据库索引，需要全表扫描，性能差&lt;/li&gt;
&lt;li&gt;搜索效果差，只能首尾位模糊匹配，无法实现复杂的搜索需求，比如用户输出关键词是&lt;code&gt;平果手机&lt;/code&gt;，&lt;code&gt;苹果首机&lt;/code&gt;之类的，sql就匹配不到&lt;/li&gt;
&lt;li&gt;无法得到文档与搜索条件的相关性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;搜索的核心目标实际上是保证搜索的效果和性能，为了高效的实现全文检索，我们可以通过倒排索引来解决&lt;/p&gt;
&lt;p&gt;倒排索引是区别于正排索引的概念&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;正排索引：是以文档对象的唯一ID 作为索引，以文档内容作为记录的结构&lt;/li&gt;
&lt;li&gt;倒排索引：Inverted index，指的是将文档内容中的单词作为索引，将包含该词的文档 ID 作为记录的结构&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_four_pic_zhengpai_index_vs_inverted_index.png&#34; alt=&#34;picture 6&#34;&gt;&lt;/p&gt;
&lt;p&gt;有了倒排索引，能快速、灵活地实现各类搜索需求。整个搜索过程中我们不需要做任何文本的模糊匹配。&lt;/p&gt;
&lt;h3 id=&#34;22-lucene中倒排索引的结构&#34;&gt;2.2 Lucene中倒排索引的结构&lt;/h3&gt;
&lt;p&gt;根据倒排索引的概念，最简单的我们可以用一个Map来描述这个结构， Map 的 Key 存储分词后的单词（也叫Term），Map的Value存储一些列的文档ID的集合
但全文搜索引擎在海量数据的情况下需要存储大量的文本，如果是用Map存储Directory的话有这样几个明显的缺点&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Dictionary 是非常大的（比如我们搜索中的一个字段在Directory里可能有上千万个Term）因此存储所有的Term需要大量的内存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Postings 也可能会占据大量的存储空间（一个Term多的有几百万个doc）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Posting List 不仅仅需要包含文档的id，为了加快搜索的速度还需要包含更多信息，比如下面这些&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文档 id（DocId, Document Id），包含单词的所有文档唯一id，用于去正排索引中查询原始数据&lt;/li&gt;
&lt;li&gt;词频（TF，Term Frequency），记录 Term 在每篇文档中出现的次数，用于后续相关性算分&lt;/li&gt;
&lt;li&gt;位置（Position），记录 Term 在每篇文档中的分词位置（多个），用于做词语搜索（Phrase Query）&lt;/li&gt;
&lt;li&gt;偏移（Offset），记录 Term 在每篇文档的开始和结束位置，用于高亮显示等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此上面说的基于 Map 的实现方式几乎是不可行的。在海量数据背景下，倒排索引的实现直接关系到存储成本以及搜索性能，为此，Lucene 引入了多种巧妙的数据结构和算法，同时也把倒排索引的结构组织成如下图所示
&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_four_pic_inverted_index_structure.png&#34; alt=&#34;picture 7&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以看到Lucene的把上述“简单的Map”引拆成了三个主要的部分，Term Index是一个字典索引，保存了单词的前缀和其在Term Dictionary中位置的映射，Term Dictionary保存了所有的单词和倒排列表的映射，Posting List（也即是倒排表）保存了具体的文档id,词率和相关的元信息&lt;/p&gt;
&lt;h2 id=&#34;3-lucene核心数据结构&#34;&gt;3. Lucene核心数据结构&lt;/h2&gt;
&lt;h3 id=&#34;31-fst&#34;&gt;3.1 FST&lt;/h3&gt;
&lt;p&gt;FST 结构正是上述的字典索引，当文档数量越来越多时，Dictionary 中的 Term 也会越来越多，那查询效率必然也会逐渐变低，因此需要一个很好的数据结构为 Dictionary 建构一个索引，这就是 Terms Index(.tip文件)，Lucene 采用了 FST 这个数据结构来实现这个索引，FST即Finite State Transducer（有限状态转换器）它具备以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给定一个 Input 可以得到一个 output，用法类似于 HashMap&lt;/li&gt;
&lt;li&gt;共享前缀、后缀节省空间。FST 的内存消耗要比 HashMap 少很多&lt;/li&gt;
&lt;li&gt;词查找复杂度为 O(len(str))&lt;/li&gt;
&lt;li&gt;构建后不可变更&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下图为 mon/1，thrus/4，tues/2 生成的 FST，可以看到 thrus 和 tues 共享了前缀 t 以及后缀 s&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_four_pic_lucene_fst_case.png&#34; alt=&#34;图 8&#34;&gt;&lt;/p&gt;
&lt;p&gt;根据 FST 就可以将需要搜索 Term 作为 Input，对其途径的边上的值进行累加就可以得到 output，下述为以 input 为 thrus 的读取逻辑：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始状态0&lt;/li&gt;
&lt;li&gt;输入t， FST 从0 -&amp;gt; 3， output=2&lt;/li&gt;
&lt;li&gt;输入h，FST 从3 -&amp;gt; 4， output=2+2=4&lt;/li&gt;
&lt;li&gt;输入r， FST 从4 -&amp;gt; 5， output=4+0&lt;/li&gt;
&lt;li&gt;输入u，FST 从5 -&amp;gt; 7， output=4+0&lt;/li&gt;
&lt;li&gt;输入s， FST 到达终止节点，output=4+0=4&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;那么 Term Dictionary 生成的 FST 对应 input 和 output 是什么呢？&lt;/p&gt;
&lt;p&gt;实际上 FST 是通过 Dictionary 的每个 NodeBlock(下文会讲，也即Term Directory的位置指针) 的前缀构成，所以通过 FST 可以直接找到这个Term在Directory文件上具体的 File Pointer, 然后再遍历Directory文件中具有相同后缀的Entry进行查找&lt;/p&gt;
&lt;p&gt;因此FST在 Lucene 中具有以下功能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;快速试错，即在 FST 上找不到相关前缀可以直接跳出，不再需要遍历整个 Dictionary，作用类似于 BloomFilter&lt;/li&gt;
&lt;li&gt;通过 FST 可以直接计算出相关Term前缀在Term Directory文件中位置，起到快速定位的作用&lt;/li&gt;
&lt;li&gt;FST 也是一个 Automation(自动状态机)。这是正则表达式的一种实现方式，所以 FST 能提供正则表达式的能力&lt;/li&gt;
&lt;li&gt;通过 FST 能够极大的提高近似查询的性能，包括通配符查询、SpanQuery、PrefixQuery 等&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;更多FST内容可以参考[2]&lt;/p&gt;
&lt;h3 id=&#34;32-term-directory&#34;&gt;3.2 Term Directory&lt;/h3&gt;
&lt;p&gt;Terms Dictionary 通过 .tim 文件存储，负责存储所有的 Term 数据，同时它也是 Term 与 Postings 的关系纽带，存储了每个 Term 和其对应的 Postings 文件位置指针，如下图所示
&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_four_pic_term_directory_to_postings_list.png&#34; alt=&#34;图 6&#34;&gt;&lt;/p&gt;
&lt;p&gt;Terms Dictionary 内部采用 NodeBlock 这种结构对 Term 进行压缩存储，处理过程会将相同前缀的 Term 压缩为一个 NodeBlock，然后将每个 Term 的后缀以及对应 Term 的 Posting 关联信息处理为一个 Entry 保存到 Block，如下图所示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_four_pic_lucene_termdirectory_nodeblock.png&#34; alt=&#34;图 9&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;33-posting-list&#34;&gt;3.3 Posting List&lt;/h3&gt;
&lt;p&gt;PostingList 包含文档 id、词频、位置等多个信息，这些数据之间本身是相对独立的，因此 Lucene 将 Postings List 拆成三个文件存储：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.doc文件：记录 Postings 的 docId 信息和 Term 的词频&lt;/li&gt;
&lt;li&gt;.pay文件：记录 Payload 信息和偏移量信息&lt;/li&gt;
&lt;li&gt;.pos文件：记录位置信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个做法有没有点列式存储的味道？其好处也很明显：一来可以提高读取效率，因为基本所有的查询都会用 .doc 文件获取文档 id，但一般的查询仅需要用到 .doc 文件就足够了，只有对于近似查询等位置相关的查询才需要用位置相关数据， 二来这样存储数据很方便对数据进行压缩&lt;/p&gt;
&lt;p&gt;三个文件整体实现差不太多，这里仅介绍.doc 文件，.doc 文件存储的是每个 Term 对应的文档 Id 和词频。每个 Term 都包含一对 TermFreqs 和 SkipData 结构，其中 TermFreqs 存放 docId 和词频信息，SkipData 为跳表结构，用于实现 TermFreqs 内部的快速跳转&lt;/p&gt;
&lt;p&gt;Posting List 采用多个文件进行存储，最终我们可以得到每个 Term 的如下信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SkipOffset：用来描述当前 term 信息在 .doc 文件中跳表信息的起始位置。&lt;/li&gt;
&lt;li&gt;DocStartFP：是当前 term 信息在 .doc 文件中的文档 ID 与词频信息的起始位置。&lt;/li&gt;
&lt;li&gt;PosStartFP：是当前 term 信息在 .pos 文件中的起始位置。&lt;/li&gt;
&lt;li&gt;PayStartFP：是当前 term 信息在 .pay 文件中的起始位置。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-lucene的搜索&#34;&gt;4. Lucene的搜索&lt;/h2&gt;
&lt;h3 id=&#34;41-lucene的搜索过程&#34;&gt;4.1 Lucene的搜索过程&lt;/h3&gt;
&lt;p&gt;在介绍了索引表（Term Directory）和记录表（Posting List）的结构后，就可以得到 Lucene 倒排索引的查询步骤&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过 Term Index 数据（.tip文件）中的 StartFP 获取指定字段的 FST&lt;/li&gt;
&lt;li&gt;通过 FST 找到指定 Term 在 Term Dictionary（.tim 文件）可能存在的 Block&lt;/li&gt;
&lt;li&gt;将对应 Block 加载内存，遍历 Block 中的 Entry，通过后缀（Suffix）判断是否存在指定 Term&lt;/li&gt;
&lt;li&gt;存在则通过 Entry 的 TermStat 数据中各个文件的 FP 获取 Posting 数据&lt;/li&gt;
&lt;li&gt;如果需要获取 Term 对应的所有 DocId 则直接遍历 TermFreqs，如果获取指定 DocId 数据则通过 SkipData快速跳转&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_four_pic_lucene_search_process_total.png&#34; alt=&#34;图 7&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;42-lucene-搜索接口&#34;&gt;4.2 Lucene 搜索接口&lt;/h3&gt;
&lt;p&gt;虽然Lucene的整个索引的建立过程非常的复杂，但对用户暴露的访问接口却是非常的简单，这就是好项目的具体体现&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TopDocs&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Document&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;docID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;5-lucene的索引&#34;&gt;5. Lucene的索引&lt;/h2&gt;
&lt;h3 id=&#34;51-lucene的写入接口&#34;&gt;5.1 Lucene的写入接口&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// initialization
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Directory&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NIOFSDirectory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Paths&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/index&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;IndexWriterConfig&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IndexWriterConfig&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;IndexWriter&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;writer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IndexWriter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// create a document
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Document&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Document&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TextField&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Lucene - IndexWriter&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;YES&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StringField&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;author&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;aliyun&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;YES&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// index the document
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;writer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;addDocument&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;writer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;commit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;先看下Lucene中如何使用IndexWriter来写入数据，上面是一段精简的调用示例代码，整个过程主要有三个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化：初始化IndexWriter必要的两个元素是Directory和IndexWriterConfig，Directory是Lucene中数据持久层的抽象接口，通过这层接口可以实现很多不同类型的数据持久层，例如本地文件系统、网络文件系统、数据库或者是分布式文件系统。IndexWriterConfig内提供了很多可配置的高级参数，提供给高级玩家进行性能调优和功能定制&lt;/li&gt;
&lt;li&gt;构造文档：Lucene中文档由Document表示，Document由Field构成。Lucene提供多种不同类型的Field，其FiledType决定了它所支持的索引模式，当然也支持自定义Field&lt;/li&gt;
&lt;li&gt;写入文档：通过IndexWriter的addDocument函数写入文档，写入时同时根据FieldType创建不同的索引。文档写入完成后，还不可被搜索，最后需要调用IndexWriter的commit，在commit完后Lucene才保证文档被持久化并且是searchable的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lucene中写操作主要是通过IndexWriter类实现，IndexWriter提供如下一些接口&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;long&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;addDocument&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;long&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;updateDocuments&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;long&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;deleteDocuments&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;flush&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;long&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;prepareCommit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;long&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;commit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;long&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rollback&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forceMerge&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;maybeMerge&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;addDocument：比较纯粹的一个API，就是向Lucene内新增一个文档。Lucene内部没有主键索引，所有新增文档都会被认为一个新的文档，分配一个独立的docId。&lt;/li&gt;
&lt;li&gt;updateDocuments：更新文档，但是和数据库的更新不太一样。数据库的更新是查询后更新，Lucene的更新是查询后删除再新增。流程是先delete by term，后add document。但是这个流程又和直接先调用delete后调用add效果不一样，只有update能够保证在Thread内部删除和新增保证原子性，详细流程在下一章节会细说。&lt;/li&gt;
&lt;li&gt;deleteDocuments：删除文档，支持两种类型删除，by term和by query。在IndexWriter内部这两种删除的流程不太一样，在下一章节再细说。&lt;/li&gt;
&lt;li&gt;flush：触发强制flush，将所有Thread的In-memory buffer flush成segment文件，这个动作可以清理内存，强制对数据做持久化。&lt;/li&gt;
&lt;li&gt;prepareCommit/commit/rollback：commit后数据才可被搜索，commit是一个二阶段操作，prepareCommit是二阶段操作的第一个阶段，也可以通过调用commit一步完成，rollback提供了回滚到last commit的操作。&lt;/li&gt;
&lt;li&gt;maybeMerge/forceMerge：maybeMerge触发一次MergePolicy的判定，而forceMerge则触发一次强制merge。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过这些接口可以完成单个文档的写入，更新和删除功能，包括了分词，倒排创建，正排创建等等所有搜索相关的流程。只要Doc通过IndexWriter写入后，后面就可以通过IndexSearcher搜索了,但有一些问题Lucene没有解决&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上述操作是单机的，而不是我们需要的分布式&lt;/li&gt;
&lt;li&gt;文档写入Lucene后并不是立即可查询的，需要生成完整的Segment后才可被搜索，如何在实时性和可靠性之间取得平衡&lt;/li&gt;
&lt;li&gt;Lucene不支持部分文档更新，但是这又是一个强需求，如何支持部分更新&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;[1] &lt;a href=&#34;https://www.amazingkoala.com.cn/Lucene&#34;&gt;Lucene 源码分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&#34;https://www.shenyanchao.cn/blog/2018/12/04/lucene-fst/&#34;&gt;Lucene的词典FST深入剖析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] &lt;a href=&#34;https://whatua.com/2019/10/26/fst-tire-tree-%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/&#34;&gt;FST &amp;amp; tire-tree 应用场景&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[4] &lt;a href=&#34;https://en.wikipedia.org/wiki/K-d_tree&#34;&gt;KDTree wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[5] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/509892041&#34;&gt;Lucene Search 深入分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[6] &lt;a href=&#34;https://juejin.cn/post/6844903651878633479#heading-11&#34;&gt;Lucene 解析 - IndexWriter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/categories/distribute-storge/">distribute storge</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/elasticsearch/">elasticsearch</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/distribte-storage/">distribte storage</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/storage/">storage</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>捋一捋ElasticSearch（二）| 分布式特性</title>
                <link>https://neteric.top/posts/elasticsearch_principle_three/</link>
                <guid isPermaLink="true">https://neteric.top/posts/elasticsearch_principle_three/</guid>
                <pubDate>Tue, 04 Oct 2022 10:21:24 &#43;0800</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;blockquote&gt;
&lt;p&gt;前段时间学习过&lt;a href=&#34;https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/&#34;&gt;MIT6.284 分布式系统&lt;/a&gt;这门课程，该课程主要讲解分布式系统中一些设计背景和经典案例，正好工作需要详细了解下ElasticSearch的原理，于是就按照分布式系统需要考虑的问题来拆解下ElasticSearch在分布式特性方面有哪些考虑和设计，做过哪些权衡和取舍&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;1-分布式系统&#34;&gt;1. 分布式系统&lt;/h2&gt;
&lt;h3 id=&#34;11-设计分布式系统的驱动力&#34;&gt;1.1 设计分布式系统的驱动力&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;可扩展性（Scalability&lt;/strong&gt;：人们需要获得更高的计算性能。可以这么理解这一点，（大量的计算机意味着）大量的并行运算，大量CPU、大量内存、以及大量磁盘在并行的运行&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可用性（Availability）&lt;/strong&gt;：另一个人们构建分布式系统的原因是，它可以提供容错（tolerate faults）比如两台计算机运行完全相同的任务，其中一台发生故障，可以切换到另一台&lt;/li&gt;
&lt;li&gt;延迟考虑: 如果客户遍布世界各地，通常需要考虑在全球范围内部署服务，以方便用户就近访问最近数据中心所提供的服务，从而避免数据请求跨越了半个地球才能到达目标&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-数据分片&#34;&gt;2. 数据分片&lt;/h2&gt;
&lt;p&gt;海量的数据集，对系统的存储容量和查询能力都有很高的要求，分布式系统如何处理海量的数据请求并具有一定扩展性？&lt;/p&gt;
&lt;h3 id=&#34;21-为何要分片&#34;&gt;2.1 为何要分片&lt;/h3&gt;
&lt;p&gt;采用数据分片的主要目的是提高可扩展性。不同的分片可以放在一个无共享集群的不同节点上。这样一个大数据集可以分散在更多的磁盘上，查询负载也随之分布到更多的磁盘和处理器上。对单个分片进行查询时，每个节点对自己所在分片可以独立执行查询操作，因此添加更多的节点可以提高查询吞吐量。 超大而复杂的查询尽管比较困难，但也可能做到跨节点的并行处理&lt;/p&gt;
&lt;p&gt;es中分片是一个功能完整的搜索引擎（一个分片就是一个Lucene的实例）它拥有使用一个节点上的所有资源的能力&lt;/p&gt;
&lt;h3 id=&#34;22-怎么路由&#34;&gt;2.2 怎么路由&lt;/h3&gt;
&lt;p&gt;当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？
首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;shard = hash(routing) % number_of_primary_shards&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再对 number_of_primary_shards （主分片的数量）取模。这个分布在 0 到 number_of_primary_shards-1 之间的数，就是我们所寻求的文档所在分片的位置&lt;/p&gt;
&lt;p&gt;实际上，所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档（例如所有属于同一个用户的文档）都被存储到同一个分片中&lt;/p&gt;
&lt;h3 id=&#34;23-分片数量&#34;&gt;2.3 分片数量&lt;/h3&gt;
&lt;p&gt;主分片的数量在索引创建的时候指定，后期无法更改。实际上，这个数目定义了这个索引能够存储的最大数据量，也就是说一个索引的容量是有上限的，不能无限扩容。副本分片的数量随时可以修改，增加副本分片可以增加数据查询的能力&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少，你需要增加更多的硬件资源来提升吞吐量。但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去2个节点的情况下不丢失任何数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;24-elasticsearch-分片方式的局限性&#34;&gt;2.4 ElasticSearch 分片方式的局限性&lt;/h3&gt;
&lt;p&gt;上面我们讲了es分片路由的方式，相信聪明的你已经很快想到这种方式有一个非常严重的问题，就是如果number_of_primary_shards发生了变化，也就是一个索引的主分片的数量被扩大或者缩小的话，大量的文档到分片的映射关系将会发生改变。这就解释了为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目前为止你是不是觉得es为何设计的如此之low，为何不用一致性hash之类的解决方案呢？ 其实这只是在性能和便利性之间的一个折中罢了，es也是有办法可以进行主分片的扩容的，只是es的设计者认为主分片扩容是很伤性能的一件事，用户应该在前期就规划好单个索引的数据量来避免不必要的数据迁移&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html#:~:text=The%20split%20index%20API%20allows,is%20determined%20by%20the%20index.&#34;&gt;新版本es的split index解决方案&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-分片的复制&#34;&gt;3. 分片的复制&lt;/h2&gt;
&lt;h3 id=&#34;31-为何要复制&#34;&gt;3.1 为何要复制&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;当部分组件出现位障，系统依然可以继续工作，从而提高&lt;strong&gt;可用性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;扩展至多台机器以同时提供数据访问服务，从而提高读吞吐量&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-如何保证副本数据的一致性&#34;&gt;3.2 如何保证副本数据的一致性&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;我们知道在分布式系统中，数据复制一般有三种方式：主从复制，多主复制，无主复制，这些复制方式都有各种的优缺点，显然es用的是主从复制的思路，那么有当使用主从复制的时候，一个主副本会有1到多个从副本，这些副本之间的一致性如何保证呢？如果主几点挂了，是否会丢失数据呢？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;es在数据一致性问题方面做的非常灵活，用户可以针对单个文档的单次提交干预其存储行为，用户在提交文档进行索引的时候可以传入一个&lt;code&gt;wait_for_active_shards&lt;/code&gt;参数（es5之前叫&lt;code&gt;consistency&lt;/code&gt;），表示需要得到几个副本的写入成功响应后再返回，否则就等待或者重试，直到超时。默认情况下，&lt;code&gt;wait_for_active_shards=1&lt;/code&gt;, 可以通过设置 &lt;code&gt;index.write.wait_for_active_shards&lt;/code&gt; 在索引的setting中动态覆盖此默认值。也可以在每次提交参数中指定,类似下面这样
`&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;POST /my-index-000001/_doc?wait_for_active_shards&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;@timestamp&amp;#34;&lt;/span&gt;: &lt;span class=&#34;s2&#34;&gt;&amp;#34;2099-11-15T13:12:00&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;: &lt;span class=&#34;s2&#34;&gt;&amp;#34;111GET /search HTTP/1.1 200 1070000&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;: &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span class=&#34;s2&#34;&gt;&amp;#34;kimchy&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;写入操作响应的_shards部分显示复制成功/失败的分片副本数,如下所示&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;_shards&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;total&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;failed&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;successful&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;4-文档分布式索引过程文档写入过程&#34;&gt;4. 文档分布式索引过程（文档写入过程）&lt;/h2&gt;
&lt;h3 id=&#34;41-单个文档索引过程&#34;&gt;4.1 单个文档索引过程&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_three_pic_es_singledoc_write_process.png&#34; alt=&#34;图 2&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户端向任意节点发起请求(例图中node1),此时可以将node1叫做协调节点，协调节点负责根据文档-&amp;gt;分片的路由规则计算出主分片id&lt;/li&gt;
&lt;li&gt;协调节点将请求转发到主分片所在的node节点（例图中node3）的Lucene实例上&lt;/li&gt;
&lt;li&gt;执行主分片的写入操作,如果协调节点收到主分片写入成功的响应。则并行的执行写入副本操作（局部更新文档情况，会有retry_on_conflict逻辑）&lt;/li&gt;
&lt;li&gt;协调节点判断如果有wait_for_active_shards数量的副本分片都写入成功，协调节点讲返回给客户端ACK，否则等到timeout时间后返回给客户端超时响应&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;42-多个文档索引过程&#34;&gt;4.2 多个文档索引过程&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_three_pic_es_mutidoc_write_process.png&#34; alt=&#34;图 3&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户端向任意节点发起bulk请求(例图中node1),此时可以将node1叫做协调节点，协调节点负责根据文档-&amp;gt;分片的路由规则计算出主分片id&lt;/li&gt;
&lt;li&gt;协调节点(例图中node1) 向每个索引所在的节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机&lt;/li&gt;
&lt;li&gt;主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;bulk API 还可以在整个批量请求的最顶层使用 wait_for_active_shards 参数，以及在每个请求中的元数据中使用 routing 参数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;5-文档分布式取回fetch过程&#34;&gt;5. 文档分布式取回（fetch）过程&lt;/h2&gt;
&lt;p&gt;协调节点可以从主分片或者任意副本分片检索文档&lt;/p&gt;
&lt;h3 id=&#34;51-单个文档的fetch过程&#34;&gt;5.1 单个文档的fetch过程&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_three_pic_es_singledoc_read_process.png&#34; alt=&#34;picture 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;以下是从主分片或者副本分片检索文档的步骤顺序：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;客户端向任意节点发起请求(例图中node1),此时可以将node1叫做协调节点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;协调节点负责根据文档-&amp;gt;分片的路由规则计算出主分片id（例如分片0），分片0的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到node2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;node2 将文档返回给协调节点（node1） ，然后协调节点将文档返回给客户端&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡
在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 只是所有副本都写入成功了过后，文档在主分片和副本分片才都是可用的&lt;/p&gt;
&lt;h3 id=&#34;52-多个文档的fetch过程&#34;&gt;5.2 多个文档的fetch过程&lt;/h3&gt;
&lt;p&gt;多文档索引使用bulk API，类似的多文档fetch使用mget API, 区别在于协调节点知道每个文档存在哪个分片中。 它将整个多文档请求分解成每个分片的多文档请求，并且将这些请求并行转发到每个参与节点。&lt;/p&gt;
&lt;p&gt;以下是使用单个 mget 请求取回多个文档所需的步骤顺序：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户端向任意节点发起mget请求(例图中node1),此时可以将node1叫做协调节点,协调节点将多个请求的文档按照所在的分片进行分类&lt;/li&gt;
&lt;li&gt;协调节点向每个分片构建多文档获取请求，然后并行转发这些请求到每个主分片或者副本分片所在的节点上。一旦收到所有答复， 协调节点构建响应并将其返回给客户端。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;当然，也可以对个文档设置 routing 参数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_three_pic_es_mutidoc_read_process.png&#34; alt=&#34;picture 2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;6-集群发现机制&#34;&gt;6. 集群发现机制&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;集群发现机制包括：集群启动，节点发现，master选举，并在每次集群状态发生变化时发布集群状态&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;参见&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html&#34;&gt;官方文档&lt;/a&gt;&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/categories/distribute-storge/">distribute storge</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/elasticsearch/">elasticsearch</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/distribte-storage/">distribte storage</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/storage/">storage</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>捋一捋ElasticSearch（一）| 基本使用</title>
                <link>https://neteric.top/posts/elasticsearch_principle_one/</link>
                <guid isPermaLink="true">https://neteric.top/posts/elasticsearch_principle_one/</guid>
                <pubDate>Sun, 02 Oct 2022 10:21:24 &#43;0800</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;blockquote&gt;
&lt;p&gt;罗列一些DSL的基本使用，后续遇到相关问题可以完善和查询使用,本文的用法，可以参考&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html&#34;&gt;官方文档&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;-1-索引管理&#34;&gt;📌 1. 索引管理&lt;/h2&gt;
&lt;h3 id=&#34;11-索引介绍&#34;&gt;1.1 索引介绍&lt;/h3&gt;
&lt;h3 id=&#34;12-创建索引&#34;&gt;1.2 创建索引&lt;/h3&gt;
&lt;p&gt;创建名字为&lt;code&gt;product&lt;/code&gt;的索引，并指定id为1的文档的内容&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;PUT /product/_doc/1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span class=&#34;s2&#34;&gt;&amp;#34;computer&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_one_create_index.png&#34; alt=&#34;create_index&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;13-修改索引&#34;&gt;1.3 修改索引&lt;/h3&gt;
&lt;h3 id=&#34;14-打开关闭索引&#34;&gt;1.4 打开/关闭索引&lt;/h3&gt;
&lt;h3 id=&#34;15-删除索引&#34;&gt;1.5 删除索引&lt;/h3&gt;
&lt;h3 id=&#34;16-查看索引&#34;&gt;1.6 查看索引&lt;/h3&gt;
&lt;h3 id=&#34;17-批量索引文档&#34;&gt;1.7 批量索引文档&lt;/h3&gt;
&lt;h3 id=&#34;18-索引设置&#34;&gt;1.8 索引设置&lt;/h3&gt;
&lt;p&gt;在源码中找到测试数据并下载&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl https://github.com/elastic/elasticsearch/blob/v6.8.19/docs/src/test/resources/accounts.json
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将下载到的accounts.json文件批量导入到EleasticSearch中&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -H &amp;#34;Content-Type: application/json&amp;#34; -XPOST &amp;#34;username:password@localhost:9200/bank/_bulk?pretty&amp;amp;refresh&amp;#34; --data-binary &amp;#34;@./accounts.json&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;-2-文档管理&#34;&gt;📜 2. 文档管理&lt;/h2&gt;
&lt;h2 id=&#34;-3搜索&#34;&gt;🔎 3.搜索&lt;/h2&gt;
&lt;h3 id=&#34;31-查询所有&#34;&gt;3.1 查询所有&lt;/h3&gt;
&lt;p&gt;查询index名为bank的数据，&lt;code&gt;match_all&lt;/code&gt;表示查询所有的数据，&lt;code&gt;sort&lt;/code&gt;即按照什么字段排序&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GET /bank/_search
{
  &amp;#34;query&amp;#34;: { &amp;#34;match_all&amp;#34;: {} },
  &amp;#34;sort&amp;#34;: [
    { &amp;#34;account_number&amp;#34;: &amp;#34;desc&amp;#34; }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查询结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_one_bank_search_all.png&#34; alt=&#34;picture 23&#34;&gt;&lt;/p&gt;
&lt;p&gt;相关字段解释：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  - took – Elasticsearch运行查询所花费的时间（以毫秒为单位） 
  - timed_out –搜索请求是否超时 
  - _shards - 搜索了多少个分片，成功、失败或跳过了多少个分片
  - max_score – 找到的最相关文档的分数 
  - hits.total.value - 找到了多少个匹配的文档 
  - hits._score - 文档的相关性得分（使用match_all时不适用）
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;32-分页查询&#34;&gt;3.2 分页查询&lt;/h3&gt;
&lt;p&gt;从第&lt;code&gt;3&lt;/code&gt;页记录开始查询，每页&lt;code&gt;5&lt;/code&gt;条数据,与Sql语句中的offset和limit类似&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GET /bank/_search
{
  &amp;#34;query&amp;#34;: { &amp;#34;match_all&amp;#34;: {} },
  &amp;#34;sort&amp;#34;: [
    { &amp;#34;account_number&amp;#34;: &amp;#34;asc&amp;#34; }
  ],
  &amp;#34;from&amp;#34;: 10,
  &amp;#34;size&amp;#34;: 5
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;33-指定字段查询&#34;&gt;3.3 指定字段查询&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GET /bank/_search
{
  &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;address&amp;#34;: &amp;#34;mill lane&amp;#34; } }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;34-多条件查询&#34;&gt;3.4 多条件查询&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/elasticsearch_principle_one_search_by_field.png&#34; alt=&#34;picture 24&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;35-聚合查询aggregation&#34;&gt;3.5 聚合查询（Aggregation）&lt;/h3&gt;
&lt;p&gt;在SQL中有group by，在ES中它叫Aggregation，即聚合运算&lt;/p&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html&#34;&gt;官方文档&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/categories/distribute-storge/">distribute storge</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/elasticsearch/">elasticsearch</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/distribte-storage/">distribte storage</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/storage/">storage</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>存储引擎对比</title>
                <link>https://neteric.top/posts/storage-engine-compare/</link>
                <guid isPermaLink="true">https://neteric.top/posts/storage-engine-compare/</guid>
                <pubDate>Thu, 18 Aug 2022 16:10:03 &#43;0800</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;h3 id=&#34;硬盘结构&#34;&gt;硬盘结构&lt;/h3&gt;
&lt;p&gt;硬盘内部主要部件为磁盘盘片、传动手臂、读写磁头和主轴马达。实际数据都是写在盘片上，读写主要是通过传动手臂上的读写磁头来完成。实际运行时，主轴让磁盘盘片转动，然后传动手臂可伸展让读取头在盘片上进行读写操作。磁盘物理结构如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://info.mrtlab.com/uploadfile/201305/14/104000249.JPG&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;由于单一盘片容量有限，一般硬盘都有两张以上的盘片，每个盘片有两面，都可记录信息，所以一张盘片对应着两个磁头。盘片被分为许多扇形的区域，每个区域叫一个扇区，硬盘中每个扇区的大小固定为512字节。盘片表面上以盘片中心为圆心，不同半径的同心圆称为磁道，不同盘片相同半径的磁道所组成的圆柱称为柱面。磁道与柱面都是表示不同半径的圆，在许多场合，磁道和柱面可以互换使用。磁盘盘片垂直视角如下图所示&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://info.mrtlab.com/uploadfile/201305/14/1040344565.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;影响硬盘性能的因素&#34;&gt;影响硬盘性能的因素&lt;/h3&gt;
&lt;p&gt;影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I/O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;寻道时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tseek是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3-15ms。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;旋转延迟&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trotation是指盘片旋转将请求数据所在的扇区移动到读写磁盘下方所需要的时间。旋转延迟取决于磁盘转速，通常用磁盘旋转一周所需时间的1/2表示。比如：7200rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000rpm的磁盘其平均旋转延迟为2ms。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据传输时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ttransfer是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率，数据传输时间通常远小于前两部分消耗时间。简单计算时可忽略。&lt;/p&gt;
&lt;p&gt;结论：**机械硬盘的连续读写性能很好，但随机读写性能很差，**这主要是因为磁头移动到正确的磁道上需要时间，随机读写时，磁头需要不停的移动，时间都浪费在了磁头寻址上，所以性能不高&lt;/p&gt;
&lt;h3 id=&#34;linux-io流程&#34;&gt;Linux IO流程&lt;/h3&gt;
&lt;p&gt;类似于网络的分层结构，下图显示了Linux系统中对于磁盘的一次读请求在核心空间中所要经历的层次模型。从图中看出：对于磁盘的一次读请求，首先经过虚拟文件系统层（VFS Layer），其次是具体的文件系统层（例如Ext2），接下来是Cache层（Page Cache Layer）、通用块层（Generic Block Layer）、I/O调度层（I/O Scheduler Layer）、块设备驱动层（Block Device Driver Layer），最后是物理块设备层（Block Device Layer）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/storage_engine_compare/fig01.png&#34; alt=&#34;image-20220819105617539&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.thomas-krenn.com/de/wikiDE/images/e/e0/Linux-storage-stack-diagram_v4.10.png&#34;&gt;kernel 4.10 linux io stack&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;结论： &lt;strong&gt;一次硬盘读取IO路径非常长，要经历用户态到内核态的上下文切换，IO调度，块设备驱动，中断等逻辑&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;存储分类&#34;&gt;存储分类&lt;/h3&gt;
&lt;h4 id=&#34;结构化数据&#34;&gt;结构化数据&lt;/h4&gt;
&lt;p&gt;可以从名称中看出，是高度组织和整齐格式化的数据。结构化数据可以轻易放入表格和电子表格中的数据类型，典型的比如使用关系型数据库表示和存储，表现为二维形式的数据。一般特点是：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的&lt;/p&gt;
&lt;h5 id=&#34;结构化数据的存储&#34;&gt;结构化数据的存储&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;关系型数据库：如 MySQL、PostreSQL、Oracle、SQLServer， 其组成一般包括： 连接器，解析器，优化器，执行器，&lt;strong&gt;存储引擎&lt;/strong&gt;， 管理硬盘的单机文件系统&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;半结构化数据&#34;&gt;半结构化数据&lt;/h4&gt;
&lt;p&gt;半结构化数据是结构化数据的一种形式，它并不符合关系型数据库或其他数据表的形式关联起来的数据模型结构，但包含相关标记，用来分隔语义元素以及对记录和字段进行分层。因此，它也被称为自描述的结构。常见的半结构数据格式有XML、JSON、Yaml&lt;/p&gt;
&lt;h5 id=&#34;半结构化数据的存储&#34;&gt;半结构化数据的存储&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;K/V存储：Rocksdb、LevelDB、BlotDB 其组成一般包括：连接器，优化器，&lt;strong&gt;存储引擎&lt;/strong&gt;，单机文件系统&lt;/li&gt;
&lt;li&gt;文档存储： MongoDB、ElasticSearch&lt;/li&gt;
&lt;li&gt;列式存储（也叫表格存储）： ClickHouse、Apache Druid 、Hbase&lt;/li&gt;
&lt;li&gt;内存K/V存储： Redis、Memcache&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;非结构化数据&#34;&gt;非结构化数据&lt;/h4&gt;
&lt;p&gt;数据结构不规则或不完整，没有预定义的数据模型，不方便用数据库二维逻辑表来表现的数据。本质上是结构化数据之外的一切数据，如：全文文本、图片、音频、影视、超媒体等信息&lt;/p&gt;
&lt;h5 id=&#34;非结构化数据的存储&#34;&gt;非结构化数据的存储&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;单机文件系统： 如 ext2/3/4, xfs, btrfs, bluefs,ntfs, fat32&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分布式块/对象/文件存储：如 ceph, hdfs, glusterfs, MinIO，这类非结构化数据的存储又叫Blob存储，其组成部分一般都包括：管理硬盘的单机文件系统 ，分布式特性的实现（分片，复制，一致性），元数据管理系统（元数据一般都是半结构化数据），接口实现（块，对象，文件）可参考前面的技术分享：&lt;a href=&#34;https://duodian.feishu.cn/file/boxcnPObkzRjzgF1mfrwJnzbMPc&#34;&gt;https://duodian.feishu.cn/file/boxcnPObkzRjzgF1mfrwJnzbMPc&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;存储使用场景&#34;&gt;存储使用场景&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;属性&lt;/th&gt;
&lt;th&gt;OLTP&lt;/th&gt;
&lt;th&gt;OLAP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;主要读取模式&lt;/td&gt;
&lt;td&gt;小数据量的随机读，通过 key 查询&lt;/td&gt;
&lt;td&gt;大数据量的聚合（max,min,sum, avg）查询&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;主要写入模式&lt;/td&gt;
&lt;td&gt;随机访问，低延迟写入&lt;/td&gt;
&lt;td&gt;批量导入（ETL）或者流式写入&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;主要应用场景&lt;/td&gt;
&lt;td&gt;通过 web 方式使用的最终用户&lt;/td&gt;
&lt;td&gt;互联网分析，为了辅助决策&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;如何看待数据&lt;/td&gt;
&lt;td&gt;当前时间点的最新状态&lt;/td&gt;
&lt;td&gt;随着时间推移的&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;数据量&lt;/td&gt;
&lt;td&gt;单次查询和总体都相对较小，总体通常 GB 到 TB&lt;/td&gt;
&lt;td&gt;单次查询和总体都相对巨大，总体通常 TB 到 PB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;请求数量&lt;/td&gt;
&lt;td&gt;相对频繁，侧重在线交易&lt;/td&gt;
&lt;td&gt;相对较少，侧重离线分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;瓶颈&lt;/td&gt;
&lt;td&gt;Disk Seek&lt;/td&gt;
&lt;td&gt;Disk Bandwidth&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;存储格式&lt;/td&gt;
&lt;td&gt;多用行存&lt;/td&gt;
&lt;td&gt;列存逐渐流行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;用户&lt;/td&gt;
&lt;td&gt;比较普遍，一般应用用的比较多&lt;/td&gt;
&lt;td&gt;多为商业用户&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;场景举例&lt;/td&gt;
&lt;td&gt;银行交易&lt;/td&gt;
&lt;td&gt;商业分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;产品举例&lt;/td&gt;
&lt;td&gt;MySQL&lt;/td&gt;
&lt;td&gt;ClickHouse&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;lsm-tree的起源&#34;&gt;LSM Tree的起源&lt;/h2&gt;
&lt;h3 id=&#34;世界上最简单的数据库&#34;&gt;世界上最简单的数据库&lt;/h3&gt;
&lt;p&gt;首先来看，世界上“最简单”的数据库，由两个 Bash 函数构成：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;db_set &lt;span class=&#34;o&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$2&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &amp;gt;&amp;gt; database
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;db_get &lt;span class=&#34;o&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; grep &lt;span class=&#34;s2&#34;&gt;&amp;#34;^&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;,&amp;#34;&lt;/span&gt; database &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sed -e &lt;span class=&#34;s2&#34;&gt;&amp;#34;s/^&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;,//&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; tail -n &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这两个函数实现了一个基于字符串的 KV 存储（只支持 get/set，不支持 delete）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ db_set &lt;span class=&#34;m&#34;&gt;123456&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;{&amp;#34;name&amp;#34;:&amp;#34;London&amp;#34;,&amp;#34;attractions&amp;#34;:[&amp;#34;BigBen&amp;#34;,&amp;#34;LondonEye&amp;#34;]}&amp;#39;&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ db_set &lt;span class=&#34;m&#34;&gt;42&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;{&amp;#34;name&amp;#34;:&amp;#34;SanFrancisco&amp;#34;,&amp;#34;attractions&amp;#34;:[&amp;#34;GoldenGateBridge&amp;#34;]}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ db_get &lt;span class=&#34;m&#34;&gt;42&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span class=&#34;s2&#34;&gt;&amp;#34;San Francisco&amp;#34;&lt;/span&gt;,&lt;span class=&#34;s2&#34;&gt;&amp;#34;attractions&amp;#34;&lt;/span&gt;:&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Golden Gate Bridge&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;来分析下它为什么 work，也反映了日志结构存储的最基本原理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;set：在文件末尾追加一个 KV 对。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;get：匹配所有 Key，返回最后（也即最新）一条 KV 对中的 Value&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新操作： 使用set在文件末尾追加一条该key的新记录&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;删除操作： 使用set在文件末尾追加一条该key的墓碑记录，当查询到墓碑记录代表这个key已删除&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;引入哈希索引&#34;&gt;引入哈希索引&lt;/h3&gt;
&lt;p&gt;可以看出，上述“世界上最简单的数据库”写入的速度非常快，几乎是数据库能做到的极限。但是读需要全文逐行扫描，会慢很多，如果数据量达到百GB级别，该数据库的读变得几乎不可用，为了加快读，我们需要构建&lt;strong&gt;索引&lt;/strong&gt;（一种允许基于某些字段查找的额外数据结构）&lt;/p&gt;
&lt;p&gt;索引从原数据中构建，只为加快查找。因此索引会耗费一定额外空间，和插入时间（每次插入要更新索引），即，重新以空间和写换读取。&lt;/p&gt;
&lt;p&gt;依上小节的例子，所有数据顺序追加到磁盘上。为了加快查询，我们在内存中构建一个哈希索引：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Key 是查询 Key&lt;/li&gt;
&lt;li&gt;Value 是 KV 条目的起始位置和长度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/ch03-fig01.png&#34; alt=&#34;ddia-3-1-hash-map-csv.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;看起来很简单，但这正是 &lt;a href=&#34;https://docs.riak.com/riak/kv/2.2.3/setup/planning/backend/bitcask/index.html&#34; title=&#34;Bitcask&#34;&gt;Bitcask&lt;/a&gt; 的基本设计，关键是他能很好的 Work（在小数据量时，即所有 key 都能存到内存中时）能提供很高的读写性能&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;写：文件追加写。&lt;/li&gt;
&lt;li&gt;读：一次内存查询，一次磁盘 seek；如果数据已经被缓存，则 seek 也可以省掉。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果你的 key 集合很小（意味着能全放内存），但是每个 key 更新很频繁，那么 Bitcask 便是你的菜。举个栗子：频繁更新的视频播放量，key 是视频 url，value 是视频播放量&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;但有个很重要问题，单个文件越来越大，磁盘空间不够怎么办？&lt;/p&gt;
&lt;p&gt;在文件到达一定尺寸后，就新建一个文件，将原文件变为只读。同时为了回收多个 key 多次写入的造成的空间浪费，可以将只读文件进行紧缩（ compact ），将旧文件进行重写，挤出“水分”（被覆写的数据）以进行垃圾回收。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/ch03-fig03.png&#34; alt=&#34;ddia-3-3-compaction-sim.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;当然，如果我们想让其&lt;strong&gt;工业可用&lt;/strong&gt;，还有很多问题需要解决：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;文件格式&lt;/strong&gt;。对于&lt;strong&gt;日志&lt;/strong&gt;来说，CSV 不是一种紧凑的数据格式，有很多空间浪费。比如，可以用 length + record bytes 。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;记录删除&lt;/strong&gt;。之前只支持 put\get，但实际还需要支持 delete。但日志结构又不支持更新，怎么办呢？一般是写一个特殊标记（比如墓碑记录，tombstone）以表示该记录已删除。之后 compact 时真正删除即可。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;宕机恢复&lt;/strong&gt;。在机器重启时，内存中的哈希索引将会丢失。当然，可以全盘扫描以重建，但通常一个小优化是，对于每个 segment file， 将其索引条目和数据文件一块持久化，重启时只需加载索引条目即可。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;记录写坏、少写&lt;/strong&gt;。系统任何时候都有可能宕机，由此会造成记录写坏、少写。为了识别错误记录，我们需要增加一些校验字段，以识别并跳过这种数据。为了跳过写了部分的数据，还要用一些特殊字符来标识记录间的边界。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;并发控制&lt;/strong&gt;。由于只有一个活动（追加）文件，因此写只有一个天然并发度。但其他的文件都是不可变的（compact 时会读取然后生成新的），因此读取和紧缩可以并发执行。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;乍一看，基于日志的存储结构存在着不少浪费：需要追加进行更新和删除。但日志结构有几个原地更新结构无法做的优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;以顺序写代替随机写&lt;/strong&gt;。对于磁盘和 SSD，顺序写都要比随机写快几个数量级。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;简易的并发控制&lt;/strong&gt;。由于大部分的文件都是不可变（immutable）的，因此更容易做并发读取和紧缩。也不用担心原地更新会造成新老数据交替。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更少的内部碎片&lt;/strong&gt;。每次紧缩会将垃圾完全挤出。但是原地更新就会在 page 中留下一些不可用空间&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;3-sstable&#34;&gt;3. SSTable&lt;/h3&gt;
&lt;p&gt;上一节讲的基于内存的哈希索引也有其局限：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;所有 Key 必须放内存&lt;/strong&gt;：一旦 Key 的数据量超过内存大小，这种方案便不再 work。当然你可以设计基于磁盘的哈希表，但那又会带来大量的随机写&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不支持范围查询&lt;/strong&gt;：由于 key 是无序的，不能简单的支持扫描kitty001到kitty999区间内的所有key，只能采用逐个查找的方式查询每个key&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;compact效率低&lt;/strong&gt;：日志文件compact需要遍历文件内容，效率低下&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有这些局限的主要原因是在外存上的数据是简单追加写而形成的，并没有按照某个字段有序。假设加一个限制，让这些文件按 key 有序。我们称这种格式为：SSTable（Sorted String Table）。&lt;/p&gt;
&lt;p&gt;这种文件格式有什么优点呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;高效的数据文件合并&lt;/strong&gt;。即有序文件的归并外排，顺序读，顺序写&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/ch03-fig04.png&#34; alt=&#34;ddia-3-4-merge-sst.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;不需要在内存中保存所有数据的索引&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;仅需要记录下每个文件界限（以区间表示：[startKey, endKey]，当然实际会记录的更细）即可。查找某个 Key 时，去所有包含该 Key 的区间对应的文件二分查找即可（也即是&lt;strong&gt;稀疏索引&lt;/strong&gt;）在文件中查找特定的key时，不再需要在内存中保存所有键的索引。 如下图假设正在查找handiwork ，且不知道该键在段文件中的确切偏移。但是，如果知道handbag 和handsome 的偏移 ，考虑到根据键排序，则键handiwork一定位于它们两者之间。这意味着可以跳到handbag 的偏移，从那里开始扫描，直到找到hαndiwork,如果找到handsome都还没找到则可以断定这个文件中没有handiwork这个key的记录&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/16/j8tM6IUk1QrJXuw.png&#34; alt=&#34;ddia-3-5-sst-index.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;分块压缩，节省空间，减少 IO&lt;/strong&gt;。相邻 Key 共享前缀，既然每次都要批量取，那正好一组 key batch 到一块，称为 block，且只记录 block 的索引。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;构建和维护-sstables&#34;&gt;构建和维护 SSTables&lt;/h4&gt;
&lt;p&gt;SSTables 格式听起来很美好，但须知数据是乱序的来的，我们如何得到有序的数据文件呢？&lt;/p&gt;
&lt;p&gt;这可以拆解为两个小问题：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;构建 SSTable 文件&lt;/strong&gt;。将乱序数据在外存（磁盘 or SSD）中上整理为有序文件，是比较难的。但是在内存就方便的多。于是一个大胆的想法就形成了：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在内存中维护一个有序结构（称为 &lt;strong&gt;MemTable&lt;/strong&gt;）。典型实现如：红黑树、AVL 树、跳表。&lt;/li&gt;
&lt;li&gt;到达一定阈值之后全量 dump 到外存。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;维护 SSTable 文件&lt;/strong&gt;。为什么需要维护呢？首先要问，对于上述复合结构，我们怎么进行查询：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先去 MemTable 中查找，如果命中则返回。&lt;/li&gt;
&lt;li&gt;再去 SSTable 按时间顺序由新到旧逐一查找。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果 SSTable 文件越来越多，则查找代价会越来越大。因此需要将多个 SSTable 文件合并，以减少文件数量，同时进行 GC，我们称之为&lt;strong&gt;紧缩&lt;/strong&gt;（ Compaction）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;该方案的问题&lt;/strong&gt;：如果出现宕机，内存中的数据结构将会消失。 解决方法也很经典：WAL。&lt;/p&gt;
&lt;h3 id=&#34;从-sstables-到-lsm-tree&#34;&gt;从 SSTables 到 LSM-Tree&lt;/h3&gt;
&lt;p&gt;将前面几节的一些碎片有机的组织起来，便是时下流行的存储引擎 LevelDB 和 RocksDB 后面的存储结构：LSM-Tree&lt;/p&gt;
&lt;p&gt;这种数据结构是 Patrick O’Neil 等人，在 1996 年提出的：&lt;a href=&#34;https://www.cs.umb.edu/~poneil/lsmtree.pdf&#34; title=&#34;The Log-Structured Merge-Tree&#34;&gt;The Log-Structured Merge-Tree&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Elasticsearch 和 Solr 的索引引擎 Lucene，也使用类似 LSM-Tree 存储结构。但其数据模型不是 KV，但类似：word → document list。&lt;/p&gt;
&lt;h5 id=&#34;lsm-tree的性能优化&#34;&gt;LSM-Tree的性能优化&lt;/h5&gt;
&lt;p&gt;如果想让一个引擎工程上可用，还会做大量的性能优化。对于 LSM-Tree 来说，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;优化 SSTable 的查找&lt;/strong&gt; ：常用 &lt;a href=&#34;https://www.qtmuniao.com/2020/11/18/leveldb-data-structures-bloom-filter/&#34;&gt;&lt;strong&gt;Bloom Filter&lt;/strong&gt;&lt;/a&gt;。该数据结构可以使用较少的内存为每个 SSTable 做一些指纹，起到一些初筛的作用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;层级化组织 SSTable&lt;/strong&gt;：以控制 Compaction 的顺序和时间。常见的有 size-tiered 和 leveled compaction。LevelDB 便是支持后者而得名。前者比较简单粗暴，后者性能更好，也因此更为常见。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/ch03-sized-tiered.png&#34; alt=&#34;ddia-sized-tierd-compact.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于 RocksDB 来说，工程上的优化和使用上的优化就更多了。在其 &lt;a href=&#34;https://github.com/facebook/rocksdb/wiki&#34; title=&#34;rocksdb wiki&#34;&gt;Wiki&lt;/a&gt; 上随便摘录几点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Column Family&lt;/li&gt;
&lt;li&gt;前缀压缩和过滤&lt;/li&gt;
&lt;li&gt;键值分离，BlobDB&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但无论有多少变种和优化，LSM-Tree 的核心思想——&lt;strong&gt;保存一组合理组织、后台合并的 SSTables&lt;/strong&gt; ——简约而强大。可以方便的进行范围遍历，可以变大量随机为少量顺序。&lt;/p&gt;
&lt;h2 id=&#34;另一个世界-b-族树&#34;&gt;另一个世界-B 族树&lt;/h2&gt;
&lt;p&gt;虽然先讲的 LSM-Tree，但是它要比 B+ 树新的多。B 树于 1970 年被 R. Bayer and E. McCreight &lt;a href=&#34;https://dl.acm.org/doi/10.1145/1734663.1734671&#34; title=&#34;b tree paper&#34;&gt;提出&lt;/a&gt;后，便迅速流行了起来。现在几乎所有的关系型数据中，它都是数据索引标准一般的实现。与 LSM-Tree 一样，它也支持高效的&lt;strong&gt;点查&lt;/strong&gt;和&lt;strong&gt;范围查&lt;/strong&gt;，但却使用了完全不同的组织方式&lt;/p&gt;
&lt;h3 id=&#34;起源-顺序存储的scheme&#34;&gt;起源-顺序存储的Scheme&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/fig98.png&#34; alt=&#34;image-20220817200550307&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示的二维表scheme,我们可以把行数据顺序的记录到外存（序列化过后）上？ 这样记录插入速度也是可用做的非常快的，但顺序写入过后要查找和更新就相对比较慢了，在计算机科学中，用于加快查找的数据结构自然就是树，那能不能把二维表数据以树的形式在硬盘上组织呢？B+树的发展过程其实是面向硬盘特性编程的思考过程，下面一一道来！&lt;/p&gt;
&lt;h3 id=&#34;二叉查找树&#34;&gt;二叉查找树&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/fig99.png&#34; alt=&#34;5fLd8bp0nu&#34;&gt;&lt;/p&gt;
&lt;p&gt;从图中可以看到，我们为 product 表(产品库存表)建立了一个二叉查找树的索引。&lt;/p&gt;
&lt;p&gt;二叉查找树的特点就是任何节点的左子节点的键值都小于当前节点的键值，右子节点的键值都大于当前节点的键值。顶端的节点我们称为根节点，没有子节点的节点我们称之为叶节点。&lt;/p&gt;
&lt;p&gt;如果我们需要查找 id=5 的产品信息，利用我们创建的二叉查找树索引，查找流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将根节点作为当前节点，把 5与当前节点的键值 4比较，5大于 4，接下来我们把当前节点&amp;gt;的右子节点作为当前节点。&lt;/li&gt;
&lt;li&gt;继续把 5和当前节点的键值 6 比较，发现 5小于 6，把当前节点的左子节点作为当前节点。&lt;/li&gt;
&lt;li&gt;把 5 和当前节点的键值 5对比，5等于 5，满足条件，我们从当前节点中取出 data，即 id=5，name=book, num=899。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用二叉查找树我们只需要 3 次即可找到匹配的数据。如果在表中一条条的查找的话，我们需要 5次才能找到&lt;/p&gt;
&lt;h3 id=&#34;平衡二叉查找树&#34;&gt;平衡二叉查找树&lt;/h3&gt;
&lt;p&gt;上面我们讲解了利用二叉查找树可以快速的找到数据。但是，如果上面的二叉查找树是这样的构造：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/fig100.png&#34; alt=&#34;image-20220817202250598&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个时候可以看到我们的二叉查找树变成了一个链表。如果我们需要查找 id=7 的产品信息，我们需要查找 7 次，也就相当于全表扫描了。导致这个现象的原因其实是二叉查找树变得不平衡了，也就是高度太高了，从而导致查找效率的不稳定，为了解决这个问题，我们需要保证二叉查找树一直保持平衡，就需要用到平衡二叉树了。&lt;/p&gt;
&lt;p&gt;平衡二叉树又称 AVL 树，在满足二叉查找树特性的基础上，要求每个节点的左右子树的高度差不能超过 1。&lt;/p&gt;
&lt;p&gt;下面是平衡二叉树和非平衡二叉树的对比：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/fig101.png&#34; alt=&#34;image-20220817203243292&#34;&gt;&lt;/p&gt;
&lt;p&gt;由平衡二叉树的构造我们可以发现第一张图中的二叉树其实就是一棵平衡二叉树。&lt;/p&gt;
&lt;p&gt;平衡二叉树保证了树的构造是平衡的，当我们插入或删除数据导致不满足平衡二叉树不平衡时，平衡二叉树会进行调整树上的节点来保持平衡。具体的调整方式这里就不介绍了。平衡二叉树相比于二叉查找树来说，查找效率更稳定，总体的查找速度也更快。&lt;/p&gt;
&lt;p&gt;除了平衡二叉查找树，还有很多自平衡的二叉树，比如红黑树，它也是通过一些约束条件来达到自平衡，不过红黑树的约束条件比较复杂，不是本篇的重点，大家可以看《数据结构》相关的书籍来了解红黑树的约束条件。&lt;/p&gt;
&lt;h3 id=&#34;b-树&#34;&gt;B 树&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;不管平衡二叉查找树还是红黑树，都会随着插入的元素增多，而导致树的高度变高，这就意味着磁盘 I/O 操作次数多，会影响整体数据查询的效率&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如，下面这个平衡二叉查找树的高度为 5，那么在访问最底部的节点时，就需要磁盘 5 次 I/O 操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://img-blog.csdnimg.cn/img_convert/2d26d30c953cd47c6ab637ad0eba2f99.png&#34; alt=&#34;图片&#34;&gt;&lt;/p&gt;
&lt;p&gt;根本原因是因为它们都是二叉树，也就是每个节点只能保存 2 个子节点 ，如果我们把二叉树改成 M 叉树（M&amp;gt;2）呢？&lt;/p&gt;
&lt;p&gt;比如，当 M=3 时，在同样的节点个数情况下，三叉树比二叉树的树要矮&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://img-blog.csdnimg.cn/img_convert/00fb73de7014a87958f1597345e9ef2f.png&#34; alt=&#34;图片&#34;&gt;&lt;/p&gt;
&lt;p&gt;因此，&lt;strong&gt;当树的节点越多的时候，并且树的分叉数 M 越大的时候，M 叉树的高度会远小于二叉树的高度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;自平衡二叉树虽然能保持查询操作的时间复杂度在O(logn)，但是因为它本质上是一个二叉树，每个节点只能有 2 个子节点，那么当节点个数越多的时候，树的高度也会相应变高，这样就会增加磁盘的 I/O 次数，从而影响数据查询的效率。为了解决树的高度的问题，后面就出来了 B 树，它不再限制一个节点就只能有 2 个子节点，而是允许 M 个子节点 (M&amp;gt;2)，从而降低树的高度。B 树的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶，所以 B 树就是一个多叉树。
B树相对平衡二叉树在节点空间的利用率上进行改进，B树在每个节点保存更多的数据，减少了树的高度，从而提升了查找的性能，在数据库应用中，B树的每个节点存储的数据量大约为4K, 这是因为考虑到磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次对磁盘进行IO数据读取时，同一个磁盘块的数据会被一次性读取出来，所以每一次磁盘IO都可以读取到B树中一个节点的全部数据。、&lt;/p&gt;
&lt;h3 id=&#34;b-树-1&#34;&gt;B+ 树&lt;/h3&gt;
&lt;p&gt;B+ 树就是对 B 树做了一个升级，MySQL 中索引的数据结构就是采用了 B+ 树，B+ 树结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://img-blog.csdnimg.cn/img_convert/b6678c667053a356f46fc5691d2f5878.png&#34; alt=&#34;图片&#34;&gt;&lt;/p&gt;
&lt;p&gt;B+ 树与 B 树差异的点，主要是以下这几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；&lt;/li&gt;
&lt;li&gt;所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；&lt;/li&gt;
&lt;li&gt;非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）。&lt;/li&gt;
&lt;li&gt;非叶子节点中有多少个子节点，就有多少个索引；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;查找&lt;/strong&gt;。从根节点出发，进行二分查找，然后加载新的页到内存中，继续二分，直到命中或者到叶子节点。 查找复杂度，树的高度—— O(lgn)，影响树高度的因素：分支因子（分叉数，通常是几百个）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;插入 or 更新&lt;/strong&gt;。和查找过程一样，定位到原 Key 所在页，插入或者更新后，将页完整写回。如果页剩余空间不够，则分裂后写入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分裂 or 合并&lt;/strong&gt;。级联分裂和合并。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一个记录大于一个 page 怎么办？&lt;/p&gt;
&lt;p&gt;树的节点是逻辑概念，page or block 是物理概念。一个逻辑节点可以对应多个物理 page。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;b树优化&#34;&gt;B+树优化&lt;/h3&gt;
&lt;p&gt;B+树不像 LSM-Tree ，会在原地修改数据文件。&lt;/p&gt;
&lt;p&gt;在树结构调整时，可能会级联修改很多 Page。比如叶子节点分裂后，就需要写入两个新的叶子节点，和一个父节点（更新叶子指针）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加预写日志（WAL），将所有修改操作记录下来，预防宕机时中断树结构调整而产生的混乱现场。&lt;/li&gt;
&lt;li&gt;使用 latch 对树结构进行并发控制。&lt;/li&gt;
&lt;li&gt;不使用 WAL，而在写入时利用 Copy On Write 技术。同时，也方便了并发控制。如 LMDB、BoltDB。&lt;/li&gt;
&lt;li&gt;对中间节点的 Key 做压缩，保留足够的路由信息即可。以此，可以节省空间，增大分支因子。&lt;/li&gt;
&lt;li&gt;为了优化范围查询，有的 B 族树将叶子节点存储时物理连续。但当数据不断插入时，维护此有序性的代价非常大。&lt;/li&gt;
&lt;li&gt;为叶子节点增加兄弟指针，以避免顺序遍历时的回溯。即 B+ 树的做法，但远不局限于 B+ 树。&lt;/li&gt;
&lt;li&gt;B 树的变种，分形树，从 LSM-tree 借鉴了一些思想以优化 seek。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;b-trees-和-lsm-trees-对比&#34;&gt;B-Trees 和 LSM-Trees 对比&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;存储引擎&lt;/th&gt;
&lt;th&gt;B-Tree&lt;/th&gt;
&lt;th&gt;LSM-Tree&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;优势&lt;/td&gt;
&lt;td&gt;读取更快&lt;/td&gt;
&lt;td&gt;写入更快&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;写放大&lt;/td&gt;
&lt;td&gt;1. 数据和 WAL&lt;br/&gt;2. 更改数据时多次覆盖整个 Page&lt;/td&gt;
&lt;td&gt;1. 数据和 WAL&lt;br/&gt;2. Compaction&lt;/td&gt;
&lt;td&gt;SSD 不能过多擦除。因此 SSD 内部的固件中也多用日志结构来减少随机小写。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;写吞吐&lt;/td&gt;
&lt;td&gt;相对较低：&lt;br/&gt;1. 大量随机写。&lt;/td&gt;
&lt;td&gt;相对较高：&lt;br/&gt;1. 较低的写放大（取决于数据和配置）&lt;br/&gt;2. 顺序写入。&lt;br/&gt;3. 更为紧凑。&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;压缩率&lt;/td&gt;
&lt;td&gt;1. 存在较多内部碎片。&lt;/td&gt;
&lt;td&gt;1. 更加紧凑，没有内部碎片。&lt;br/&gt;2. 压缩潜力更大（共享前缀）。&lt;/td&gt;
&lt;td&gt;但紧缩不及时会造成 LSM-Tree 存在很多垃圾&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;后台流量&lt;/td&gt;
&lt;td&gt;1. 更稳定可预测，不会受后台 compaction 突发流量影响。&lt;/td&gt;
&lt;td&gt;1. 写吞吐过高，compaction 跟不上，会进一步加重读放大。&lt;br/&gt;2. 由于外存总带宽有限，compaction 会影响读写吞吐。&lt;br/&gt;3. 随着数据越来越多，compaction 对正常写影响越来越大。&lt;/td&gt;
&lt;td&gt;RocksDB 写入太过快会引起 write stall，即限制写入，以期尽快 compaction 将数据下沉。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;存储放大&lt;/td&gt;
&lt;td&gt;1. 有些 Page 没有用满&lt;/td&gt;
&lt;td&gt;1. 同一个 Key 存多遍&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;并发控制&lt;/td&gt;
&lt;td&gt;1. 同一个 Key 只存在一个地方&lt;br/&gt;2. 树结构容易加范围锁。&lt;/td&gt;
&lt;td&gt;同一个 Key 会存多遍，一般使用 MVCC 进行控制。&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;存储引擎的其他应用&#34;&gt;存储引擎的其他应用&lt;/h2&gt;
&lt;h3 id=&#34;全文索引和模糊索引&#34;&gt;&lt;strong&gt;全文索引和模糊索引&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;前述索引只提供全字段的精确匹配，而不提供类似搜索引擎的功能。比如，按字符串中包含的单词查询，针对笔误的单词查询。&lt;/p&gt;
&lt;p&gt;在工程中常用 &lt;a href=&#34;https://lucene.apache.org/&#34; title=&#34;Apace Lucene&#34;&gt;Apace Lucene&lt;/a&gt; 库，和其包装出来的服务：&lt;a href=&#34;https://www.elastic.co/cn/&#34; title=&#34;Elasticsearch&#34;&gt;Elasticsearch&lt;/a&gt;。他也使用类似 LSM-tree 的日志存储结构，但其索引是一个有限状态自动机，在行为上类似 Trie 树。&lt;/p&gt;
&lt;h3 id=&#34;全内存数据结构&#34;&gt;全内存数据结构&lt;/h3&gt;
&lt;p&gt;随着单位内存成本下降，甚至支持持久化（non-volatile memory，NVM，如 Intel 的 &lt;a href=&#34;https://www.intel.cn/content/www/cn/zh/products/details/memory-storage/optane-dc-persistent-memory.html&#34; title=&#34;傲腾&#34;&gt;傲腾&lt;/a&gt;），全内存数据库也逐渐开始流行。&lt;/p&gt;
&lt;p&gt;根据是否需要持久化，内存数据大概可以分为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;不需要持久化&lt;/strong&gt;。如只用于缓存的 Memcached。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要持久化&lt;/strong&gt;。通过 WAL、定期 snapshot、远程备份等等来对数据进行持久化。但使用内存处理全部读写，因此仍是内存数据库。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;VoltDB, MemSQL, and Oracle TimesTen 是提供关系模型的内存数据库。RAMCloud 是提供持久化保证的 KV 数据库。Redis and Couchbase 仅提供弱持久化保证。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;内存数据库存在优势的原因不仅在于不需要读取磁盘，而在更于不需要对数据结构进行&lt;strong&gt;序列化、编码&lt;/strong&gt;后以适应磁盘所带来的&lt;strong&gt;额外开销&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当然，内存数据库还有以下优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;提供更丰富的数据抽象&lt;/strong&gt;。如 set 和 queue 这种只存在于内存中的数据抽象。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现相对简单&lt;/strong&gt;。因为所有数据都在内存中。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，内存数据库还可以通过类似操作系统 swap 的方式，提供比物理机内存更大的存储空间，但由于其有更多数据库相关信息，可以将换入换出的粒度做的更细、性能做的更好。&lt;/p&gt;
&lt;p&gt;基于&lt;strong&gt;非易失性存储器&lt;/strong&gt;（non-volatile memory，NVM） 的存储引擎也是这些年研究的一个热点。&lt;/p&gt;
&lt;h3 id=&#34;列存&#34;&gt;列存&lt;/h3&gt;
&lt;p&gt;在OLAP的场景下，数据有可能达到数十亿行和数 PB 大。虽然事实表可能通常有几十上百列，但是单次查询通常只关注其中几个维度（列）。&lt;/p&gt;
&lt;p&gt;如查询&lt;strong&gt;人们是否更倾向于在一周的某一天购买新鲜水果或糖果&lt;/strong&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weekday&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;category&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;SUM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fact_sales&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;quantity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;AS&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;quantity_sold&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fact_sales&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;JOIN&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_date&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;ON&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fact_sales&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date_key&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date_key&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;JOIN&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_product&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;ON&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fact_sales&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;product_sk&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;product_sk&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2013&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;AND&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;category&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;IN&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Fresh fruit&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Candy&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;GROUP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;BY&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weekday&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim_product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;category&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;由于传统数据库通常是按行存储的，这意味着对于属性（列）很多的表，哪怕只查询一个属性，也必须从磁盘上取出很多属性，无疑浪费了 IO 带宽、增大了读放大。&lt;/p&gt;
&lt;p&gt;于是一个很自然的想法呼之欲出：每一个列分开存储好不好？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/ch03-fig10.png&#34; alt=&#34;ddia-3-10-store-by-column.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;不同列之间同一个行的字段可以通过下标来对应。当然也可以内嵌主键来对应，但那样存储成本就太高了。&lt;/p&gt;
&lt;h4 id=&#34;列压缩&#34;&gt;列压缩&lt;/h4&gt;
&lt;p&gt;将所有数据分列存储在一块，带来了一个意外的好处，由于同一属性的数据相似度高，因此更易压缩。&lt;/p&gt;
&lt;p&gt;如果每一列中值域相比行数要小的多，可以用&lt;strong&gt;位图编码（ &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bitmap_index&#34; title=&#34;bitmap encoding&#34;&gt;bitmap encoding&lt;/a&gt;&lt;/em&gt; ）&lt;/strong&gt;。举个例子，零售商可能有数十亿的销售交易，但只有 100,000 个不同的产品。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/ch03-fig11.png&#34; alt=&#34;ddia-3-11-compress.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中，是一个列分片中的数据，可以看出只有 {29, 30, 31, 68, 69, 74} 六个离散值。针对每个值出现的位置，我们使用一个 bit array 来表示：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;bit map 下标对应列的下标&lt;/li&gt;
&lt;li&gt;值为 0 则表示该下标没有出现该值&lt;/li&gt;
&lt;li&gt;值为 1 则表示该下标出现了该值&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果 bit array 是稀疏的，即大量的都是 0，只要少量的 1。其实还可以使用 &lt;strong&gt;&lt;a href=&#34;https://zh.wikipedia.org/zh/%E6%B8%B8%E7%A8%8B%E7%BC%96%E7%A0%81&#34; title=&#34;游程编码&#34;&gt;游程编码&lt;/a&gt;（RLE， Run-length encoding）&lt;/strong&gt; 进一步压缩：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将连续的 0 和 1，改写成 &lt;code&gt;数量+值&lt;/code&gt;，比如 &lt;code&gt;product_sk = 29&lt;/code&gt; 是 &lt;code&gt;9 个 0，1 个 1，8 个 0&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;使用一个小技巧，将信息进一步压缩。比如将同值项合并后，肯定是 0 1 交错出现，固定第一个值为 0，则交错出现的 0 和 1 的值也不用写了。则 &lt;code&gt;product_sk = 29&lt;/code&gt;  编码变成 &lt;code&gt;9，1，8&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;由于我们知道 bit array 长度，则最后一个数字也可以省掉，因为它可以通过 &lt;code&gt;array len - sum(other lens)&lt;/code&gt; 得到，则 &lt;code&gt;product_sk = 29&lt;/code&gt;  的编码最后变成：&lt;code&gt;9，1&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;位图索引很适合应对查询中的逻辑运算条件，比如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;product_sk&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;IN&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;68&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;69&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;可以转换为 &lt;code&gt;product_sk = 30&lt;/code&gt;、&lt;code&gt;product_sk = 68&lt;/code&gt;和 &lt;code&gt;product_sk = 69&lt;/code&gt;这三个 bit array 按位或（OR）。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;product_sk&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;AND&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;store_sk&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;可以转换为 &lt;code&gt;product_sk = 31&lt;/code&gt;和 &lt;code&gt;store_sk = 3&lt;/code&gt;的 bit array 的按位与，就可以得到所有需要的位置。&lt;/p&gt;
&lt;h4 id=&#34;内存带宽和向量化处理&#34;&gt;内存带宽和向量化处理&lt;/h4&gt;
&lt;p&gt;数仓的超大规模数据量带来了以下瓶颈：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;内存处理带宽&lt;/li&gt;
&lt;li&gt;CPU 分支预测错误和&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%81%9C%E9%A1%BF&#34; title=&#34;流水线停顿&#34;&gt;流水线停顿&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;关于内存的瓶颈可已通过前述的数据压缩来缓解。对于 CPU 的瓶颈可以使用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;列式存储和压缩可以让数据尽可能多地缓存在 L1 中，结合位图存储进行快速处理。&lt;/li&gt;
&lt;li&gt;使用 SIMD 用更少的时钟周期处理更多的数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;列式存储的排序&#34;&gt;列式存储的排序&lt;/h4&gt;
&lt;p&gt;由于数仓查询多集中于聚合算子（比如 sum，avg，min，max），列式存储中的存储顺序相对不重要。但也免不了需要对某些列利用条件进行筛选，为此我们可以如 LSM-Tree 一样，对所有行按某一列进行排序后存储。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注意，不可能同时对多列进行排序。因为我们需要维护多列间的下标间的对应关系，才可能按行取数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;同时，排序后的那一列，压缩效果会更好。&lt;/p&gt;
&lt;h4 id=&#34;不同副本不同排序&#34;&gt;不同副本，不同排序&lt;/h4&gt;
&lt;p&gt;在分布式数据库（数仓这么大，通常是分布式的）中，同一份数据我们会存储多份。对于每一份数据，我们可以按不同列有序存储。这样，针对不同的查询需求，便可以路由到不同的副本上做处理。当然，这样也最多只能建立副本数（通常是 3 个左右）列索引。&lt;/p&gt;
&lt;p&gt;这一想法由 C-Store 引入，并且为商业数据仓库 Vertica 采用。&lt;/p&gt;
&lt;h4 id=&#34;列式存储的写入&#34;&gt;列式存储的写入&lt;/h4&gt;
&lt;p&gt;上述针对数仓的优化（列式存储、数据压缩和按列排序）都是为了解决数仓中常见的读写负载，读多写少，且读取都是超大规模的数据。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我们针对读做了优化，就让写入变得相对困难。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;比如 B 树的&lt;strong&gt;原地更新流&lt;/strong&gt;是不太行的。举个例子，要在中间某行插入一个数据，&lt;strong&gt;纵向&lt;/strong&gt;来说，会影响所有的列文件（如果不做 segment 的话）；为了保证多列间按下标对应，&lt;strong&gt;横向&lt;/strong&gt;来说，又得更新该行不同列的所有列文件。&lt;/p&gt;
&lt;p&gt;所幸我们有 LSM-Tree 的追加流。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将新写入的数据在&lt;strong&gt;内存&lt;/strong&gt;中 Batch 好，按行按列，选什么数据结构可以看需求。&lt;/li&gt;
&lt;li&gt;然后达到一定阈值后，批量刷到&lt;strong&gt;外存&lt;/strong&gt;，并与老数据合并。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;数仓 Vertica 就是这么做的。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在存储引擎领域，没有真正的银弹，纵观各种现在的存储引擎的存储解决方案，都是在读和写之间做权衡（trade off）。恰当的&lt;strong&gt;存储格式&lt;/strong&gt;能加快写（比如LSM Tree），但是会让读取很慢；也可以加快读（查找树、B族树），但会让写入较慢。为了弥补读性能，可以构建索引，但是会牺牲写入性能和耗费额外空间。只有合理的硬盘存储结构和内存数据结构（索引）的结合，才能发挥出特定的效果，解决特定的问题。&lt;/p&gt;
&lt;p&gt;参考文献&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;《数据敏感型应用设计》&lt;/li&gt;
&lt;li&gt;《MySQL技术内幕 InnoDB存储引擎》第二版&lt;/li&gt;
&lt;li&gt;《ElasticSearch 权威指南》&lt;/li&gt;
&lt;li&gt;《Lucene 原理与源码分析》&lt;/li&gt;
&lt;li&gt;《大规模分布式存储系统：原理解析与架构实战》&lt;/li&gt;
&lt;li&gt;《Ceph设计原理与实现》&lt;/li&gt;
&lt;li&gt;《Ceph源码分析》&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lrita.github.io/images/posts/filesystem/Linux.Kernel.IO.Scheduler.pdf&#34;&gt;https://lrita.github.io/images/posts/filesystem/Linux.Kernel.IO.Scheduler.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tech.meituan.com/2017/05/19/about-desk-io.html&#34;&gt;https://tech.meituan.com/2017/05/19/about-desk-io.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://info.mrtlab.com/picdoc/Hard-disk-structure.html&#34;&gt;http://info.mrtlab.com/picdoc/Hard-disk-structure.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/categories/%E5%AD%98%E5%82%A8/">存储</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/b-tree/">B&#43;Tree</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/lsm-tree/">LSM Tree</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/log-structed/">Log Structed</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/linux-io/">Linux IO</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/disk-struct/">Disk Struct</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>关于负载均衡</title>
                <link>https://neteric.top/posts/about_load_balance/</link>
                <guid isPermaLink="true">https://neteric.top/posts/about_load_balance/</guid>
                <pubDate>Sat, 29 Feb 2020 17:22:54 &#43;0800</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;h3 id=&#34;为何需要负载均衡&#34;&gt;为何需要负载均衡？&lt;/h3&gt;
&lt;p&gt;业务初期，我们一般会先使用单台服务器对外提供服务。随着业务流量越来越大，单台服务器无论如何优化，无论采用多好的硬件，总会有性能天花板，当单服务器的性能无法满足业务，自然就想到把多台服务器组成集群来提高请求处理能力。集群必须要以统一的入口方式对外提供服务，所以需要一个 &lt;strong&gt;流量调度器&lt;/strong&gt; ，通过均衡的算法将用户大量的请求均衡地分发到后端集群不同的服务器上&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;负载平衡&lt;/strong&gt;（Load balancing）是一种&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA&#34;&gt;计算机&lt;/a&gt;技术，用来在多个计算机（&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%9B%86%E7%BE%A4&#34;&gt;计算机集群&lt;/a&gt;）、网络连接、CPU、磁盘驱动器或其他资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、同时避免过载的目的。 使用带有负载平衡的多个服务器组件，取代单一的组件，可以通过&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E5%86%97%E9%A4%98&#34;&gt;冗余&lt;/a&gt;提高可靠性。负载平衡服务通常是由专用软件和硬件来完成。 主要作用是将大量作业合理地分摊到多个操作单元上进行执行，用于解决互联网架构中的&lt;strong&gt;高并发&lt;/strong&gt;和&lt;strong&gt;高可用&lt;/strong&gt;的问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用负载均衡带来的好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提高了系统的整体性能&lt;/li&gt;
&lt;li&gt;提高了系统的扩展性&lt;/li&gt;
&lt;li&gt;提高了系统的可靠性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;负载均衡器从广义上可以分为 3 类：DNS 实现负载均衡、硬件负载均衡、软件负载均衡&lt;/p&gt;
&lt;h4 id=&#34;dns-实现负载均衡&#34;&gt;DNS 实现负载均衡&lt;/h4&gt;
&lt;p&gt;一个域名通过 DNS 解析到多个 IP，每个 IP 对应不同的服务器实例，这样就完成了流量的调度，虽然没有使用常规的负载均衡器，但也的确完成了负载均衡的功能。&lt;/p&gt;
&lt;p&gt;通过 DNS 实现负载均衡的方式，最大的优点就是实现简单，成本低，无需自己开发或维护负载均衡设备，不过存在一些缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;服务器故障切换延迟大，服务器升级不方便&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们知道 DNS 与用户之间是层层的缓存，即便是在故障发生时及时通过 DNS 修改或摘除故障服务器，但中间由于经过运营商的 DNS 缓存，且缓存很有可能不遵循 TTL 规则，导致 DNS 生效时间变得非常缓慢，有时候一天后还会有些许的请求流量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;流量调度不均衡，粒度太粗&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DNS 调度的均衡性，受地区运营商 LocalDNS 返回 IP 列表的策略有关系，有的运营商并不会轮询返回多个不同的 IP 地址。另外，某个运营商 LocalDNS 背后服务了多少用户，这也会构成流量调度不均的重要因素。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;流量分配策略比较简单，支持的算法较少&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DNS 一般只支持 RR 的轮询方式，流量分配策略比较简单，不支持权重、Hash 等调度算法。
DNS 支持的 IP 列表有限制。我们知道 DNS 使用 UDP 报文进行信息传递，每个 UDP 报文大小受链路的 MTU 限制，所以报文中存储的 IP 地址数量也是非常有限的，阿里 DNS 系统针对同一个域名支持配置 10 个不同的 IP 地址。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;一些大公司一般也会利用 DNS 来实现地理级别的负载均衡，实现就近访问，提高访问速度，这种方式一般是入口流量的基础负载均衡，下层会有更专业的负载均衡设备实现的负载架构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;硬件负载均衡&#34;&gt;硬件负载均衡&lt;/h4&gt;
&lt;p&gt;硬件负载均衡是通过专门的硬件设备来实现负载均衡功能，类似于交换机、路由器，是一个负载均衡专用的网络设备。目前业界典型的硬件负载均衡设备有两款：F5 和 A10。这类设备性能强劲、功能强大，但价格非常昂贵，一般只有 “土豪” 公司才会使用此类设备，普通业务量级的公司一般负担不起，二是业务量没那么大，用这些设备也是浪费。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;硬件负载均衡的优点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能强大&lt;/strong&gt;：全面支持各层级的负载均衡，支持全面的负载均衡算法。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;性能强大&lt;/strong&gt;：性能远超常见的软件负载均衡器。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;稳定性高&lt;/strong&gt;：商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;安全防护&lt;/strong&gt;：除了具备负载均衡外，还具备防火墙、防 DDoS 攻击等安全功能，貌似还支持 SNAT 功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;硬件负载均衡的缺点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;价格昂贵&lt;/li&gt;
&lt;li&gt;扩展性差，无法进行扩展和定制。&lt;/li&gt;
&lt;li&gt;调试和维护比较麻烦，需要专业人员。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;软件负载均衡&#34;&gt;软件负载均衡&lt;/h4&gt;
&lt;p&gt;软件负载均衡，可以在普通的服务器上运行负载均衡软件，实现负载均衡功能。目前常见的有 Nginx、HAproxy、LVS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nginx&lt;/strong&gt; ：是 7 层负载均衡，支持 HTTP、E-mail 协议，貌似也支持 4 层负载均衡了。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HAproxy&lt;/strong&gt;：HAProxy提供了L4(TCP)和L7(HTTP)两种负载均衡能力，具备丰富的功能。HAProxy的社区非常活跃，版本更新快速（最新稳定版1.7.2于2017/01/13推出）。最关键的是，HAProxy具备媲美商用负载均衡器的性能和稳定性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LVS&lt;/strong&gt; ：是纯 4 层的负载均衡，运行在内核态，性能是软件负载均衡中最高的，因为是在四层，所以也更通用一些。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;软件负载均衡的优点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;简单&lt;/strong&gt;：无论是部署还是维护都比较简单。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;便宜&lt;/strong&gt;：买个 Linux 服务器，装上软件即可。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灵活&lt;/strong&gt;：4 层和 7 层负载均衡可以根据业务进行选择；也可以根据业务特点，比较方便进行扩展和定制功能。
据了解如 BAT 等大厂都是 LVS 重度使用者，就是因为 LVS 非常出色的性能，能为公司节省很大成本，了解到很多大公司使用的 LVS 都是定制版的，做过很多性能方面的优化，比开源版本性能会高出很多。目前只有淘宝开源过优化过的 alibaba/LVS，支持 FNAT 模式，但是也很久没有更新过了。另外爱奇艺去年开源出了 DPDK 版本的 LVS，名叫 DPVS，性能非常强悍。&lt;/li&gt;
&lt;/ul&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/categories/network/">network</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/network/">network</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/loadbalancer/">LoadBalancer</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/lvs/">LVS</category>
                                
                            
                                
                                
                                
                                    <category domain="https://neteric.top/tags/nginx/">Nginx</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title></title>
                <link>https://neteric.top/posts/koordinator%E8%B0%83%E7%A0%94/</link>
                <guid isPermaLink="true">https://neteric.top/posts/koordinator%E8%B0%83%E7%A0%94/</guid>
                <pubDate>Mon, 01 Jan 0001 00:00:00 &#43;0000</pubDate>
                
                    <author>neteric@126.com (neteric)</author>
                
                <copyright>[蜀ICP备2022023345号-1](http://beian.miit.gov.cn/)</copyright>
                
                    <description>&lt;h3 id=&#34;问题思考&#34;&gt;问题思考&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Koordinator自动检测CPU Throttle事件，并自动将CPU Limit调整为适当的值&amp;rdquo;，这个方式其实对在线业务是有影响的，那如何做到对在线业务的影响&amp;lt;5%的呢？是怎么计算的呢？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;混部的场景下，即使限制了离线业务cpu的使用，但离线业务的内存仍然没有释放，在线业务的高峰期由于内存问题还是无法处理更多的在线请求，这个情况怎么处理呢？&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;标准通用的混部解决方案&#34;&gt;标准、通用的混部解决方案&lt;/h2&gt;
&lt;p&gt;混部需要一套完整、自闭环的调度回路，但在企业应用混部的过程中，将要面临的两大挑战是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用如何接入到混部平台&lt;/li&gt;
&lt;li&gt;应用如何在平台上能够运行稳定、高效&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;大规模生产实践锤炼&#34;&gt;大规模生产实践锤炼&lt;/h3&gt;
&lt;p&gt;2021 双 11 之后阿里对外宣布了“首次！统一调度系统规模化落地，全面支撑阿里巴巴双 11 全业务”：&lt;/p&gt;
&lt;p&gt;作为阿里巴巴的核心项目，阿里云（容器团队和大数据团队）联合阿里巴巴资源效能团队、蚂蚁容器编排团队，历时一年多研发和技术攻坚，实现了从“混部技术”到今天“统一调度技术”的全面升级。&lt;/p&gt;
&lt;p&gt;今天，统一调度已实现阿里巴巴电商、搜推广、MaxCompute 大数据的调度全面统一，实现了 Pod 调度和 task 高性能调度的统一，实现了完整的资源视图统一和调度协同，实现了多种复杂业务形态的混部和利用率提升，全面支撑了全球数十个数据中心、数百万容器、数千万核的大规模资源调度。
作为云原生混部的践行者，阿里巴巴是真刀真枪的在生产环境中推进混部技术理念，并在去年双 11 完成了超过千万核的混部规模，通过混部技术帮助阿里巴巴双 11 节约超过 50% 的大促资源成本，在大促快上快下链路上提速 100%，助力大促实现丝滑的用户体验。
正是在双 11 这样的峰值场景驱动之下，阿里的混部调度技术持续演进，积累了大量的生产实践经验，到今天已经是第三代即云原生全业务混部系统。Koordinator 的架构基于阿里巴巴内部的超大规模生产实践，结合阿里云看到的企业容器客户的真实诉求，在标准化、通用化上做出了更多的突破，实现首个基于标准 Kubernetes 的生产可用的开源混部系统。&lt;/p&gt;
&lt;h3 id=&#34;支持丰富的负载类型&#34;&gt;支持丰富的负载类型&lt;/h3&gt;
&lt;p&gt;混部是一套针对延迟敏感服务的精细化编排+大数据计算工作负载混合部署的资源调度解决方案，核心技术在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;精细的资源编排，以满足性能及长尾时延的要求，关键点是精细化的资源调度编排策略及 QoS 感知策略；&lt;/li&gt;
&lt;li&gt;智能的资源超卖，以更低成本满足计算任务对计算资源的需求，并保证计算效率的同时不影响延迟敏感服务的响应时间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://neteric.top/images/Koordinator%E8%B0%83%E7%A0%94_pic_1668160549204.png&#34; alt=&#34;picture 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图是 Koordinator 混部资源超卖模型，也是混部最关键最核心的地方。其中超卖的基本思想是去利用那些已分配但未使用的资源来运行低优先级的任务，如图所示的四条线分别是资源使用量（红线），短生命周期可超卖量（蓝线），长生命周期可超卖量（浅蓝），以及资源总量（黑线）。
该资源模型足够精炼的同时也具备很强的灵活性，支持丰富的在线资源编排类别，支持短生命周期的批处理任务（MapReduce 类别），以及实时计算类的生命周期任务。Koordinator 整个混部资源调度的大厦构建在这样一个资源模型的基础之上，配合上优先级抢占、负载感知、干扰识别和 QoS 保障技术，构建出混部资源调度底层核心系统。Koordinator 社区将围绕这个思路投入建设，持续将混部场景的调度能力展开，解决企业面临的真实业务场景问题。&lt;/p&gt;
&lt;h3 id=&#34;零侵入低接入成本&#34;&gt;零侵入，低接入成本&lt;/h3&gt;
&lt;p&gt;企业接入混部最大的挑战是如何让应用跑在混部平台之上，这第一步的门槛往往是最大的拦路虎。Koordinator 针对这一问题，结合内部生产实践经验，设计了“零侵入”的混部调度系统：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对 Kubernetes 平台的零侵入：无需修改任何 Kubernetes 原生组件，而是以插件的方式增强混部需要的各种能力&lt;/li&gt;
&lt;li&gt;对工作负载编排系统的零侵入：无需修改计算任务的管理引擎（operator），而是以配置化的方式管理混部策略参数，简化应用接入的难度&lt;/li&gt;
&lt;li&gt;支持系统的平滑升级：已有的 Kubernetes 集群的调度器可以平滑升级到 Koordinator，存量的 Pod 的标准调度能力可以无损接管到 Koordinator 中&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过在这三方面的努力，降低用户在生产环境中引入 Koordinator 的难度，只在帮助用户解决生产环境应用混部的第一道拦路虎。&lt;/p&gt;
&lt;h2 id=&#34;版本特性深入解读&#34;&gt;版本特性深入解读&lt;/h2&gt;
&lt;p&gt;自 2022 年 4 月份 Koordinator 发布以来，社区致力于解决任务混部最核心的资源编排、资源隔离的问题&lt;/p&gt;
&lt;p&gt;在版本迭代过程中，Koordinator社区始终围绕三大能力而构建，即任务调度、差异化 SLO 以及 QoS 感知调度能力。&lt;/p&gt;
&lt;h3 id=&#34;任务调度&#34;&gt;任务调度&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Enhanced Coscheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Koordinator 在启动之初，期望支持 Kubernetes 多种工作负载的混部调度，提高工作负载的运行时效率和可靠性，其中就包括了机器学习和大数据领域中广泛存在的具备 All-or-Nothing 需求的作业负载。例如当提交一个Job 时会产生多个任务，这些任务期望要么全部调度成功，要么全部失败。这种需求称为 All-or-Nothing，对应的实现被称作 Gang Scheduling(or Coscheduling) 。为了解决 All-or-Nothing 调度需求，Koordinator 基于社区已有的 Coscheduling 实现了 Enhanced Coscheduling：
支持 Strict/NonStrict 模式，解决大作业场景长时间得不到资源问题
支持 AI 场景多角色联合的 coscheduling 策略，例如一个 TF  训练 Job 中包含 PS 和 Worker 两种角色，并且两种角色都需要单独定义 MinMember，但又期望两种角色的 All-or-Nothing 都满足后才能继续调度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enhanced ElasticQuota Scheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;企业的 Kubernetes 一般由多个产品和研发团队共用一个规模比较大的 Kubernetes 集群，由资源运维团队统一管理集群内大量 CPU/Memory/Disk 等资源。
Koordinator 为帮助用户管理好资源额度，提升资源额度的使用效率，实现降本增效，Koordinator 基于基于社区 ElasticQuota CRD 实现了 Enhanced  ElasticQuota Scheduling ，具备如下增强特性：
兼容社区的 ElasticQuota CRD，用户可以无缝升级到 Koordinator
支持树形结构管理 Quota，契合企业的组织架构
支持按照共享权重(shared weight)保障公平性
允许用户设置是否允许借用 Quota 给其他消费对象&lt;/p&gt;
&lt;p&gt;Koordinator ElasticQuota Scheduling 通过额度借用机制和公平性保障机制，Koordinator 把空闲的额度复用给更需要的业务使用。当额度的所有者需要额度时，Koordinator 又可以保障有额度可用。通过树形管理机制管理 Quota，可以方便的与大多数公司的组织结构映射，解决同部门内或者不同部门间的额度管理需求。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fine-grained Device Scheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 AI 领域，GPU、RDMA、FPGA 等异构资源被广泛的应用，Koordinator 针对异构资源调度场景，提供了精细化的设备调度管理机制，包括：
支持 GPU 共享，GPU 独占，GPU 超卖
支持百分比的 GPU 调度
支持 GPU 多卡调度
NVLink 拓扑感知调度（doing）&lt;/p&gt;
&lt;h3 id=&#34;差异化-slo&#34;&gt;差异化 SLO&lt;/h3&gt;
&lt;p&gt;差异化 SLO 是 Koordinator 提供的核心混部能力，保障资源超卖之后的 Pod 的运行稳定性。Koordinator 定了一一组 Priority &amp;amp; QoS，用户按照这一最佳实践的方式接入应用，配合完善的资源隔离策略，最终保障不同应用类型的服务质量。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU Supress&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Koordinator 的单机组件 koordlet 会根据节点的负载水位情况，调整 BestEffort 类型 Pod 的 CPU 资源额度。这种机制称为 CPU Suppress。当节点的在线服务类应用的负载较低时，koordlet 会把更多空闲的资源分配给 BestEffort 类型的 Pod 使用；当在线服务类应用的负载上升时，koordlet 又会把分配给 BestEffort 类型的 Pod 使用的 CPU 还给在线服务类应用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU Burst&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CPU Burst 是一种服务级别目标 (SLO) 感知的资源调度功能。用户可以使用 CPU Burst 来提高对延迟敏感的应用程序的性能。内核的调度器会因为容器设置的 CPU Limit 压制容器的 CPU，这个过程称为 CPU Throttle。该过程会降低应用程序的性能。&lt;/p&gt;
&lt;p&gt;Koordinator 自动检测 CPU Throttle 事件，并自动将 CPU Limit 调整为适当的值。通过 CPU Burst 机制能够极大地提高延迟敏感的应用程序的性能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于内存安全阈值的主动驱逐机制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当延迟敏感的应用程序对外提供服务时，内存使用量可能会由于突发流量而增加。类似地，BestEffort 类型的工作负载可能存在类似的场景，例如，当前计算负载超过预期的资源请求/限制。这些场景会增加节点整体内存使用量，对节点侧的运行时稳定性产生不可预知的影响。例如，它会降低延迟敏感的应用程序的服务质量，甚至变得不可用。尤其是在混部场景下，这个问题更具挑战性。
我们在 Koordinator 中实现了基于内存安全阈值的主动驱逐机制。koordlet 会定期检查 Node 和 Pods 最近的内存使用情况，检查是否超过了安全阈值。如果超过，它将驱逐一些 BestEffort 类型的 Pod 释放内存。在驱逐前根据 Pod 指定的优先级排序，优先级越低，越优先被驱逐。相同的优先级会根据内存使用率（RSS）进行排序，内存使用率越高越优先被驱逐。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于资源满足的驱逐机制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CPU Suppress 在线应用的负载升高时可能会频繁的压制离线任务，这虽然可以很好的保障在线应用的运行时质量，但是对离线任务还是有一些影响的。虽然离线任务是低优先级的，但频繁压制会导致离线任务的性能得不到满足，严重的也会影响到离线的服务质量。而且频繁的压制还存在一些极端的情况，如果离线任务在被压制时持有内核全局锁等特殊资源，那么频繁的压制可能会导致优先级反转之类的问题，反而会影响在线应用。虽然这种情况并不经常发生。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，Koordinator 提出了一种基于资源满足度的驱逐机制。我们把实际分配的 CPU 总量与期望分配的 CPU 总量的比值成为 CPU 满足度。当离线任务组的 CPU 满足度低于阈值，而且离线任务组的 CPU 利用率超过 90% 时，koordlet 会驱逐一些低优先级的离线任务，释放出一些资源给更高优先级的离线任务使用。通过这种机制能够改善离线任务的资源需求。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L3 Cache 和内存带宽分配(MBA) 隔离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;混部场景下，同一台机器上部署不同类型的工作负载，这些工作负载会在硬件更底层的维度发生频繁的资源竞争。因此如果竞争冲突严重时，是无法保障工作负载的服务质量的。
Koordinator 基于 Resource Director Technology (RDT, 资源导向技术) ，控制由不同优先级的工作负载可以使用的末级缓存（服务器上通常为 L3 缓存）。RDT 还使用内存带宽分配 (MBA) 功能来控制工作负载可以使用的内存带宽。这样可以隔离工作负载使用的 L3 缓存和内存带宽，确保高优先级工作负载的服务质量，并提高整体资源利用率。&lt;/p&gt;
&lt;h3 id=&#34;qos-感知调度重调度&#34;&gt;QoS 感知调度、重调度&lt;/h3&gt;
&lt;p&gt;Koordinator 差异化 SLO 能力在节点侧提供了诸多 QoS 保障能力能够很好的解决运行时的质量问题。同时 Koordinator Scheduler 也在集群维度提供了增强的调度能力，保障在调度阶段为不同优先级和类型的 Pod 分配合适的节点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负载感知调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;超发资源可以极大的提升集群的资源利用率，但也会凸显集群内节点之间资源利用率不均匀的现象。这个现象在非混部环境下也是存在的，只是因为 Kubernetes 原生是不支持资源超发机制，节点上的利用率往往不是很高，一定程度上掩盖了这个问题。但当混部时，资源利用率会上升到比较高的水位时就暴露了这个问题。
利用率不均匀一般是节点之间不均匀以及出现局部的负载热点，局部的负载热点会可能影响工作负载的整体运行效果。另一个是在负载高的节点上，在线应用和离线任务之间可能会存在的严重的资源冲突，影响到在线应用的运行时质量。
为了解决这个问题， Koordinator 的调度器提供了一个可配置的调度插件控制集群的利用率。该调度能力主要依赖于 koordlet 上报的节点指标数据，在调度时会过滤掉负载高于某个阈值的节点，防止 Pod 在这种负载较高的节点上无法获得很好的资源保障，另一方面是避免负载已经较高的节点继续恶化。在打分阶段选择利用率更低的节点。该插件会基于时间窗口和预估机制规避因瞬间调度太多的 Pod 到冷节点机器出现一段时间后冷节点过热的情况。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;精细化 CPU 调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;随着资源利用率的提升进入到混部的深水区，需要对资源运行时的性能做更深入的调优，更精细的资源编排可以更好的保障运行时质量，从而通过混部将利用率推向更高的水平。
我们把 Koordinator QoS 在线应用 LS 类型做了更细致的划分，分为 LSE、LSR 和 LS 三种类型。拆分后的 QoS 类型具备更高的隔离性和运行时质量。通过这样的拆分，整个 Koordinator QoS 语义更加精确和完整，并且兼容 Kubernetes 已有的 QoS 语义。
而且我们针对 Koordinator QoS，设计了一套丰富灵活的 CPU 编排策略，如下表所示。
图片&lt;/p&gt;
&lt;p&gt;不同的 QoS 类型的工作负载具备不同的隔离性：
图片
Koordinator Scheduler 会针对 LSE/LSR 类型的 Pod 分配具体的 CPU 逻辑核，并更新到 Pod Annotation 中，由单机侧 koordlet 配合，把调度器分配的 CPU 更新到 cgroup 中。
调度器在分配 CPU 时，会根据 CPU 拓扑结构分配，默认尽可能的保障分配的 CPU 属于同一个 NUMA Node 以获得更好的性能。并且 CPU 调度时，支持 Pod 根据需要设置不同的互斥策略，例如一个系统中多个核心的服务部署在相同的物理核上，性能表现比较差，但分开就完全没问题，此时就可以配置互斥策略，调度器会尽可能的把这种互斥属性的 Pod 分配使用不同的物理核。并且在打分阶段，调度器会参考集群的整体情况，选择符合 CPU 编排要求的节点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源预留（Reservation）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Koordinator 支持在不侵入 Kubernetes 已有的机制和代码前提下，实现了资源预留的原子能力(Reservation)。Koordinator Reservation API 允许用户不修改 Pod Spec 或者存量的 Workload(例如 Deployment, StatefulSet）即可以预留资源。资源预留在容量管理、碎片优化、调度成功率和重调度等场景有重要作用：
当有重要的工作负载在未来某段时间需要资源时，可以提前预留资源满足需求。
用户在 PaaS 上发起扩容时，可以通过资源预留能力尝试预留，预留成功后发起扩容，保障 PaaS 的 SLA。
用户在 PaaS 上发布时，如果应用采用了滚动发布的能力，可以通过资源预留保留即将销毁的 Pod 持有的资源，在滚动发布时，新建的 Pod 可以复用预留下来的原有资源，能够有效提高滚动发布的成功率。
碎片优化场景中，可以通过资源预留占住空闲的碎片资源，并把可被整理的 Pod 迁移到这些节点。
重调度时在发起驱逐前，先尝试预留资源，预留成功后发起驱逐，避免驱逐后无资源可用影响应用的可用性。&lt;/p&gt;
&lt;p&gt;图片&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;灵活可扩展且安全的重调度器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;调度器调度时是根据当时集群内的情况和配置，做出综合的判断，选择出一个最合适的节点分配给 Pod 使用。但随着时间和工作负载的变化，原本最合适的节点也会变差，差异化 SLO 和调度器都提供了丰富的能力帮助改善这些问题，但差异化 SLO 更多还是关注在自身单机的情况，无法感知全局的变化。
从控制的角度看，我们也需要根据集群内的情况做出决策，把异常的 Pod 驱逐迁移到更合适的节点，让这些 Pod 有机会可以更好的对外服务。因此 Koordinator Descheduler 应运而生。
我们认为 Pod 迁移是一个复杂的过程，涉及到审计、资源分配、应用启动等步骤，还夹杂着应用的发布升级、扩容/缩容场景和集群管理员的资源运维操作。因此，如何管理 Pod 迁移过程的稳定性风险，保证应用不会因为 Pod 的迁移影响可用性，是一个非常关键的必须解决的问题。
为了让大家更好的理解，举几个场景：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;社区的重调度器内置的多个重调度策略根据自身逻辑判断某个 Pod 是否要被迁移，需要迁移时调用 Kubernetes Eviction API  发起驱逐。但是这个过程并不关注被驱逐的 Pod 在将来是否可以分配到资源。因此存在大量 Pod 被驱逐后因为没有资源而处于 Pending 状态的情况。如果应用此时有大量请求进来，又因为没有足够的可用的 Pod 导致可用性异常。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;另外，社区重调度器调用的 Kubernetes Evcition API 虽然会检查 PDB 确保在安全范围内驱逐，但是众所周知，众多的 workload Controller 在发布和缩容场景都是通过直接调用 Delete API 的形式销毁 Pod，此时并不会被 PDB 限制。这就导致重调度时如果遇到上述场景，是很可能引发严重的稳定性问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我们认为 Pod 腾挪不是一个简单的后台自动化逻辑，有相当多的场景和用户期望由人工介入手工迁移 Pod，甚至期望重调度时发起的自动迁移请求应该被拦截掉，经过审批决定是否执行。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Koordinator 基于 CRD 定义了一个名为 PodMigrationJob API。重调度器或者其他自动化自愈组件通过 PodMigrationJob 可以安全的迁移 Pod。PodMigrationJob Controller 在处理 PodMigrationJob 时会先尝试通过 Koordinator Reservation 机制预留资源，预留失败则迁移失败；资源预留成功后发起驱逐操作并等待预留的资源被消费。中间的过程都会记录到 PodMigrationJobStatus 中，并产生相关的 Event。
我们在 Koordinator 中实现了一个全新的 Descheduler Framework。我们认为重调度场景：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需要一个插件化机制实现自定义的重调度策略，但又不希望这个抽象过于复杂；&lt;/li&gt;
&lt;li&gt;需要具备基本的插件管理能力，通过配置启用和禁用插件；&lt;/li&gt;
&lt;li&gt;具备统一的插件配置下发机制，方便插件自定义参数；&lt;/li&gt;
&lt;li&gt;并能够方便的扩展和使用统一的 Evictor 机制；&lt;/li&gt;
&lt;li&gt;另外期望用户能够基于 controller-runtime 实现 controller 并纳入统一的插件管理机制。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Koordinator descheduler framework 提供了插件配置管理（例如启用、禁用，统一配置等）、插件初始化、插件执行周期管理等机制。并且该框架内置了基于 PodMigrationJob 实现的 Controller，并作为 Evictor Plugin 方便被各种重调度插件使用，帮助重调度插件安全的迁移 Pod。
基于 Koordinator descheduler framework，用户可以非常容易的扩展实现自定义重调度策略，就像基于 Kubernetes scheduling framework 的实现自定义的调度插件一样简单。并且用户也可以以插件的形式实现 controller，支持基于 Event 触发重调度的场景。&lt;/p&gt;
&lt;h2 id=&#34;koordinator-的未来规划&#34;&gt;Koordinator 的未来规划&lt;/h2&gt;
&lt;p&gt;Koordinator 社区将不断丰富大数据计算任务混部的形态，拓展包括 Hadoop YARN 等计算框架混部支持，丰富任务混部解决方案，项目上持续的完善干扰检测、问题诊断体系，推进更多的负载类型融入 Koordinator 生态，并取得更好的资源运行效率。&lt;/p&gt;
&lt;p&gt;图片
Koordinator 社区将持续的保持中立的发展趋势，联合各厂商持续的推进混部能力的标准化，也欢迎大家加入社区共同推进混部的标准化进程。&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                    
                
            </item>
        
    </channel>
</rss>
