---
title: "存储引擎对比"
date: 2022-08-18T16:10:03+08:00
draft: false
tags: ["B+Tree","LSM Tree", "Log Structed", "Linux IO", "Disk Struct"]
categories: ["存储"]
---
## 一、背景

### 1. 硬盘结构

硬盘内部主要部件为磁盘盘片、传动手臂、读写磁头和主轴马达。实际数据都是写在盘片上，读写主要是通过传动手臂上的读写磁头来完成。实际运行时，主轴让磁盘盘片转动，然后传动手臂可伸展让读取头在盘片上进行读写操作。磁盘物理结构如下图所示：

![img](http://info.mrtlab.com/uploadfile/201305/14/104000249.JPG)

由于单一盘片容量有限，一般硬盘都有两张以上的盘片，每个盘片有两面，都可记录信息，所以一张盘片对应着两个磁头。盘片被分为许多扇形的区域，每个区域叫一个扇区，硬盘中每个扇区的大小固定为512字节。盘片表面上以盘片中心为圆心，不同半径的同心圆称为磁道，不同盘片相同半径的磁道所组成的圆柱称为柱面。磁道与柱面都是表示不同半径的圆，在许多场合，磁道和柱面可以互换使用。磁盘盘片垂直视角如下图所示

![img](http://info.mrtlab.com/uploadfile/201305/14/1040344565.jpg)

### 2. 影响硬盘性能的因素

影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I/O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成。

- 寻道时间

Tseek是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3-15ms。

- 旋转延迟

Trotation是指盘片旋转将请求数据所在的扇区移动到读写磁盘下方所需要的时间。旋转延迟取决于磁盘转速，通常用磁盘旋转一周所需时间的1/2表示。比如：7200rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000rpm的磁盘其平均旋转延迟为2ms。

- 数据传输时间

Ttransfer是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率，数据传输时间通常远小于前两部分消耗时间。简单计算时可忽略。

结论：**机械硬盘的连续读写性能很好，但随机读写性能很差，**这主要是因为磁头移动到正确的磁道上需要时间，随机读写时，磁头需要不停的移动，时间都浪费在了磁头寻址上，所以性能不高

### 3. Linux IO流程

类似于网络的分层结构，下图显示了Linux系统中对于磁盘的一次读请求在核心空间中所要经历的层次模型。从图中看出：对于磁盘的一次读请求，首先经过虚拟文件系统层（VFS Layer），其次是具体的文件系统层（例如Ext2），接下来是Cache层（Page Cache Layer）、通用块层（Generic Block Layer）、I/O调度层（I/O Scheduler Layer）、块设备驱动层（Block Device Driver Layer），最后是物理块设备层（Block Device Layer）

![image-20220819105617539](/images/storage_engine_compare/fig01.png)

[kernel 4.10 linux io stack](https://www.thomas-krenn.com/de/wikiDE/images/e/e0/Linux-storage-stack-diagram_v4.10.png)

结论： **一次硬盘读取IO路径非常长，要经历用户态到内核态的上下文切换，IO调度，块设备驱动，中断等逻辑**

### 4. 存储分类

#### 结构化数据

可以从名称中看出，是高度组织和整齐格式化的数据。结构化数据可以轻易放入表格和电子表格中的数据类型，典型的比如使用关系型数据库表示和存储，表现为二维形式的数据。一般特点是：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的

##### 结构化数据的存储

- 关系型数据库：如 MySQL、PostreSQL、Oracle、SQLServer， 其组成一般包括： 连接器，解析器，优化器，执行器，**存储引擎**， 管理硬盘的单机文件系统

#### 半结构化数据

半结构化数据是结构化数据的一种形式，它并不符合关系型数据库或其他数据表的形式关联起来的数据模型结构，但包含相关标记，用来分隔语义元素以及对记录和字段进行分层。因此，它也被称为自描述的结构。常见的半结构数据格式有XML、JSON、Yaml

##### 半结构化数据的存储

- K/V存储：Rocksdb、LevelDB、BlotDB 其组成一般包括：连接器，优化器，**存储引擎**，单机文件系统
- 文档存储： MongoDB、ElasticSearch
- 列式存储（也叫表格存储）： ClickHouse、Apache Druid 、Hbase
- 内存K/V存储： Redis、Memcache

#### 非结构化数据

数据结构不规则或不完整，没有预定义的数据模型，不方便用数据库二维逻辑表来表现的数据。本质上是结构化数据之外的一切数据，如：全文文本、图片、音频、影视、超媒体等信息

##### 非结构化数据的存储

- 单机文件系统： 如 ext2/3/4, xfs, btrfs, bluefs,ntfs, fat32

- 分布式块/对象/文件存储：如 ceph, hdfs, glusterfs, MinIO，这类非结构化数据的存储又叫Blob存储，其组成部分一般都包括：管理硬盘的单机文件系统 ，分布式特性的实现（分片，复制，一致性），元数据管理系统（元数据一般都是半结构化数据），接口实现（块，对象，文件）可参考前面的技术分享：https://duodian.feishu.cn/file/boxcnPObkzRjzgF1mfrwJnzbMPc

### 5. 存储使用场景

| 属性         | OLTP                                        | OLAP                                        |
| ------------ | ------------------------------------------- | ------------------------------------------- |
| 主要读取模式 | 小数据量的随机读，通过 key 查询             | 大数据量的聚合（max,min,sum, avg）查询      |
| 主要写入模式 | 随机访问，低延迟写入                        | 批量导入（ETL）或者流式写入                 |
| 主要应用场景 | 通过 web 方式使用的最终用户                 | 互联网分析，为了辅助决策                    |
| 如何看待数据 | 当前时间点的最新状态                        | 随着时间推移的                              |
| 数据量       | 单次查询和总体都相对较小，总体通常 GB 到 TB | 单次查询和总体都相对巨大，总体通常 TB 到 PB |
| 请求数量     | 相对频繁，侧重在线交易                      | 相对较少，侧重离线分析                      |
| 瓶颈         | Disk Seek                                   | Disk Bandwidth                              |
| 存储格式     | 多用行存                                    | 列存逐渐流行                                |
| 用户         | 比较普遍，一般应用用的比较多                | 多为商业用户                                |
| 场景举例     | 银行交易                                    | 商业分析                                    |
| 产品举例     | MySQL                                       | ClickHouse                                  |



## 二、LSM Tree的起源

### 1. 世界上最简单的数据库

首先来看，世界上“最简单”的数据库，由两个 Bash 函数构成：

```bash
#!/bin/bash
db_set () {
	echo "$1,$2" >> database
}

db_get () {
	grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

这两个函数实现了一个基于字符串的 KV 存储（只支持 get/set，不支持 delete）：

```bash
$ db_set 123456 '{"name":"London","attractions":["BigBen","LondonEye"]}' 
$ db_set 42 '{"name":"SanFrancisco","attractions":["GoldenGateBridge"]}'
$ db_get 42
{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
```

来分析下它为什么 work，也反映了日志结构存储的最基本原理：

1. set：在文件末尾追加一个 KV 对。

2. get：匹配所有 Key，返回最后（也即最新）一条 KV 对中的 Value

3. 更新操作： 使用set在文件末尾追加一条该key的新记录

4. 删除操作： 使用set在文件末尾追加一条该key的墓碑记录，当查询到墓碑记录代表这个key已删除

### 2. 引入哈希索引



可以看出，上述“世界上最简单的数据库”写入的速度非常快，几乎是数据库能做到的极限。但是读需要全文逐行扫描，会慢很多，如果数据量达到百GB级别，该数据库的读变得几乎不可用，为了加快读，我们需要构建**索引**（一种允许基于某些字段查找的额外数据结构）

索引从原数据中构建，只为加快查找。因此索引会耗费一定额外空间，和插入时间（每次插入要更新索引），即，重新以空间和写换读取。

依上小节的例子，所有数据顺序追加到磁盘上。为了加快查询，我们在内存中构建一个哈希索引：

1. Key 是查询 Key
2. Value 是 KV 条目的起始位置和长度。

![ddia-3-1-hash-map-csv.png](/images/ch03-fig01.png)

看起来很简单，但这正是 [Bitcask](https://docs.riak.com/riak/kv/2.2.3/setup/planning/backend/bitcask/index.html "Bitcask") 的基本设计，关键是他能很好的 Work（在小数据量时，即所有 key 都能存到内存中时）能提供很高的读写性能

1. 写：文件追加写。
2. 读：一次内存查询，一次磁盘 seek；如果数据已经被缓存，则 seek 也可以省掉。

如果你的 key 集合很小（意味着能全放内存），但是每个 key 更新很频繁，那么 Bitcask 便是你的菜。举个栗子：频繁更新的视频播放量，key 是视频 url，value 是视频播放量

> 但有个很重要问题，单个文件越来越大，磁盘空间不够怎么办？
>
> 在文件到达一定尺寸后，就新建一个文件，将原文件变为只读。同时为了回收多个 key 多次写入的造成的空间浪费，可以将只读文件进行紧缩（ compact ），将旧文件进行重写，挤出“水分”（被覆写的数据）以进行垃圾回收。

![ddia-3-3-compaction-sim.png](/images/ch03-fig03.png)

当然，如果我们想让其**工业可用**，还有很多问题需要解决：

1. **文件格式**。对于**日志**来说，CSV 不是一种紧凑的数据格式，有很多空间浪费。比如，可以用 length + record bytes 。
2. **记录删除**。之前只支持 put\get，但实际还需要支持 delete。但日志结构又不支持更新，怎么办呢？一般是写一个特殊标记（比如墓碑记录，tombstone）以表示该记录已删除。之后 compact 时真正删除即可。
3. **宕机恢复**。在机器重启时，内存中的哈希索引将会丢失。当然，可以全盘扫描以重建，但通常一个小优化是，对于每个 segment file， 将其索引条目和数据文件一块持久化，重启时只需加载索引条目即可。
4. **记录写坏、少写**。系统任何时候都有可能宕机，由此会造成记录写坏、少写。为了识别错误记录，我们需要增加一些校验字段，以识别并跳过这种数据。为了跳过写了部分的数据，还要用一些特殊字符来标识记录间的边界。
5. **并发控制**。由于只有一个活动（追加）文件，因此写只有一个天然并发度。但其他的文件都是不可变的（compact 时会读取然后生成新的），因此读取和紧缩可以并发执行。

乍一看，基于日志的存储结构存在着不少浪费：需要追加进行更新和删除。但日志结构有几个原地更新结构无法做的优点：

1. **以顺序写代替随机写**。对于磁盘和 SSD，顺序写都要比随机写快几个数量级。
2. **简易的并发控制**。由于大部分的文件都是不可变（immutable）的，因此更容易做并发读取和紧缩。也不用担心原地更新会造成新老数据交替。
3. **更少的内部碎片**。每次紧缩会将垃圾完全挤出。但是原地更新就会在 page 中留下一些不可用空间

  

### 3. SSTable 

上一节讲的基于内存的哈希索引也有其局限：

1. **所有 Key 必须放内存**：一旦 Key 的数据量超过内存大小，这种方案便不再 work。当然你可以设计基于磁盘的哈希表，但那又会带来大量的随机写
2. **不支持范围查询**：由于 key 是无序的，不能简单的支持扫描kitty001到kitty999区间内的所有key，只能采用逐个查找的方式查询每个key
3. **compact效率低**：日志文件compact需要遍历文件内容，效率低下

有这些局限的主要原因是在外存上的数据是简单追加写而形成的，并没有按照某个字段有序。假设加一个限制，让这些文件按 key 有序。我们称这种格式为：SSTable（Sorted String Table）。

这种文件格式有什么优点呢？

- **高效的数据文件合并**。即有序文件的归并外排，顺序读，顺序写

![ddia-3-4-merge-sst.png](/images/ch03-fig04.png)



- **不需要在内存中保存所有数据的索引**

  仅需要记录下每个文件界限（以区间表示：[startKey, endKey]，当然实际会记录的更细）即可。查找某个 Key 时，去所有包含该 Key 的区间对应的文件二分查找即可（也即是**稀疏索引**）在文件中查找特定的key时，不再需要在内存中保存所有键的索引。 如下图假设正在查找handiwork ，且不知道该键在段文件中的确切偏移。但是，如果知道handbag 和handsome 的偏移 ，考虑到根据键排序，则键handiwork一定位于它们两者之间。这意味着可以跳到handbag 的偏移，从那里开始扫描，直到找到hαndiwork,如果找到handsome都还没找到则可以断定这个文件中没有handiwork这个key的记录

![ddia-3-5-sst-index.png](https://s2.loli.net/2022/04/16/j8tM6IUk1QrJXuw.png)

- **分块压缩，节省空间，减少 IO**。相邻 Key 共享前缀，既然每次都要批量取，那正好一组 key batch 到一块，称为 block，且只记录 block 的索引。

#### 构建和维护 SSTables

SSTables 格式听起来很美好，但须知数据是乱序的来的，我们如何得到有序的数据文件呢？

这可以拆解为两个小问题：

**构建 SSTable 文件**。将乱序数据在外存（磁盘 or SSD）中上整理为有序文件，是比较难的。但是在内存就方便的多。于是一个大胆的想法就形成了：

1. 在内存中维护一个有序结构（称为 **MemTable**）。典型实现如：红黑树、AVL 树、跳表。
2. 到达一定阈值之后全量 dump 到外存。

**维护 SSTable 文件**。为什么需要维护呢？首先要问，对于上述复合结构，我们怎么进行查询：

1. 先去 MemTable 中查找，如果命中则返回。
2. 再去 SSTable 按时间顺序由新到旧逐一查找。

如果 SSTable 文件越来越多，则查找代价会越来越大。因此需要将多个 SSTable 文件合并，以减少文件数量，同时进行 GC，我们称之为**紧缩**（ Compaction）。

**该方案的问题**：如果出现宕机，内存中的数据结构将会消失。 解决方法也很经典：WAL。

### 4. 从 SSTables 到 LSM-Tree

将前面几节的一些碎片有机的组织起来，便是时下流行的存储引擎 LevelDB 和 RocksDB 后面的存储结构：LSM-Tree

这种数据结构是 Patrick O’Neil 等人，在 1996 年提出的：[The Log-Structured Merge-Tree](https://www.cs.umb.edu/~poneil/lsmtree.pdf "The Log-Structured Merge-Tree")。

Elasticsearch 和 Solr 的索引引擎 Lucene，也使用类似 LSM-Tree 存储结构。但其数据模型不是 KV，但类似：word → document list。

##### LSM-Tree的性能优化

如果想让一个引擎工程上可用，还会做大量的性能优化。对于 LSM-Tree 来说，包括：

- **优化 SSTable 的查找** ：常用 [**Bloom Filter**](https://www.qtmuniao.com/2020/11/18/leveldb-data-structures-bloom-filter/)。该数据结构可以使用较少的内存为每个 SSTable 做一些指纹，起到一些初筛的作用。

- **层级化组织 SSTable**：以控制 Compaction 的顺序和时间。常见的有 size-tiered 和 leveled compaction。LevelDB 便是支持后者而得名。前者比较简单粗暴，后者性能更好，也因此更为常见。

![ddia-sized-tierd-compact.png](/images/ch03-sized-tiered.png)

对于 RocksDB 来说，工程上的优化和使用上的优化就更多了。在其 [Wiki](https://github.com/facebook/rocksdb/wiki "rocksdb wiki") 上随便摘录几点：

1. Column Family
2. 前缀压缩和过滤
3. 键值分离，BlobDB

但无论有多少变种和优化，LSM-Tree 的核心思想——**保存一组合理组织、后台合并的 SSTables** ——简约而强大。可以方便的进行范围遍历，可以变大量随机为少量顺序。



## 三、另一个世界-B 族树

虽然先讲的 LSM-Tree，但是它要比 B+ 树新的多。B 树于 1970 年被 R. Bayer and E. McCreight [提出](https://dl.acm.org/doi/10.1145/1734663.1734671 "b tree paper")后，便迅速流行了起来。现在几乎所有的关系型数据中，它都是数据索引标准一般的实现。与 LSM-Tree 一样，它也支持高效的**点查**和**范围查**，但却使用了完全不同的组织方式

### 1. 起源-顺序存储的Scheme


![image-20220817200550307](/images/fig98.png)

如上图所示的二维表scheme,我们可以把行数据顺序的记录到外存（序列化过后）上？ 这样记录插入速度也是可用做的非常快的，但顺序写入过后要查找和更新就相对比较慢了，在计算机科学中，用于加快查找的数据结构自然就是树，那能不能把二维表数据以树的形式在硬盘上组织呢？B+树的发展过程其实是面向硬盘特性编程的思考过程，下面一一道来！

### 2. 二叉查找树

![5fLd8bp0nu](/images/fig99.png)

从图中可以看到，我们为 product 表(产品库存表)建立了一个二叉查找树的索引。

二叉查找树的特点就是任何节点的左子节点的键值都小于当前节点的键值，右子节点的键值都大于当前节点的键值。顶端的节点我们称为根节点，没有子节点的节点我们称之为叶节点。

如果我们需要查找 id=5 的产品信息，利用我们创建的二叉查找树索引，查找流程如下：

- 将根节点作为当前节点，把 5与当前节点的键值 4比较，5大于 4，接下来我们把当前节点>的右子节点作为当前节点。
- 继续把 5和当前节点的键值 6 比较，发现 5小于 6，把当前节点的左子节点作为当前节点。
- 把 5 和当前节点的键值 5对比，5等于 5，满足条件，我们从当前节点中取出 data，即 id=5，name=book, num=899。

利用二叉查找树我们只需要 3 次即可找到匹配的数据。如果在表中一条条的查找的话，我们需要 5次才能找到

### 3. 平衡二叉查找树

上面我们讲解了利用二叉查找树可以快速的找到数据。但是，如果上面的二叉查找树是这样的构造：

![image-20220817202250598](/images/fig100.png)

这个时候可以看到我们的二叉查找树变成了一个链表。如果我们需要查找 id=7 的产品信息，我们需要查找 7 次，也就相当于全表扫描了。导致这个现象的原因其实是二叉查找树变得不平衡了，也就是高度太高了，从而导致查找效率的不稳定，为了解决这个问题，我们需要保证二叉查找树一直保持平衡，就需要用到平衡二叉树了。

平衡二叉树又称 AVL 树，在满足二叉查找树特性的基础上，要求每个节点的左右子树的高度差不能超过 1。

下面是平衡二叉树和非平衡二叉树的对比：

![image-20220817203243292](/images/fig101.png)

由平衡二叉树的构造我们可以发现第一张图中的二叉树其实就是一棵平衡二叉树。

平衡二叉树保证了树的构造是平衡的，当我们插入或删除数据导致不满足平衡二叉树不平衡时，平衡二叉树会进行调整树上的节点来保持平衡。具体的调整方式这里就不介绍了。平衡二叉树相比于二叉查找树来说，查找效率更稳定，总体的查找速度也更快。

除了平衡二叉查找树，还有很多自平衡的二叉树，比如红黑树，它也是通过一些约束条件来达到自平衡，不过红黑树的约束条件比较复杂，不是本篇的重点，大家可以看《数据结构》相关的书籍来了解红黑树的约束条件。

### 4. B 树

**不管平衡二叉查找树还是红黑树，都会随着插入的元素增多，而导致树的高度变高，这就意味着磁盘 I/O 操作次数多，会影响整体数据查询的效率**。

比如，下面这个平衡二叉查找树的高度为 5，那么在访问最底部的节点时，就需要磁盘 5 次 I/O 操作。

![图片](https://img-blog.csdnimg.cn/img_convert/2d26d30c953cd47c6ab637ad0eba2f99.png)



根本原因是因为它们都是二叉树，也就是每个节点只能保存 2 个子节点 ，如果我们把二叉树改成 M 叉树（M>2）呢？

比如，当 M=3 时，在同样的节点个数情况下，三叉树比二叉树的树要矮

![图片](https://img-blog.csdnimg.cn/img_convert/00fb73de7014a87958f1597345e9ef2f.png)

因此，**当树的节点越多的时候，并且树的分叉数 M 越大的时候，M 叉树的高度会远小于二叉树的高度**。

自平衡二叉树虽然能保持查询操作的时间复杂度在O(logn)，但是因为它本质上是一个二叉树，每个节点只能有 2 个子节点，那么当节点个数越多的时候，树的高度也会相应变高，这样就会增加磁盘的 I/O 次数，从而影响数据查询的效率。为了解决树的高度的问题，后面就出来了 B 树，它不再限制一个节点就只能有 2 个子节点，而是允许 M 个子节点 (M>2)，从而降低树的高度。B 树的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶，所以 B 树就是一个多叉树。
B树相对平衡二叉树在节点空间的利用率上进行改进，B树在每个节点保存更多的数据，减少了树的高度，从而提升了查找的性能，在数据库应用中，B树的每个节点存储的数据量大约为4K, 这是因为考虑到磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次对磁盘进行IO数据读取时，同一个磁盘块的数据会被一次性读取出来，所以每一次磁盘IO都可以读取到B树中一个节点的全部数据。、


### 5. B+ 树

B+ 树就是对 B 树做了一个升级，MySQL 中索引的数据结构就是采用了 B+ 树，B+ 树结构如下图：

![图片](https://img-blog.csdnimg.cn/img_convert/b6678c667053a356f46fc5691d2f5878.png)

B+ 树与 B 树差异的点，主要是以下这几点：

- 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
- 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；
- 非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）。
- 非叶子节点中有多少个子节点，就有多少个索引；

**查找**。从根节点出发，进行二分查找，然后加载新的页到内存中，继续二分，直到命中或者到叶子节点。 查找复杂度，树的高度—— O(lgn)，影响树高度的因素：分支因子（分叉数，通常是几百个）。

**插入 or 更新**。和查找过程一样，定位到原 Key 所在页，插入或者更新后，将页完整写回。如果页剩余空间不够，则分裂后写入。

**分裂 or 合并**。级联分裂和合并。

- 一个记录大于一个 page 怎么办？
  
    树的节点是逻辑概念，page or block 是物理概念。一个逻辑节点可以对应多个物理 page。
    

### 6. B+树优化

B+树不像 LSM-Tree ，会在原地修改数据文件。

在树结构调整时，可能会级联修改很多 Page。比如叶子节点分裂后，就需要写入两个新的叶子节点，和一个父节点（更新叶子指针）。

1. 增加预写日志（WAL），将所有修改操作记录下来，预防宕机时中断树结构调整而产生的混乱现场。
2. 使用 latch 对树结构进行并发控制。
3. 不使用 WAL，而在写入时利用 Copy On Write 技术。同时，也方便了并发控制。如 LMDB、BoltDB。
4. 对中间节点的 Key 做压缩，保留足够的路由信息即可。以此，可以节省空间，增大分支因子。
5. 为了优化范围查询，有的 B 族树将叶子节点存储时物理连续。但当数据不断插入时，维护此有序性的代价非常大。
6. 为叶子节点增加兄弟指针，以避免顺序遍历时的回溯。即 B+ 树的做法，但远不局限于 B+ 树。
7. B 树的变种，分形树，从 LSM-tree 借鉴了一些思想以优化 seek。

## 五、B-Trees 和 LSM-Trees 对比

| 存储引擎 | B-Tree | LSM-Tree | 备注 |
| --- | --- | --- | --- |
| 优势 | 读取更快 | 写入更快 |  |
| 写放大 | 1. 数据和 WAL<br/>2. 更改数据时多次覆盖整个 Page | 1. 数据和 WAL<br/>2. Compaction | SSD 不能过多擦除。因此 SSD 内部的固件中也多用日志结构来减少随机小写。 |
| 写吞吐 | 相对较低：<br/>1. 大量随机写。 | 相对较高：<br/>1. 较低的写放大（取决于数据和配置）<br/>2. 顺序写入。<br/>3. 更为紧凑。 |  |
| 压缩率 | 1. 存在较多内部碎片。 | 1. 更加紧凑，没有内部碎片。<br/>2. 压缩潜力更大（共享前缀）。 | 但紧缩不及时会造成 LSM-Tree 存在很多垃圾 |
| 后台流量 | 1. 更稳定可预测，不会受后台 compaction 突发流量影响。 | 1. 写吞吐过高，compaction 跟不上，会进一步加重读放大。<br/>2. 由于外存总带宽有限，compaction 会影响读写吞吐。<br/>3. 随着数据越来越多，compaction 对正常写影响越来越大。 | RocksDB 写入太过快会引起 write stall，即限制写入，以期尽快 compaction 将数据下沉。 |
| 存储放大 | 1. 有些 Page 没有用满 | 1. 同一个 Key 存多遍 |  |
| 并发控制 | 1. 同一个 Key 只存在一个地方<br/>2. 树结构容易加范围锁。 | 同一个 Key 会存多遍，一般使用 MVCC 进行控制。 |  |

## 六、存储引擎的其他应用

### 1. **全文索引和模糊索引（Full-text search and fuzzy indexes）**。

前述索引只提供全字段的精确匹配，而不提供类似搜索引擎的功能。比如，按字符串中包含的单词查询，针对笔误的单词查询。

在工程中常用 [Apace Lucene](https://lucene.apache.org/ "Apace Lucene") 库，和其包装出来的服务：[Elasticsearch](https://www.elastic.co/cn/ "Elasticsearch")。他也使用类似 LSM-tree 的日志存储结构，但其索引是一个有限状态自动机，在行为上类似 Trie 树。

### 2. 全内存数据结构

随着单位内存成本下降，甚至支持持久化（non-volatile memory，NVM，如 Intel 的 [傲腾](https://www.intel.cn/content/www/cn/zh/products/details/memory-storage/optane-dc-persistent-memory.html "傲腾")），全内存数据库也逐渐开始流行。

根据是否需要持久化，内存数据大概可以分为两类：

1. **不需要持久化**。如只用于缓存的 Memcached。
2. **需要持久化**。通过 WAL、定期 snapshot、远程备份等等来对数据进行持久化。但使用内存处理全部读写，因此仍是内存数据库。

> VoltDB, MemSQL, and Oracle TimesTen 是提供关系模型的内存数据库。RAMCloud 是提供持久化保证的 KV 数据库。Redis and Couchbase 仅提供弱持久化保证。
> 

内存数据库存在优势的原因不仅在于不需要读取磁盘，而在更于不需要对数据结构进行**序列化、编码**后以适应磁盘所带来的**额外开销**。

当然，内存数据库还有以下优点：

1. **提供更丰富的数据抽象**。如 set 和 queue 这种只存在于内存中的数据抽象。
2. **实现相对简单**。因为所有数据都在内存中。

此外，内存数据库还可以通过类似操作系统 swap 的方式，提供比物理机内存更大的存储空间，但由于其有更多数据库相关信息，可以将换入换出的粒度做的更细、性能做的更好。

基于**非易失性存储器**（non-volatile memory，NVM） 的存储引擎也是这些年研究的一个热点。

### 3. 列存

在OLAP的场景下，数据有可能达到数十亿行和数 PB 大。虽然事实表可能通常有几十上百列，但是单次查询通常只关注其中几个维度（列）。

如查询**人们是否更倾向于在一周的某一天购买新鲜水果或糖果**：

```sql
SELECT
  dim_date.weekday,
  dim_product.category,
  SUM(fact_sales.quantity) AS quantity_sold
FROM fact_sales
  JOIN dim_date ON fact_sales.date_key = dim_date.date_key
  JOIN dim_product ON fact_sales.product_sk = dim_product.product_sk
WHERE
  dim_date.year = 2013 AND
  dim_product.category IN ('Fresh fruit', 'Candy')
GROUP BY
  dim_date.weekday, dim_product.category;
```

由于传统数据库通常是按行存储的，这意味着对于属性（列）很多的表，哪怕只查询一个属性，也必须从磁盘上取出很多属性，无疑浪费了 IO 带宽、增大了读放大。

于是一个很自然的想法呼之欲出：每一个列分开存储好不好？

![ddia-3-10-store-by-column.png](/images/ch03-fig10.png)

不同列之间同一个行的字段可以通过下标来对应。当然也可以内嵌主键来对应，但那样存储成本就太高了。

#### 列压缩

将所有数据分列存储在一块，带来了一个意外的好处，由于同一属性的数据相似度高，因此更易压缩。

如果每一列中值域相比行数要小的多，可以用**位图编码（ *[bitmap encoding](https://en.wikipedia.org/wiki/Bitmap_index "bitmap encoding")* ）**。举个例子，零售商可能有数十亿的销售交易，但只有 100,000 个不同的产品。

![ddia-3-11-compress.png](/images/ch03-fig11.png)

上图中，是一个列分片中的数据，可以看出只有 {29, 30, 31, 68, 69, 74} 六个离散值。针对每个值出现的位置，我们使用一个 bit array 来表示：

1. bit map 下标对应列的下标
2. 值为 0 则表示该下标没有出现该值
3. 值为 1 则表示该下标出现了该值

如果 bit array 是稀疏的，即大量的都是 0，只要少量的 1。其实还可以使用 **[游程编码](https://zh.wikipedia.org/zh/%E6%B8%B8%E7%A8%8B%E7%BC%96%E7%A0%81 "游程编码")（RLE， Run-length encoding）** 进一步压缩：

1. 将连续的 0 和 1，改写成 `数量+值`，比如 `product_sk = 29` 是 `9 个 0，1 个 1，8 个 0`。
2. 使用一个小技巧，将信息进一步压缩。比如将同值项合并后，肯定是 0 1 交错出现，固定第一个值为 0，则交错出现的 0 和 1 的值也不用写了。则 `product_sk = 29`  编码变成 `9，1，8`
3. 由于我们知道 bit array 长度，则最后一个数字也可以省掉，因为它可以通过 `array len - sum(other lens)` 得到，则 `product_sk = 29`  的编码最后变成：`9，1`

位图索引很适合应对查询中的逻辑运算条件，比如：

```sql
WHERE product_sk IN（30，68，69）
```

可以转换为 `product_sk = 30`、`product_sk = 68`和 `product_sk = 69`这三个 bit array 按位或（OR）。

```sql
WHERE product_sk = 31 AND store_sk = 3
```

可以转换为 `product_sk = 31`和 `store_sk = 3`的 bit array 的按位与，就可以得到所有需要的位置。

#### 内存带宽和向量化处理

数仓的超大规模数据量带来了以下瓶颈：

1. 内存处理带宽
2. CPU 分支预测错误和[流水线停顿](https://zh.wikipedia.org/wiki/%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%81%9C%E9%A1%BF "流水线停顿")

关于内存的瓶颈可已通过前述的数据压缩来缓解。对于 CPU 的瓶颈可以使用：

1. 列式存储和压缩可以让数据尽可能多地缓存在 L1 中，结合位图存储进行快速处理。
2. 使用 SIMD 用更少的时钟周期处理更多的数据。

#### 列式存储的排序

由于数仓查询多集中于聚合算子（比如 sum，avg，min，max），列式存储中的存储顺序相对不重要。但也免不了需要对某些列利用条件进行筛选，为此我们可以如 LSM-Tree 一样，对所有行按某一列进行排序后存储。

> 注意，不可能同时对多列进行排序。因为我们需要维护多列间的下标间的对应关系，才可能按行取数据。
> 

同时，排序后的那一列，压缩效果会更好。

#### 不同副本，不同排序

在分布式数据库（数仓这么大，通常是分布式的）中，同一份数据我们会存储多份。对于每一份数据，我们可以按不同列有序存储。这样，针对不同的查询需求，便可以路由到不同的副本上做处理。当然，这样也最多只能建立副本数（通常是 3 个左右）列索引。

这一想法由 C-Store 引入，并且为商业数据仓库 Vertica 采用。

#### 列式存储的写入

上述针对数仓的优化（列式存储、数据压缩和按列排序）都是为了解决数仓中常见的读写负载，读多写少，且读取都是超大规模的数据。

> 我们针对读做了优化，就让写入变得相对困难。
> 

比如 B 树的**原地更新流**是不太行的。举个例子，要在中间某行插入一个数据，**纵向**来说，会影响所有的列文件（如果不做 segment 的话）；为了保证多列间按下标对应，**横向**来说，又得更新该行不同列的所有列文件。

所幸我们有 LSM-Tree 的追加流。

1. 将新写入的数据在**内存**中 Batch 好，按行按列，选什么数据结构可以看需求。
2. 然后达到一定阈值后，批量刷到**外存**，并与老数据合并。

数仓 Vertica 就是这么做的。

## 七、总结

在存储引擎领域，没有真正的银弹，纵观各种现在的存储引擎的存储解决方案，都是在读和写之间做权衡（trade off）。恰当的**存储格式**能加快写（比如LSM Tree），但是会让读取很慢；也可以加快读（查找树、B族树），但会让写入较慢。为了弥补读性能，可以构建索引，但是会牺牲写入性能和耗费额外空间。只有合理的硬盘存储结构和内存数据结构（索引）的结合，才能发挥出特定的效果，解决特定的问题。

参考文献

1. 《数据敏感型应用设计》
2. 《MySQL技术内幕 InnoDB存储引擎》第二版
3. 《ElasticSearch 权威指南》
4. 《Lucene 原理与源码分析》
5. 《大规模分布式存储系统：原理解析与架构实战》
6. 《Ceph设计原理与实现》
7. 《Ceph源码分析》
8. https://lrita.github.io/images/posts/filesystem/Linux.Kernel.IO.Scheduler.pdf
9. https://tech.meituan.com/2017/05/19/about-desk-io.html
10. http://info.mrtlab.com/picdoc/Hard-disk-structure.html