---
title: "捋一捋ElasticSearch（三）| 分布式特性"
date: 2022-10-04T10:21:24+08:00
draft: false
tags:
  [
    "elasticsearch",
    "distribte storage",
    "storage"
  ]
categories: ["distribute storge"]
---
> 前段时间学习过[MIT6.284 分布式系统](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/)这门课程，该课程主要讲解分布式系统中一些设计背景和经典案例，正好工作需要详细了解下ElasticSearch的原理，于是就按照分布式系统需要考虑的问题来拆解下ElasticSearch在分布式特性方面有哪些考虑和设计，做过哪些权衡和取舍

## 1. 分布式系统
### 1.1 设计分布式系统的驱动力

- **可扩展性（Scalability**：人们需要获得更高的计算性能。可以这么理解这一点，（大量的计算机意味着）大量的并行运算，大量CPU、大量内存、以及大量磁盘在并行的运行
- **可用性（Availability）**：另一个人们构建分布式系统的原因是，它可以提供容错（tolerate faults）比如两台计算机运行完全相同的任务，其中一台发生故障，可以切换到另一台
- 延迟考虑: 如果客户遍布世界各地，通常需要考虑在全球范围内部署服务，以方便用户就近访问最近数据中心所提供的服务，从而避免数据请求跨越了半个地球才能到达目标


## 2. 数据分片

海量的数据集，对系统的存储容量和查询能力都有很高的要求，分布式系统如何处理海量的数据请求并具有一定扩展性？
### 2.1 为何要分片
采用数据分片的主要目的是提高可扩展性。不同的分片可以放在一个无共享集群的不同节点上。这样一个大数据集可以分散在更多的磁盘上，查询负载也随之分布到更多的磁盘和处理器上。对单个分片进行查询时，每个节点对自己所在分片可以独立执行查询操作，因此添加更多的节点可以提高查询吞吐量。 超大而复杂的查询尽管比较困难，但也可能做到跨节点的并行处理

es中分片是一个功能完整的搜索引擎（一个分片就是一个Lucene的实例）它拥有使用一个节点上的所有资源的能力

### 2.2 怎么路由

当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？
首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：

``` shard = hash(routing) % number_of_primary_shards ```

routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再对 number_of_primary_shards （主分片的数量）取模。这个分布在 0 到 number_of_primary_shards-1 之间的数，就是我们所寻求的文档所在分片的位置

实际上，所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档（例如所有属于同一个用户的文档）都被存储到同一个分片中

### 2.3 分片数量

主分片的数量在索引创建的时候指定，后期无法更改。实际上，这个数目定义了这个索引能够存储的最大数据量，也就是说一个索引的容量是有上限的，不能无限扩容。副本分片的数量随时可以修改，增加副本分片可以增加数据查询的能力
>当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少，你需要增加更多的硬件资源来提升吞吐量。但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去2个节点的情况下不丢失任何数据。

### 2.4 ElasticSearch 分片方式的局限性

 上面我们讲了es分片路由的方式，相信聪明的你已经很快想到这种方式有一个非常严重的问题，就是如果number_of_primary_shards发生了变化，也就是一个索引的主分片的数量被扩大或者缩小的话，大量的文档到分片的映射关系将会发生改变。这就解释了为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了

 > 目前为止你是不是觉得es为何设计的如此之low，为何不用一致性hash之类的解决方案呢？ 其实这只是在性能和便利性之间的一个折中罢了，es也是有办法可以进行主分片的扩容的，只是es的设计者认为主分片扩容是很伤性能的一件事，用户应该在前期就规划好单个索引的数据量来避免不必要的数据迁移

 [新版本es的split index解决方案](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html#:~:text=The%20split%20index%20API%20allows,is%20determined%20by%20the%20index.)

## 3. 分片的复制

### 3.1 为何要复制

- 当部分组件出现位障，系统依然可以继续工作，从而提高**可用性**
- 扩展至多台机器以同时提供数据访问服务，从而提高读吞吐量

### 3.2 如何保证副本数据的一致性

> 我们知道在分布式系统中，数据复制一般有三种方式：主从复制，多主复制，无主复制，这些复制方式都有各种的优缺点，显然es用的是主从复制的思路，那么有当使用主从复制的时候，一个主副本会有1到多个从副本，这些副本之间的一致性如何保证呢？如果主几点挂了，是否会丢失数据呢？

es在数据一致性问题方面做的非常灵活，用户可以针对单个文档的单次提交干预其存储行为，用户在提交文档进行索引的时候可以传入一个`wait_for_active_shards`参数（es5之前叫`consistency`），表示需要得到几个副本的写入成功响应后再返回，否则就等待或者重试，直到超时。默认情况下，`wait_for_active_shards=1`, 可以通过设置 `index.write.wait_for_active_shards` 在索引的setting中动态覆盖此默认值。也可以在每次提交参数中指定,类似下面这样
`

``` shell
POST /my-index-000001/_doc?wait_for_active_shards=3
{
  "@timestamp": "2099-11-15T13:12:00",
  "message": "111GET /search HTTP/1.1 200 1070000",
  "user": {
    "id": "kimchy"
  }
}
```

写入操作响应的_shards部分显示复制成功/失败的分片副本数,如下所示

```json
{
  "_shards": {
    "total": 2,
    "failed": 0,
    "successful": 2
  }
}
```
## 4. 文档分布式索引过程（文档写入过程）

### 4.1 单个文档索引过程

![图 2](/images/elasticsearch_principle_three_pic_es_singledoc_write_process.png)  

1. 客户端向任意节点发起请求(例图中node1),此时可以将node1叫做协调节点，协调节点负责根据文档->分片的路由规则计算出主分片id
2. 协调节点将请求转发到主分片所在的node节点（例图中node3）的Lucene实例上
3. 执行主分片的写入操作,如果协调节点收到主分片写入成功的响应。则并行的执行写入副本操作（局部更新文档情况，会有retry_on_conflict逻辑）
4. 协调节点判断如果有wait_for_active_shards数量的副本分片都写入成功，协调节点讲返回给客户端ACK，否则等到timeout时间后返回给客户端超时响应

### 4.2 多个文档索引过程

![图 3](/images/elasticsearch_principle_three_pic_es_mutidoc_write_process.png)  

1. 客户端向任意节点发起bulk请求(例图中node1),此时可以将node1叫做协调节点，协调节点负责根据文档->分片的路由规则计算出主分片id
2. 协调节点(例图中node1) 向每个索引所在的节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机
3. 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端

> bulk API 还可以在整个批量请求的最顶层使用 wait_for_active_shards 参数，以及在每个请求中的元数据中使用 routing 参数


## 5. 文档分布式取回（fetch）过程
协调节点可以从主分片或者任意副本分片检索文档

### 5.1 单个文档的fetch过程
![picture 1](/images/elasticsearch_principle_three_pic_es_singledoc_read_process.png) 

以下是从主分片或者副本分片检索文档的步骤顺序：

1. 客户端向任意节点发起请求(例图中node1),此时可以将node1叫做协调节点

2. 协调节点负责根据文档->分片的路由规则计算出主分片id（例如分片0），分片0的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到node2

3. node2 将文档返回给协调节点（node1） ，然后协调节点将文档返回给客户端

在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡
在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 只是所有副本都写入成功了过后，文档在主分片和副本分片才都是可用的

### 5.2 多个文档的fetch过程

多文档索引使用bulk API，类似的多文档fetch使用mget API, 区别在于协调节点知道每个文档存在哪个分片中。 它将整个多文档请求分解成每个分片的多文档请求，并且将这些请求并行转发到每个参与节点。

以下是使用单个 mget 请求取回多个文档所需的步骤顺序：

1. 客户端向任意节点发起mget请求(例图中node1),此时可以将node1叫做协调节点,协调节点将多个请求的文档按照所在的分片进行分类
2. 协调节点向每个分片构建多文档获取请求，然后并行转发这些请求到每个主分片或者副本分片所在的节点上。一旦收到所有答复， 协调节点构建响应并将其返回给客户端。
   
> 当然，也可以对个文档设置 routing 参数

![picture 2](/images/elasticsearch_principle_three_pic_es_mutidoc_read_process.png)  


## 分布式一致性和节点选举