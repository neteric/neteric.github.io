# 离在线混部调研

## 1. 背景

公司战略目标：降本增效

HPA上线使得在线业务的波谷期空闲出大量的计算资源

大数据业务云原生化是必然的趋势，公司大数据团队正在着手落地MR/Spark类的离线业务云原生化

## 2. 业界离在线混部方案分类

按照混合类型分： 分时复用和部分混合和全部混合
按照隔离类型分： 资源独享和资源共享
按照部署方式分： 虚拟机部署和容器部署

这三个维度的组合，目前实际应用中主要是`分时复用 + 独占资源 + 物理机部署`、`部分混合 + 独占资源 + 容器部署`、`全部混合 + 共享资源 + 容器部署` 这三种模式，这三种模式其实也是离在线混部的三个阶段。

### 2.1 分时复用 + 独占资源 + 物理机部署

这种组合属于入门级的在离线混部选择，比如物理机运行服务且分时整机腾挪

好处是能够快速实现在离线混部，收获成本降低的红利。技术门槛上，这种方式规避了前面说的复杂的资源隔离问题，并且调度决策足够清晰，服务部署节奏有明确的预期，整个系统可以设计得比较简单。缺点是没有充分发挥出在离线混部的资源利用率潜力，目前主要是一些初创企业在应用。阿里早期在大促期间，将所有离线作业节点下线换上在线服务，也可以看做这种形态的近似版本

### 2.2 部分混合 + 独占资源 + 容器部署

在这种模型下，业务开发人员将服务部署在云原生部署平台，选择某些指标（大部分伴随着流量潮汐特性）来衡量服务负载，平台会按照业务指定规则对服务节点数量扩缩容。当流量低峰期来临时，在线服务会有大量碎片资源可释放，部署平台会整理碎片资源，将碎片资源化零为整后，以整机的方式将资源租借给离线作业使用

### 2.3 全部混合 + 共享资源 + 容器部署

与上述几种方案最大的不同在于，转让的资源规则是动态决策的。在一个大企业中，服务数量数以万计，要求所有在线服务制定扩缩容决策是很难做到的。同时，业务在部署服务时更关注服务稳定性，常常按照最大流量评估资源，这样就导致流量低峰期有大量的资源浪费。
比较典型的是阿里，百度、腾讯的方案

## 3. 离在线混部的问题和难点

### 3.1 资源隔离

离线业务的运行不能影响在线业务的SLA

K8S本身的QoS策略就是为了按照优先级保障业务的稳定性，当Node节点的资源到达安全阈值的时候会发生[节点压力驱逐](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)优先驱逐掉BestEffort类型的离线业务，其次是Burstable

但是对于一些长尾延迟来说，仅仅通过上述K8S自身的手段，保证不了服务质量。发生争抢时，系统 load 就会比较高，所以一般业界会有如下一些更加全面的方式来保障在线业务的稳定性和SLA

CPU隔离（优先保障在线业务使用cpu）cpu burst（阿里）, L3 Cache隔离RDT技术/CAT技术，cfs抢占调度（百度）， HT超线程隔离，bt调度（腾讯），离线“大框”模型（百度）离线大框可以实时收缩

内存隔离： 存带宽分配(MBA)隔离， oom score， cgroup，离线“大框”硬限（百度）

磁盘io隔离：使用分布式存储，对块设备执行iops限速，bufferio 使用cgroupv2限制page cache使用量，离线业务remote shuffle

磁盘空间隔离：diskquota

inode数量隔离：diskquota

网络带宽隔离：tc, cgroup, ipset，网络QoS标签

进程状态数：netlink

- **干扰检测**

虽然上述讨论了几种主要资源的管理，但由于底层技术限制，部分资源的管理还不完善，并且竞争资源不仅仅是这些，所以，为了保证安全混部，还要增加干扰检测和冲突处理。换句话说，资源隔离是为了避免干扰，干扰检测是根据应用的实际运行情况在冲突发生前或发生后采取措施。在线作业可以提供获取时延数据的方式，或者暴露相关接口，另外还可以收集一些硬件指标，如通过perf收集CPI数据，RDT收集L3 Cache，或epbf收集内核关键路径数据。通过算法分析来判断在线作业是否受影响，若发现异常，便开始降低离线资源，驱逐离线任务

### 3.2 离线业务的资源保障

被压缩往往比被kill好很多，离线有一些大数据的业务可能需要运行 2—3 小时，如果在即将完成之前被 kill 掉，它势必需要重算，如果通过压缩的方式可以缓解这种破坏的行为，同时对不可压缩的资源我们足量避让就可以了，不需要粗暴的全部杀掉
1. 可压缩限压缩
2. 不可压缩资源避让足量
   
### 3.3 离在线调度框架的整合

k8s的每个节点都会上报节点资源总量（例如allocatable cpu）。对于一个待调度的pod，k8s调度器会查看pod的资源请求配额（例如cpu reqeust）以及相关调度约束，然后经过预选和优选阶段，最终挑选出一个最佳的node用于部署该pod。如果混部任务直接使用这套原生的调度机制会存在几个问题：

1. 混部pod会占用原生的资源配额（例如cpu request），这会导致在线任务发布的时候没有可用资源；

2. 原生调度本质是静态调度，没有考虑机器实际负载，因此没法有效地将混部任务调度到实际负载有空闲的机器上

3. 不支持Gang调度
   


### 3.4 可观测性体系的建立

### 3.5 部门墙

## 4. 各公司案例特点

## 5. Koordinator 方案介绍

## 6. 总结：中小公司离在线混部实践思路

线上独享内核+容器部署+动态策略

线下共享内核+容器部署+动态策略

## 参考文档

[1] [腾讯：Caelus—全场景在离线混部解决方案](https://cloud.tencent.com/developer/article/1759977)

[2] [字节跳动：自动化弹性伸缩如何支持百万级核心错峰混部](https://cloud.tencent.com/developer/news/653586)

[3] [字节跳动：混布环境下集群的性能评估与优化](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/bytedance-performance-evaluation-optimization.html)

[4] [百度大规模战略性混部系统演进](https://www.infoq.cn/article/aeut*zaiffp0q4mskdsg)

[5] [百度混部实践：如何提高 Kubernetes 集群资源利用率？](https://mp.weixin.qq.com/s/12XFN2lPB3grS5FteaF__A)

[6] [阿里巴巴：数据中心日均 CPU 利用率 45% 的运行之道--阿里巴巴规模化混部技术演进](https://developer.aliyun.com/article/651202)

[7] [阿里巴巴：Co-Location of Workloads with High Resource Efficiency](https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf)

[8] [阿里大规模业务混部下的全链路资源隔离技术演进](https://mp.weixin.qq.com/s/_DTQ4Q2dC-kN3zyozGf9QA)

[9] [一文看懂业界在离线混部技术](https://www.infoq.cn/article/knqswz6qrggwmv6axwqu)

[10] [美团：提升资源利用率与保障服务质量，鱼与熊掌不可兼得？](https://mp.weixin.qq.com/s/hQKM9beWcx7CKMvpJxznfQ)

[11] [B站云原生混部技术实践](https://mp.weixin.qq.com/s/pPEkfrLm0XEpgMU1KjiD4A)

[12] [华为：基于Volcano的离在线业务混部技术探索](https://www.bilibili.com/video/BV1AZ4y1X7AQ/?vd_source=cad2cc6310088fc3945e9d1cb002adee)

[13] [K8S节点压力驱逐](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)