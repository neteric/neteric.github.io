# 离在线混部调研

## 1. 背景

公司战略目标：降本增效

HPA上线使得在线业务的波谷期空闲出大量的计算资源

大数据业务云原生化是必然的趋势，公司大数据团队正在着手落地MR/Spark类的离线业务云原生化

## 2. 业界离在线混部方案分类

按照混合类型分： 分时复用和部分混合和全部混合
按照隔离类型分： 资源独享和资源共享
按照部署方式分： 虚拟机部署和容器部署

这三个维度的组合，目前实际应用中主要是`分时复用 + 独占资源 + 物理机部署`、`部分混合 + 独占资源 + 容器部署`、`全部混合 + 共享资源 + 容器部署` 这三种模式，这三种模式其实也是离在线混部的三个阶段。

### 2.1 分时复用 + 独占资源 + 物理机部署

这种组合属于入门级的在离线混部选择，比如物理机运行服务且分时整机腾挪

好处是能够快速实现在离线混部，收获成本降低的红利。技术门槛上，这种方式规避了前面说的复杂的资源隔离问题，并且调度决策足够清晰，服务部署节奏有明确的预期，整个系统可以设计得比较简单。缺点是没有充分发挥出在离线混部的资源利用率潜力，目前主要是一些初创企业在应用。阿里早期在大促期间，将所有离线作业节点下线换上在线服务，也可以看做这种形态的近似版本

### 2.2 部分混合 + 独占资源 + 容器部署

在这种模型下，业务开发人员将服务部署在云原生部署平台，选择某些指标（大部分伴随着流量潮汐特性）来衡量服务负载，平台会按照业务指定规则对服务节点数量扩缩容。当流量低峰期来临时，在线服务会有大量碎片资源可释放，部署平台会整理碎片资源，将碎片资源化零为整后，以整机的方式将资源租借给离线作业使用

### 2.3 全部混合 + 共享资源 + 容器部署

与上述几种方案最大的不同在于，转让的资源规则是动态决策的。在一个大企业中，服务数量数以万计，要求所有在线服务制定扩缩容决策是很难做到的。同时，业务在部署服务时更关注服务稳定性，常常按照最大流量评估资源，这样就导致流量低峰期有大量的资源浪费。
比较典型的是阿里，百度、腾讯的方案

## 3. 离在线混部的目标

1. 尽可以能的提高资源利用率
2. 保证在线业务的SLA
3. 保证离线业务运行良好

## 4. 离在线混部的难点和常见方案

### 4.1 资源隔离

为了保证离线业务的运行不能影响在线业务的SLA，那必须要做好离在线资源的隔离,其实K8S本身的QoS策略就是为了按照优先级保障业务的稳定性，当Node节点的资源到达安全阈值的时候会发生[节点压力驱逐](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)优先驱逐掉BestEffort类型的离线业务，其次是Burstable。但是当资源发生争抢，系统负载高的时候，仅仅通过K8S自身的手段，保证不了在线业务质量，在线业务会有明显的长尾延迟。所以一般会通过一些隔离手段来保证在线业务的稳定性，业界主要有如下一些方式

#### CPU 隔离

- CPU Burst

CPU Burst 技术最初由阿里云操作系统团队提出并贡献到Linux社区和龙蜥社区，分别在 Linux 5.14 和龙蜥ANCK 4.19 版本可用，在传统的 CPU Bandwidth Controller quota 和 period 基础上引入 burst 的概念。当容器的 CPU 使用低于 quota 时，可用于突发的 burst 资源累积下来；当容器的 CPU 使用超过 quota，允许使用累积的 burst 资源。最终达到的效果是将容器更长时间的平均 CPU 消耗限制在 quota 范围内，允许短时间内的 CPU 使用超过其 quota

在容器场景中使用 CPU Burst 之后，测试容器的服务质量显著提升，在实测中可以发现使用该特性技术以后，RT长尾问题几乎消失

[干掉讨厌的 CPU 限流，让容器跑得更快](https://developer.aliyun.com/article/786430?spm=a2c4g.11186623.0.0.48787123wBxE3H)

- cfs 抢占调度

CFS让高优任务占尽优势的同时，也保留了低优任务使用CPU的权利，作为一个通用的调度算法，CFS是公平也是合理的。但当我们但在混部的场景下这会导致一个监控上的毛刺，或引发线上服务的抖动,CFS从设计理念上就不支持绝对抢占，因此，腾讯云原生内核自行研发了支持绝对抢占的离线调度器——BT调度

[云原生下，TencentOS “如意” CPU QoS之绝对抢占](https://cloud.tencent.com/developer/article/1876817)

- L3 Cache隔离技术

英特尔RDT提供了一个由多个组件功能（包括 CMT、CAT、CDP、MBM 和 MBA）组成的框架，用于缓存和内存监控及分配功能。这些技术可以跟踪和控制平台上同时运行的多个应用程序、容器或 VM 使用的共享资源，例如最后一级缓存 (LLC) 和主内存 (DRAM) 带宽。RDT 可以帮助检测“吵闹的邻居”并减少性能干扰，从而确保复杂环境中关键工作负载的性能

[英特尔® 资源调配技术 (英特尔® RDT) 框架](https://www.intel.cn/content/www/cn/zh/architecture-and-technology/resource-director-technology.html)

- 超线程隔离

在某些线上业务场景中，使用超线程情况下的 QPS 比未使用超线程时下降明显，并且相应 RT 也增加了不少。根本原因跟超线程的物理性质有关，超线程技术在一个物理核上模拟两个逻辑核，两个逻辑核具有各自独立的寄存器（eax、ebx、ecx、msr 等等）和 APIC，但会共享使用物理核的执行资源，包括执行引擎、L1/L2 缓存、TLB 和系统总线等等。这就意味着，如果一对 HT 的一个核上跑了在线任务，与此同时它对应的 HT 核上跑了一个离线任务，那么它们之间是会发生竞争的，这就是我们需要解决的问题。

为了尽可能减轻这种竞争的影响，我们想要让一个核上的在线任务执行的时候，它对应的 HT 上不再运行离线任务；或者当一个核上有离线任务运行的时候，在线任务调度到了其对应的 HT 上时，离线任务会被驱赶走。听起来离线混得很惨对不对？但是这就是我们保证 HT 资源不被争抢的机制

[龙蜥Group Identity 技术](https://help.aliyun.com/document_detail/338407.html)

[龙蜥超线程隔离 SMT expeller 技术](https://mp.weixin.qq.com/s/_DTQ4Q2dC-kN3zyozGf9QA)

- 离线大框，动态绑核

通过cpuset cgroup对整体混部大框做了绑核处理，避免混部任务进程频繁切换干扰在线业务进程。当混部算力改变时，再给离线大框动态选取相应的cpu核心进行绑定。同时在选取cpu核心的时候还需要考虑了cpu HT，即尽量将同一个物理核上的逻辑核同时绑定给混部任务使用。否则，如果在线任务和混部任务分别跑在一个物理核的两个逻辑核上，在线任务还是有可能受到“noisy neighbor”干扰

[B站云原生混部技术实践](https://mp.weixin.qq.com/s/pPEkfrLm0XEpgMU1KjiD4A)

#### 内存隔离

- 存带宽分配(MBA)隔离
- 内存提前异步回收
- oom score
- 基于内存安全阈值的主动驱逐机制

#### 磁盘隔离

- 使用分布式存储实现存算分离
- 对块设备执行iops限速
- bufferio 使用cgroupv2限制cache使用量
- 离线业务remote shuffle
- 使用diskquota对文件系统空间和inode数量限制

#### 网络隔离

- tc
- ipset
- 网络QoS标签

#### 干扰检测

虽然上述讨论了几种主要资源的管理，但由于底层技术限制，部分资源的管理还不完善，并且竞争资源不仅仅是这些，所以，为了保证安全混部，还要增加干扰检测和冲突处理。换句话说，资源隔离是为了避免干扰，干扰检测是根据应用的实际运行情况在冲突发生前或发生后采取措施。在线作业可以提供获取时延数据的方式，或者暴露相关接口，另外还可以收集一些硬件指标，如通过perf收集CPI数据，RDT收集L3 Cache，或epbf收集内核关键路径数据。通过算法分析来判断在线作业是否受影响，若发现异常，便开始降低离线资源，驱逐离线任务

### 4.2 离线业务的资源保障

- 可压缩限压缩
- 不可压缩资源避让足量
- 基于资源满足的驱逐机制

### 4.3 离在线调度框架的整合

k8s的每个节点都会上报节点资源总量（例如allocatable cpu）。对于一个待调度的pod，k8s调度器会查看pod的资源请求配额（例如cpu reqeust）以及相关调度约束，然后经过预选和优选阶段，最终挑选出一个最佳的node用于部署该pod。如果混部任务直接使用这套原生的调度机制会存在几个问题：

1. 混部pod会占用原生的资源配额（例如cpu request），这会导致在线任务发布的时候没有可用资源；

2. 原生调度本质是静态调度，没有考虑机器实际负载，因此没法有效地将混部任务调度到实际负载有空闲的机器上

3. 不支持Gang调度

### 4.4 可观测性体系的建立

可观测性对于排查单机的混部问题非常有用，我们需要监控曲线来查看某个时刻的混部资源使用情况。同时，我们也可以随时查看整体集群的混部算力使用趋势

### 4.5 部门墙

公司内部，机器的产品线一般是固定的，成本和利用率也是按照产品线计算，所以通常情况下机器是不会在不同部门之间自由流转的。引入在离线混部之后，势必需要打破部门墙，对成本和资源利用率计算有一个能融合能分解的调整，才能准确反映出混部的巨大成本价值并持续精细化运营

## 4. Koordinator 介绍

<https://koordinator.sh/zh-Hans/docs/designs/enhanced-scheduler-extension>

## 5. Volcano 介绍

<https://volcano.sh/zh/docs/>

## 6. 总结：中小公司离在线混部实践思路

线上：分时复用 + 独占资源 + 动态调度

线下：共享内核+容器部署+动态策略

## 参考文档

[1] [腾讯：Caelus—全场景在离线混部解决方案](https://cloud.tencent.com/developer/article/1759977)

[2] [字节跳动：自动化弹性伸缩如何支持百万级核心错峰混部](https://cloud.tencent.com/developer/news/653586)

[3] [字节跳动：混布环境下集群的性能评估与优化](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/bytedance-performance-evaluation-optimization.html)

[4] [百度大规模战略性混部系统演进](https://www.infoq.cn/article/aeut*zaiffp0q4mskdsg)

[5] [百度混部实践：如何提高 Kubernetes 集群资源利用率？](https://mp.weixin.qq.com/s/12XFN2lPB3grS5FteaF__A)

[6] [阿里巴巴：数据中心日均 CPU 利用率 45% 的运行之道--阿里巴巴规模化混部技术演进](https://developer.aliyun.com/article/651202)

[7] [阿里巴巴：Co-Location of Workloads with High Resource Efficiency](https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf)

[8] [阿里大规模业务混部下的全链路资源隔离技术演进](https://mp.weixin.qq.com/s/_DTQ4Q2dC-kN3zyozGf9QA)

[9] [一文看懂业界在离线混部技术](https://www.infoq.cn/article/knqswz6qrggwmv6axwqu)

[10] [美团：提升资源利用率与保障服务质量，鱼与熊掌不可兼得？](https://mp.weixin.qq.com/s/hQKM9beWcx7CKMvpJxznfQ)

[11] [B站云原生混部技术实践](https://mp.weixin.qq.com/s/pPEkfrLm0XEpgMU1KjiD4A)

[12] [华为：基于Volcano的离在线业务混部技术探索](https://www.bilibili.com/video/BV1AZ4y1X7AQ/?vd_source=cad2cc6310088fc3945e9d1cb002adee)

[13] [K8S节点压力驱逐](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)

[14] [摆脱不必要的节流,同时实现高CPU利用率和高应用程序性能](https://www.youtube.com/watch?v=9ao7Ix2ugK4)
